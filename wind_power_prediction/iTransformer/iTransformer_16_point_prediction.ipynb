{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d993875",
   "metadata": {},
   "source": [
    "# iTransformer Wind Power Prediction Model\n",
    "\n",
    "This notebook implements an iTransformer (Inverted Transformer) model for 16-point wind power prediction. iTransformer is the state-of-the-art model for multivariate time series forecasting, published in 2024.\n",
    "\n",
    "## Key Features:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c0169a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== iTransformer Configuration ===\n",
      "Time Configuration: 96 points -> 16 points\n",
      "Model: d_model=64, nhead=8, layers=2\n",
      "Training: batch_size=128, lr=0.0001, epochs=300\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ===== iTransformer Configuration =====\n",
    "\n",
    "\n",
    "class iTransformerConfig:\n",
    "    # Data configuration\n",
    "    seq_length = 96          # Input sequence length (96 points, 24 hours, 15min intervals)\n",
    "    pred_length = 16         # Prediction sequence length (16 points, 4 hours)\n",
    "    train_val_split = 0.8    # Train/validation split ratio\n",
    "    \n",
    "    # Data loading\n",
    "    batch_size = 128         \n",
    "    num_workers = 8          # DataLoader workers\n",
    "    \n",
    "    # Feature engineering\n",
    "    use_feature_engineering = True  # 是否使用特征工程\n",
    "    time_interval_minutes = 15\n",
    "\n",
    "    add_trend_features = True        # 趋势特征\n",
    "\n",
    "    add_lag_features = True        # 添加滞后特征\n",
    "    lag_steps = [1, 2, 4, 8, 16, 24, 48, 96]  #\n",
    "    \n",
    "    add_ramp_features = True       # 添加功率爬坡特征\n",
    "    ramp_windows = [4, 8, 16]       # 爬坡检测窗口\n",
    "\n",
    "    add_anomaly_features = True      # 异常检测特征\n",
    "    anomaly_windows = [24, 96]       # 异常检测的滚动窗口大小\n",
    "\n",
    "    add_statistical_features = True  # 统计滚动特征\n",
    "    statistical_windows = [4, 8, 16, 24, 48, 96]  # 统计特征的滚动窗口大小\n",
    "    enable_rolling_mean = True       # 启用滚动均值\n",
    "    enable_rolling_std = True        # 启用滚动标准差\n",
    "    enable_rolling_max = True        # 启用滚动最大值\n",
    "    enable_rolling_min = True        # 启用滚动最小值\n",
    "    enable_rolling_range = True      # 启用滚动范围\n",
    "\n",
    "\n",
    "    # iTransformer model structure\n",
    "    d_model = 64            # Transformer维度（较小以适应小数据）\n",
    "    nhead = 8                # 注意力头数\n",
    "    num_encoder_layers = 2   # 编码器层数\n",
    "    dim_feedforward = 256   # FFN dimension\n",
    "    dropout = 0.1            # Dropout rate\n",
    "    activation = 'gelu'      # Activation function\n",
    "    \n",
    "    # Training configuration\n",
    "    num_epochs = 300         # Training epochs\n",
    "    learning_rate = 0.0001   # Learning rate\n",
    "\n",
    "    huber_delta = 1.0  \n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    lr_patience = 8          # LR scheduler patience\n",
    "    lr_factor = 0.5          # LR decay factor\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stop_patience = 50 \n",
    "    early_stop_min_delta = 0.0001\n",
    "    \n",
    "    # System configuration\n",
    "    random_seed = 42\n",
    "    gpu_device = '7'\n",
    "    print_freq = 50\n",
    "    \n",
    "    # Path configuration\n",
    "    data_dir = '/data/jinming/ee_prediction/data'\n",
    "    model_save_dir = '/data/jinming/ee_prediction/iTransformer/models'\n",
    "    results_dir = '/data/jinming/ee_prediction/iTransformer/results'\n",
    "\n",
    "# Create configuration instance\n",
    "config = iTransformerConfig()\n",
    "\n",
    "print(\"=== iTransformer Configuration ===\")\n",
    "print(f\"Time Configuration: {config.seq_length} points -> {config.pred_length} points\")\n",
    "print(f\"Model: d_model={config.d_model}, nhead={config.nhead}, layers={config.num_encoder_layers}\")\n",
    "print(f\"Training: batch_size={config.batch_size}, lr={config.learning_rate}, epochs={config.num_epochs}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "447a59ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda (GPU 7)\n",
      "GPU device: NVIDIA RTX A6000\n",
      "GPU memory: 47.44 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU setup\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = config.gpu_device\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(config.random_seed)\n",
    "torch.manual_seed(config.random_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(config.random_seed)\n",
    "    torch.cuda.manual_seed_all(config.random_seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device} (GPU {config.gpu_device})')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU device: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e014c267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iTransformer model class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# iTransformer Model Definition\n",
    "# Based on \"iTransformer: Inverted Transformers Are Effective for Time Series Forecasting\"\n",
    "\n",
    "class VariableEmbedding(nn.Module):\n",
    "    \"\"\"Variable-wise embedding for iTransformer\"\"\"\n",
    "    def __init__(self, seq_len, d_model):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Linear projection from sequence length to model dimension\n",
    "        self.projection = nn.Linear(seq_len, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, n_vars]\n",
    "        # Transpose to [batch_size, n_vars, seq_len]\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # Project each variable independently: [batch_size, n_vars, d_model]\n",
    "        x = self.projection(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class VariablePositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding for variables instead of time steps\"\"\"\n",
    "    def __init__(self, d_model, max_vars=1000):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Create positional encoding for variables\n",
    "        pe = torch.zeros(max_vars, d_model)\n",
    "        position = torch.arange(0, max_vars, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                           (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # [1, max_vars, d_model]\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, n_vars, d_model]\n",
    "        batch_size, n_vars, d_model = x.shape\n",
    "        x = x + self.pe[:, :n_vars, :]\n",
    "        return x\n",
    "\n",
    "class iTransformerLayer(nn.Module):\n",
    "    \"\"\"Single iTransformer encoder layer\"\"\"\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout, activation):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Multi-head self-attention\n",
    "        self.self_attn = nn.MultiheadAttention(\n",
    "            embed_dim=d_model,\n",
    "            num_heads=nhead,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Self-attention\n",
    "        attn_output, _ = self.self_attn(x, x, x)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed-forward\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_output)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class iTransformer(nn.Module):\n",
    "    \"\"\"iTransformer: Inverted Transformer for Time Series Forecasting\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.seq_len = config.seq_length\n",
    "        self.pred_len = config.pred_length\n",
    "        self.d_model = config.d_model\n",
    "        \n",
    "        # Input feature dimension (will be set dynamically)\n",
    "        self.n_vars = getattr(config, 'input_size', 1)\n",
    "        \n",
    "        # Variable embedding: project each variable from seq_len to d_model\n",
    "        self.variable_embedding = VariableEmbedding(self.seq_len, self.d_model)\n",
    "        \n",
    "        # Variable positional encoding\n",
    "        self.variable_pos_encoding = VariablePositionalEncoding(self.d_model)\n",
    "        \n",
    "        # Transformer encoder layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            iTransformerLayer(\n",
    "                d_model=config.d_model,\n",
    "                nhead=config.nhead,\n",
    "                dim_feedforward=config.dim_feedforward,\n",
    "                dropout=config.dropout,\n",
    "                activation=config.activation\n",
    "            )\n",
    "            for _ in range(config.num_encoder_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output projection: from d_model to prediction length\n",
    "        self.output_projection = nn.Linear(self.d_model, self.pred_len)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize model weights\"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name and param.dim() > 1:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "                \n",
    "    def update_input_size(self, input_size):\n",
    "        \"\"\"Dynamically update input feature dimension\"\"\"\n",
    "        self.n_vars = input_size\n",
    "        # No need to recreate layers since iTransformer handles variable dimensions automatically\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, n_vars]\n",
    "        batch_size, seq_len, n_vars = x.shape\n",
    "        \n",
    "        # Variable embedding: [batch_size, n_vars, d_model]\n",
    "        x = self.variable_embedding(x)\n",
    "        \n",
    "        # Add variable positional encoding\n",
    "        x = self.variable_pos_encoding(x)\n",
    "        \n",
    "        # Pass through encoder layers\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Output projection: [batch_size, n_vars, pred_len]\n",
    "        output = self.output_projection(x)\n",
    "        \n",
    "        # For univariate prediction, use the first variable (power)\n",
    "        # For multivariate, you might want to use specific variables or aggregation\n",
    "        if n_vars == 1:\n",
    "            output = output.squeeze(1)  # [batch_size, pred_len]\n",
    "        else:\n",
    "            # Use the first variable (original power) as target\n",
    "            output = output[:, 0, :]  # [batch_size, pred_len]\n",
    "            \n",
    "        return output\n",
    "\n",
    "print(\"iTransformer model class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e57de0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data length: 6999\n",
      "Testing data length: 2999\n",
      "Training data range: [-0.0094, 1.0000]\n",
      "Testing data range: [-0.0093, 0.9446]\n",
      "\n",
      "=== Data Preprocessing ===\n",
      "Converted 445 negative values to 0\n",
      "Converted 138 negative values to 0\n",
      "Final training data range: [0.0000, 1.0000]\n",
      "Final testing data range: [0.0000, 0.9446]\n"
     ]
    }
   ],
   "source": [
    "# Data loading and preprocessing (reuse from previous notebooks)\n",
    "train_data = pd.read_excel(f'{config.data_dir}/train.xlsx')\n",
    "test_data = pd.read_excel(f'{config.data_dir}/test.xlsx')\n",
    "\n",
    "train_power = train_data.iloc[:, 0].values\n",
    "test_power = test_data.iloc[:, 0].values\n",
    "\n",
    "print(f\"Training data length: {len(train_power)}\")\n",
    "print(f\"Testing data length: {len(test_power)}\")\n",
    "print(f\"Training data range: [{train_power.min():.4f}, {train_power.max():.4f}]\")\n",
    "print(f\"Testing data range: [{test_power.min():.4f}, {test_power.max():.4f}]\")\n",
    "\n",
    "# Simplified data preprocessing\n",
    "def simplified_data_preprocessing(data, config):\n",
    "    processed_data = data.copy()\n",
    "    processed_data = np.where(processed_data < 0, 0, processed_data)\n",
    "    \n",
    "    negative_count = np.sum(data < 0)\n",
    "    if negative_count > 0:\n",
    "        print(f\"Converted {negative_count} negative values to 0\")\n",
    "    else:\n",
    "        print(\"No negative values found\")\n",
    "    \n",
    "    return processed_data, None\n",
    "\n",
    "print(\"\\n=== Data Preprocessing ===\")\n",
    "train_power_processed, _ = simplified_data_preprocessing(train_power, config)\n",
    "test_power_processed, _ = simplified_data_preprocessing(test_power, config)\n",
    "\n",
    "print(f\"Final training data range: [{train_power_processed.min():.4f}, {train_power_processed.max():.4f}]\")\n",
    "print(f\"Final testing data range: [{test_power_processed.min():.4f}, {test_power_processed.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2465b266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training dataset...\n",
      "Created 57 features: 57 total\n",
      "Feature categories: {'original': 1, 'lag': 8, 'ramp': 9, 'other': 3, 'statistical': 30, 'anomaly': 4, 'trend': 2}\n",
      "Training dataset created successfully!\n",
      "Dataset length: 6888\n",
      "Features shape: (6999, 57)\n",
      "Input sequences shape: (6888, 96, 57)\n",
      "Target sequences shape: (6888, 16)\n",
      "\n",
      "Creating test dataset...\n",
      "Created 57 features: 57 total\n",
      "Feature categories: {'original': 1, 'lag': 8, 'ramp': 9, 'other': 3, 'statistical': 30, 'anomaly': 4, 'trend': 2}\n",
      "Test dataset created successfully!\n",
      "Dataset length: 2888\n",
      "Features shape: (2999, 57)\n",
      "Input sequences shape: (2888, 96, 57)\n",
      "Target sequences shape: (2888, 16)\n",
      "\n",
      "Updated config.input_size to: 57\n",
      "\n",
      "Feature Engineering Summary:\n",
      "✅ Total features created: 57\n",
      "✅ Feature categories: {'original': 1, 'lag': 8, 'ramp': 9, 'other': 3, 'statistical': 30, 'anomaly': 4, 'trend': 2}\n",
      "\n",
      "Feature Statistics (Raw Features - No Normalization):\n",
      "Features range: [-1.000000, 1.000000]\n",
      "Features mean: 0.167196\n",
      "Features std: 0.321856\n",
      "Target values range: [0.000000, 1.000000]\n",
      "\n",
      "Data Quality Check:\n",
      "Has NaN values: False\n",
      "Has Inf values: False\n",
      "Has very large values (>1e6): False\n",
      "✅ No problematic values found\n",
      "\n",
      "============================================================\n",
      "CREATING DATA LOADERS\n",
      "Training set size: 5510\n",
      "Validation set size: 1378\n",
      "Testing set size: 2888\n",
      "Total features: 57\n",
      "Feature categories: {'original': 1, 'lag': 8, 'ramp': 9, 'other': 3, 'statistical': 30, 'anomaly': 4, 'trend': 2}\n",
      "Ready for training!\n"
     ]
    }
   ],
   "source": [
    "# 数据集类定义\n",
    "class EnhancedPowerDataset(Dataset):\n",
    "    def __init__(self, data, seq_length=96, pred_length=16, step=1, config=None):\n",
    "        \"\"\"\n",
    "        增强的电力数据集类（支持全面特征工程）\n",
    "        \n",
    "        参数:\n",
    "        data: 原始时间序列数据\n",
    "        seq_length: 输入序列长度\n",
    "        pred_length: 预测序列长度\n",
    "        step: 滑动窗口步长\n",
    "        config: 配置对象\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.seq_length = seq_length\n",
    "        self.pred_length = pred_length\n",
    "        self.step = step\n",
    "        self.config = config\n",
    "        \n",
    "        # 创建特征\n",
    "        if config and config.use_feature_engineering:\n",
    "            self.features, self.feature_names = self._create_enhanced_features()\n",
    "            print(f\"Created {self.features.shape[1]} features: {len(self.feature_names)} total\")\n",
    "            print(f\"Feature categories: {self._get_feature_categories()}\")\n",
    "            \n",
    "            # 特征选择（如果启用）\n",
    "            if getattr(config, 'enable_feature_selection', False) and self.features.shape[1] > config.max_features:\n",
    "                self.features, self.feature_names = self._apply_feature_selection()\n",
    "                print(f\"After feature selection: {self.features.shape[1]} features\")\n",
    "        else:\n",
    "            self.features = data.reshape(-1, 1)\n",
    "            self.feature_names = ['power']\n",
    "        \n",
    "        # 创建输入输出对\n",
    "        self.X, self.y = self._create_sequences()\n",
    "\n",
    "    def _get_feature_categories(self):\n",
    "        \"\"\"获取特征类别统计\"\"\"\n",
    "        categories = {}\n",
    "        for name in self.feature_names:\n",
    "            if name == 'power':\n",
    "                category = 'original'\n",
    "            elif name.startswith('lag_'):\n",
    "                category = 'lag'\n",
    "            elif name.startswith('ramp_'):\n",
    "                category = 'ramp'\n",
    "            elif name.startswith('rolling_'):\n",
    "                category = 'statistical'\n",
    "            elif 'outlier' in name or 'anomaly' in name:\n",
    "                category = 'anomaly'\n",
    "            elif name.endswith(('_sin', '_cos')) or 'time' in name or 'hour' in name or 'weekday' in name or 'month' in name or 'season' in name:\n",
    "                category = 'temporal'\n",
    "            elif name.startswith(('diff_', 'pct_', 'cumulative_', 'relative_')):\n",
    "                category = 'trend'\n",
    "            elif name.startswith('power_level_') or name in ['zero_power', 'full_power', 'power_level_change']:\n",
    "                category = 'power_state'\n",
    "            elif name.startswith(('volatility_', 'cv_', 'skewness_', 'kurtosis_')):\n",
    "                category = 'volatility'\n",
    "            elif name.startswith(('diff_historical_', 'ratio_historical_')) or name == 'is_weekend':\n",
    "                category = 'cross_temporal'\n",
    "            elif name in ['dominant_frequency', 'spectral_energy']:\n",
    "                category = 'frequency'\n",
    "            else:\n",
    "                category = 'other'\n",
    "            \n",
    "            categories[category] = categories.get(category, 0) + 1\n",
    "        \n",
    "        return categories\n",
    "\n",
    "\n",
    "    def add_trend_features(self, power_series):\n",
    "        \"\"\"添加趋势和变化率特征（归一化版本）\"\"\"\n",
    "        features = []\n",
    "        feature_names = []\n",
    "        \n",
    "        # 一阶差分（变化率）- 归一化\n",
    "        diff_1 = power_series.diff(1).fillna(0)\n",
    "        # 裁剪到合理范围并归一化到[-1, 1]\n",
    "        diff_1_normalized = np.clip(diff_1.values, -0.5, 0.5)\n",
    "        features.append(diff_1_normalized)\n",
    "        feature_names.append('diff_1')\n",
    "        \n",
    "        # 二阶差分（加速度）- 归一化\n",
    "        diff_2 = power_series.diff(2).fillna(0)\n",
    "        # 裁剪到合理范围并归一化到[-1, 1]\n",
    "        diff_2_normalized = np.clip(diff_2.values, -0.3, 0.3)\n",
    "        features.append(diff_2_normalized)\n",
    "        feature_names.append('diff_2')\n",
    "        \n",
    "        return features, feature_names\n",
    "\n",
    "\n",
    "    def add_statistical_features(self, power_series):\n",
    "        \"\"\"添加统计滚动窗口特征（使用config中的窗口参数）\"\"\"\n",
    "        features = []\n",
    "        feature_names = []\n",
    "        \n",
    "        # 使用config中的统计窗口参数\n",
    "        windows = self.config.statistical_windows\n",
    "        \n",
    "        for window in windows:\n",
    "            # 滚动均值\n",
    "            if self.config.enable_rolling_mean:\n",
    "                rolling_mean = power_series.rolling(window=window, min_periods=1).mean().fillna(0).values\n",
    "                features.append(rolling_mean)\n",
    "                feature_names.append(f'rolling_mean_{window}')\n",
    "            \n",
    "            # 滚动标准差\n",
    "            if self.config.enable_rolling_std:\n",
    "                rolling_std = power_series.rolling(window=window, min_periods=1).std().fillna(0).values\n",
    "                features.append(rolling_std)\n",
    "                feature_names.append(f'rolling_std_{window}')\n",
    "            \n",
    "            # 滚动最大值\n",
    "            if self.config.enable_rolling_max:\n",
    "                rolling_max = power_series.rolling(window=window, min_periods=1).max().fillna(0).values\n",
    "                features.append(rolling_max)\n",
    "                feature_names.append(f'rolling_max_{window}')\n",
    "            \n",
    "            # 滚动最小值\n",
    "            if self.config.enable_rolling_min:\n",
    "                rolling_min = power_series.rolling(window=window, min_periods=1).min().fillna(0).values\n",
    "                features.append(rolling_min)\n",
    "                feature_names.append(f'rolling_min_{window}')\n",
    "            \n",
    "            # 滚动范围（max - min）\n",
    "            if self.config.enable_rolling_range and self.config.enable_rolling_max and self.config.enable_rolling_min:\n",
    "                rolling_max = power_series.rolling(window=window, min_periods=1).max().fillna(0).values\n",
    "                rolling_min = power_series.rolling(window=window, min_periods=1).min().fillna(0).values\n",
    "                rolling_range = rolling_max - rolling_min\n",
    "                features.append(rolling_range)\n",
    "                feature_names.append(f'rolling_range_{window}')\n",
    "        \n",
    "        return features, feature_names\n",
    "\n",
    "    def add_anomaly_features(self, power_series):\n",
    "        \"\"\"添加异常检测特征（使用config中的窗口参数）\"\"\"\n",
    "        features = []\n",
    "        feature_names = []\n",
    "        \n",
    "        # Z-score异常检测\n",
    "        z_scores = np.abs((power_series - power_series.mean()) / power_series.std())\n",
    "        is_outlier = (z_scores > 2).astype(int).values  # 2个标准差外为异常\n",
    "        features.append(is_outlier)\n",
    "        feature_names.append('is_outlier')\n",
    "        \n",
    "        # IQR异常检测\n",
    "        Q1 = power_series.quantile(0.25)\n",
    "        Q3 = power_series.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        is_iqr_outlier = ((power_series < (Q1 - 1.5 * IQR)) | \n",
    "                          (power_series > (Q3 + 1.5 * IQR))).astype(int).values\n",
    "        features.append(is_iqr_outlier)\n",
    "        feature_names.append('is_iqr_outlier')\n",
    "        \n",
    "        # 局部异常因子（使用config中的异常窗口参数）\n",
    "        windows = self.config.anomaly_windows\n",
    "        for window in windows:\n",
    "            local_mean = power_series.rolling(window=window, center=True).mean()\n",
    "            local_std = power_series.rolling(window=window, center=True).std()\n",
    "            local_anomaly = np.abs(power_series - local_mean) > (2 * local_std)\n",
    "            features.append(local_anomaly.fillna(0).astype(int).values)\n",
    "            feature_names.append(f'local_anomaly_{window}')\n",
    "        \n",
    "        return features, feature_names\n",
    "    \n",
    "    def _apply_feature_selection(self):\n",
    "        \"\"\"应用特征选择\"\"\"\n",
    "        from sklearn.feature_selection import VarianceThreshold\n",
    "        \n",
    "        # 方差阈值过滤\n",
    "        selector = VarianceThreshold(threshold=0.001)\n",
    "        features_filtered = selector.fit_transform(self.features)\n",
    "        selected_features = [name for i, name in enumerate(self.feature_names) \n",
    "                            if selector.get_support()[i]]\n",
    "        \n",
    "        # 如果仍然超过最大特征数，随机选择\n",
    "        if len(selected_features) > self.config.max_features:\n",
    "            np.random.seed(self.config.random_seed)\n",
    "            indices = np.random.choice(len(selected_features), self.config.max_features, replace=False)\n",
    "            features_filtered = features_filtered[:, indices]\n",
    "            selected_features = [selected_features[i] for i in indices]\n",
    "        \n",
    "        return features_filtered, selected_features\n",
    "    \n",
    "    def _create_enhanced_features(self):\n",
    "        \"\"\"创建增强的特征（完整版）\"\"\"\n",
    "        features = []\n",
    "        feature_names = []\n",
    "        \n",
    "        power_series = pd.Series(self.data)\n",
    "        \n",
    "        # 1. 原始功率数据\n",
    "        features.append(self.data)\n",
    "        feature_names.append('power')\n",
    "        \n",
    "        # 2. 滞后特征\n",
    "        if self.config.add_lag_features:\n",
    "            for lag in self.config.lag_steps:\n",
    "                if lag < len(self.data):\n",
    "                    lag_feature = np.roll(self.data, lag)\n",
    "                    lag_feature[:lag] = lag_feature[lag]\n",
    "                    features.append(lag_feature)\n",
    "                    feature_names.append(f'lag_{lag}')\n",
    "        \n",
    "        # 3. 功率爬坡特征\n",
    "        if self.config.add_ramp_features:\n",
    "            for window in self.config.ramp_windows:\n",
    "                # 功率爬坡率：单位时间内功率变化幅度\n",
    "                ramp_rate = power_series.diff(window).fillna(0).values\n",
    "                features.append(ramp_rate)\n",
    "                feature_names.append(f'ramp_rate_{window}')\n",
    "                \n",
    "                # 绝对爬坡率\n",
    "                abs_ramp_rate = np.abs(ramp_rate)\n",
    "                features.append(abs_ramp_rate)\n",
    "                feature_names.append(f'abs_ramp_rate_{window}')\n",
    "                \n",
    "                # 功率爬坡强度（滚动窗口内最大绝对变化）\n",
    "                ramp_intensity = power_series.diff().abs().rolling(window=window, min_periods=1).max().fillna(0).values\n",
    "                features.append(ramp_intensity)\n",
    "                feature_names.append(f'ramp_intensity_{window}')\n",
    "                \n",
    "                # 爬坡方向（上升=1，下降=-1，稳定=0）\n",
    "                ramp_direction = np.sign(ramp_rate)\n",
    "                features.append(ramp_direction)\n",
    "                feature_names.append(f'ramp_direction_{window}')\n",
    "        \n",
    "        # 4. 统计滚动特征\n",
    "        if self.config.add_statistical_features:\n",
    "            stat_features, stat_names = self.add_statistical_features(power_series)\n",
    "            features.extend(stat_features)\n",
    "            feature_names.extend(stat_names)\n",
    "\n",
    "        # 5. 异常检测特征\n",
    "        if self.config.add_anomaly_features:\n",
    "            anomaly_features, anomaly_names = self.add_anomaly_features(power_series)\n",
    "            features.extend(anomaly_features)\n",
    "            feature_names.extend(anomaly_names)\n",
    "        \n",
    "        \n",
    "        # 6. 新增：趋势特征\n",
    "        if getattr(self.config, 'add_trend_features', True):\n",
    "            trend_features, trend_names = self.add_trend_features(power_series)\n",
    "            features.extend(trend_features)\n",
    "            feature_names.extend(trend_names)\n",
    "        \n",
    "    \n",
    "        return np.array(features).T, feature_names  # [time_steps, num_features]\n",
    "    \n",
    "    def _create_sequences(self):\n",
    "        X, y = [], []\n",
    "        for i in range(0, len(self.data) - self.seq_length - self.pred_length + 1, self.step):\n",
    "            # 多特征输入序列\n",
    "            X.append(self.features[i:i + self.seq_length])\n",
    "            # 输出序列（仍然是原始功率数据）\n",
    "            y.append(self.data[i + self.seq_length:i + self.seq_length + self.pred_length])\n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.FloatTensor(self.X[idx]), torch.FloatTensor(self.y[idx])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 创建训练数据集\n",
    "print(\"Creating training dataset...\")\n",
    "train_dataset = EnhancedPowerDataset(\n",
    "    data=train_power_processed,\n",
    "    seq_length=config.seq_length,\n",
    "    pred_length=config.pred_length,\n",
    "    step=1,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(f\"Training dataset created successfully!\")\n",
    "print(f\"Dataset length: {len(train_dataset)}\")\n",
    "print(f\"Features shape: {train_dataset.features.shape}\")\n",
    "print(f\"Input sequences shape: {train_dataset.X.shape}\")\n",
    "print(f\"Target sequences shape: {train_dataset.y.shape}\")\n",
    "\n",
    "# 创建测试数据集\n",
    "print(\"\\nCreating test dataset...\")\n",
    "test_dataset = EnhancedPowerDataset(\n",
    "    data=test_power_processed,\n",
    "    seq_length=config.seq_length,\n",
    "    pred_length=config.pred_length,\n",
    "    step=1,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(f\"Test dataset created successfully!\")\n",
    "print(f\"Dataset length: {len(test_dataset)}\")\n",
    "print(f\"Features shape: {test_dataset.features.shape}\")\n",
    "print(f\"Input sequences shape: {test_dataset.X.shape}\")\n",
    "print(f\"Target sequences shape: {test_dataset.y.shape}\")\n",
    "\n",
    "# 更新config中的input_size\n",
    "config.input_size = train_dataset.features.shape[1]\n",
    "print(f\"\\nUpdated config.input_size to: {config.input_size}\")\n",
    "\n",
    "# 显示特征统计信息\n",
    "print(f\"\\nFeature Engineering Summary:\")\n",
    "print(f\"✅ Total features created: {len(train_dataset.feature_names)}\")\n",
    "print(f\"✅ Feature categories: {train_dataset._get_feature_categories()}\")\n",
    "\n",
    "# 检查特征范围（不进行归一化）\n",
    "print(f\"\\nFeature Statistics (Raw Features - No Normalization):\")\n",
    "print(f\"Features range: [{train_dataset.features.min():.6f}, {train_dataset.features.max():.6f}]\")\n",
    "print(f\"Features mean: {train_dataset.features.mean():.6f}\")\n",
    "print(f\"Features std: {train_dataset.features.std():.6f}\")\n",
    "print(f\"Target values range: [{train_dataset.data.min():.6f}, {train_dataset.data.max():.6f}]\")\n",
    "\n",
    "# 检查是否有异常值\n",
    "has_nan = np.isnan(train_dataset.features).any()\n",
    "has_inf = np.isinf(train_dataset.features).any()\n",
    "has_very_large = np.any(np.abs(train_dataset.features) > 1e6)\n",
    "\n",
    "print(f\"\\nData Quality Check:\")\n",
    "print(f\"Has NaN values: {has_nan}\")\n",
    "print(f\"Has Inf values: {has_inf}\")\n",
    "print(f\"Has very large values (>1e6): {has_very_large}\")\n",
    "\n",
    "if has_nan or has_inf or has_very_large:\n",
    "    print(\"⚠️  WARNING: Found problematic values in features!\")\n",
    "    print(\"🔧 Applying safe replacement (NaN/Inf → 0)...\")\n",
    "    train_dataset.features = np.nan_to_num(train_dataset.features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    test_dataset.features = np.nan_to_num(test_dataset.features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    # 重新创建序列\n",
    "    train_dataset.X, train_dataset.y = train_dataset._create_sequences()\n",
    "    test_dataset.X, test_dataset.y = test_dataset._create_sequences()\n",
    "    print(\"✅ Problematic values replaced, sequences recreated\")\n",
    "else:\n",
    "    print(\"✅ No problematic values found\")\n",
    "\n",
    "\n",
    "\n",
    "# 创建数据加载器\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING DATA LOADERS\")\n",
    "\n",
    "\n",
    "# 分割训练集和验证集\n",
    "train_size = int(config.train_val_split * len(train_dataset))\n",
    "train_indices = list(range(train_size))\n",
    "val_indices = list(range(train_size, len(train_dataset)))\n",
    "\n",
    "train_dataset_split = torch.utils.data.Subset(train_dataset, train_indices)\n",
    "val_dataset = torch.utils.data.Subset(train_dataset, val_indices)\n",
    "\n",
    "# 创建数据加载器\n",
    "train_loader = DataLoader(train_dataset_split, batch_size=config.batch_size, shuffle=True, num_workers=config.num_workers)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=config.num_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=config.num_workers)\n",
    "\n",
    "print(f\"Training set size: {len(train_dataset_split)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")\n",
    "print(f\"Testing set size: {len(test_dataset)}\")\n",
    "\n",
    "print(f\"Total features: {len(train_dataset.feature_names)}\")\n",
    "print(f\"Feature categories: {train_dataset._get_feature_categories()}\")\n",
    "print(f\"Ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69394c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== iTransformer Model Information ===\n",
      "Input sequence length: 96\n",
      "Input features: 57\n",
      "Model dimension: 64\n",
      "Attention heads: 8\n",
      "Encoder layers: 2\n",
      "Total parameters: 107,216\n",
      "Trainable parameters: 107,216\n",
      "==================================================\n",
      "Test forward pass - Input shape: torch.Size([2, 96, 57]), Output shape: torch.Size([2, 16])\n"
     ]
    }
   ],
   "source": [
    "# Create iTransformer model\n",
    "model = iTransformer(config).to(device)\n",
    "\n",
    "# Update model's input dimension\n",
    "model.update_input_size(config.input_size)\n",
    "\n",
    "# Model information\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"=== iTransformer Model Information ===\")\n",
    "print(f\"Input sequence length: {config.seq_length}\")\n",
    "print(f\"Input features: {config.input_size}\")\n",
    "print(f\"Model dimension: {config.d_model}\")\n",
    "print(f\"Attention heads: {config.nhead}\")\n",
    "print(f\"Encoder layers: {config.num_encoder_layers}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test forward pass\n",
    "with torch.no_grad():\n",
    "    sample_input = torch.randn(2, config.seq_length, config.input_size).to(device)\n",
    "    sample_output = model(sample_input)\n",
    "    print(f\"Test forward pass - Input shape: {sample_input.shape}, Output shape: {sample_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c35933cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training setup completed!\n",
      "Optimizer: AdamW (lr=0.0001\n",
      "Loss function: Huber Loss\n",
      "LR Scheduler: ReduceLROnPlateau (patience=8)\n",
      "Early stopping: patience=50\n"
     ]
    }
   ],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.HuberLoss(delta=1.0)\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=config.learning_rate, \n",
    "    betas=(0.9, 0.95)  # Better for Transformers\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=config.lr_factor, \n",
    "    patience=config.lr_patience, verbose=True, min_lr=1e-6\n",
    ")\n",
    "\n",
    "# CR metric calculation\n",
    "def calculate_CR(PM, PP):\n",
    "    N = len(PM)\n",
    "    Ri = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        if PM[i] > 0.2:\n",
    "            Ri[i] = (PM[i] - PP[i]) / PM[i]\n",
    "        else:\n",
    "            Ri[i] = (PM[i] - PP[i]) / 0.2\n",
    "    rms_error = np.sqrt(np.mean(Ri**2))\n",
    "    CR = (1 - rms_error) * 100\n",
    "    return CR\n",
    "\n",
    "# Early stopping mechanism\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=15, min_delta=0.0001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_cr = None\n",
    "        self.counter = 0\n",
    "        self.best_weights = None\n",
    "        \n",
    "    def __call__(self, val_cr, model):\n",
    "        if self.best_cr is None:\n",
    "            self.best_cr = val_cr\n",
    "            self.save_checkpoint(model)\n",
    "        elif val_cr > self.best_cr + self.min_delta:\n",
    "            self.best_cr = val_cr\n",
    "            self.counter = 0\n",
    "            self.save_checkpoint(model)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            \n",
    "        return self.counter >= self.patience\n",
    "    \n",
    "    def save_checkpoint(self, model):\n",
    "        self.best_weights = model.state_dict().copy()\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=config.early_stop_patience, \n",
    "    min_delta=config.early_stop_min_delta\n",
    ")\n",
    "\n",
    "print(f\"Training setup completed!\")\n",
    "print(f\"Optimizer: AdamW (lr={config.learning_rate}\")\n",
    "print(f\"Loss function: Huber Loss\")\n",
    "print(f\"LR Scheduler: ReduceLROnPlateau (patience={config.lr_patience})\")\n",
    "print(f\"Early stopping: patience={config.early_stop_patience}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be1a4295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# Training and validation functions\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device, config):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % config.print_freq == 0:\n",
    "            print(f'  Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.6f}')\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "def calculate_validation_cr(model, val_loader, device):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            \n",
    "            all_predictions.append(output.cpu().numpy())\n",
    "            all_targets.append(target.cpu().numpy())\n",
    "    \n",
    "    predictions = np.concatenate(all_predictions, axis=0)\n",
    "    targets = np.concatenate(all_targets, axis=0)\n",
    "    \n",
    "    # Calculate CR on all prediction points (correct approach)\n",
    "    cr_overall = calculate_CR(targets.flatten(), predictions.flatten())\n",
    "    return cr_overall\n",
    "\n",
    "print(\"Training functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e524c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting iTransformer Training ===\n",
      "Model: iTransformer\n",
      "Training epochs: 300\n",
      "Batch size: 128\n",
      "Learning rate: 0.0001\n",
      "Model parameters: 107,216\n",
      "==================================================\n",
      "\n",
      "Epoch 1/300\n",
      "  Batch 0/44, Loss: 0.529845\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_cr_scores = []\n",
    "\n",
    "print(\"=== Starting iTransformer Training ===\")\n",
    "print(f\"Model: iTransformer\")\n",
    "print(f\"Training epochs: {config.num_epochs}\")\n",
    "print(f\"Batch size: {config.batch_size}\")\n",
    "print(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "print(f\"Model parameters: {total_params:,}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "best_epoch = 0\n",
    "for epoch in range(config.num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{config.num_epochs}\")\n",
    "    \n",
    "    # Training\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device, config)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validation\n",
    "    val_loss = validate_epoch(model, val_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Calculate validation CR metric\n",
    "    val_cr = calculate_validation_cr(model, val_loader, device)\n",
    "    val_cr_scores.append(val_cr)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "    print(f\"Validation CR: {val_cr:.2f}%\")\n",
    "    print(f\"Current LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    if early_stopping(val_cr, model):\n",
    "        print(f\"\\nEarly stopping triggered! Training stopped at epoch {epoch+1}\")\n",
    "        print(f\"Best CR metric: {early_stopping.best_cr:.2f}%\")\n",
    "        best_epoch = epoch + 1\n",
    "        break\n",
    "    \n",
    "    if val_cr == early_stopping.best_cr:\n",
    "        best_epoch = epoch + 1\n",
    "\n",
    "print(\"\\n=== Training Completed ===\")\n",
    "print(f\"Best validation CR: {early_stopping.best_cr:.2f}% (Epoch {best_epoch})\")\n",
    "print(f\"Total epochs trained: {len(train_losses)}\")\n",
    "\n",
    "# Restore best weights\n",
    "if early_stopping.best_weights is not None:\n",
    "    model.load_state_dict(early_stopping.best_weights)\n",
    "    print(\"Restored best model weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f5faeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss', color='blue', linewidth=2)\n",
    "plt.plot(val_losses, label='Validation Loss', color='red', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('iTransformer Training Progress - Loss Curves')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(val_cr_scores, label='Validation CR', color='green', linewidth=2)\n",
    "plt.axhline(y=early_stopping.best_cr, color='red', linestyle='--', \n",
    "           label=f'Best CR: {early_stopping.best_cr:.2f}%')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('CR Metric (%)')\n",
    "plt.title('iTransformer Training Progress - CR Metric')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save model\n",
    "model_save_path = f'{config.model_save_dir}/itransformer_16_point_best_model.pth'\n",
    "os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'val_cr_scores': val_cr_scores,\n",
    "    'best_cr': early_stopping.best_cr,\n",
    "    'best_epoch': best_epoch,\n",
    "    'config': config.__dict__,\n",
    "    'feature_names': train_dataset.feature_names,\n",
    "    'model_info': {\n",
    "        'model_type': 'iTransformer',\n",
    "        'd_model': config.d_model,\n",
    "        'nhead': config.nhead,\n",
    "        'num_layers': config.num_encoder_layers,\n",
    "        'total_params': total_params,\n",
    "        'input_size': config.input_size\n",
    "    }\n",
    "}, model_save_path)\n",
    "\n",
    "print(f\"\\nModel saved to: {model_save_path}\")\n",
    "print(f\"Best CR metric: {early_stopping.best_cr:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08a4166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model performance\n",
    "def test_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            \n",
    "            all_predictions.append(output.cpu().numpy())\n",
    "            all_targets.append(target.cpu().numpy())\n",
    "    \n",
    "    predictions = np.concatenate(all_predictions, axis=0)\n",
    "    targets = np.concatenate(all_targets, axis=0)\n",
    "    \n",
    "    return predictions, targets\n",
    "\n",
    "print(\"=== Testing iTransformer Model ===\")\n",
    "predictions, targets = test_model(model, test_loader, device)\n",
    "\n",
    "print(f\"Prediction shape: {predictions.shape}\")\n",
    "print(f\"Target shape: {targets.shape}\")\n",
    "\n",
    "# Calculate step-wise metrics first\n",
    "print(f\"\\n=== Step-wise Test Results ===\")\n",
    "step_mse = []\n",
    "step_mae = []\n",
    "step_cr = []\n",
    "\n",
    "for step in range(16):\n",
    "    step_targets = targets[:, step]\n",
    "    step_predictions = predictions[:, step]\n",
    "    \n",
    "    mse = mean_squared_error(step_targets, step_predictions)\n",
    "    mae = mean_absolute_error(step_targets, step_predictions)\n",
    "    cr = calculate_CR(step_targets, step_predictions)\n",
    "    \n",
    "    step_mse.append(mse)\n",
    "    step_mae.append(mae)\n",
    "    step_cr.append(cr)\n",
    "    \n",
    "    print(f\"Step {step+1:2d}: MSE={mse:.6f}, MAE={mae:.6f}, CR={cr:.2f}%\")\n",
    "\n",
    "# 16个时间尺度的平均指标\n",
    "mse_average_across_steps = np.mean(step_mse)\n",
    "mae_average_across_steps = np.mean(step_mae)\n",
    "rmse_average_across_steps = np.sqrt(mse_average_across_steps)\n",
    "cr_average_across_steps = np.mean(step_cr)\n",
    "\n",
    "print(f\"\\nAverage metrics across 16 time steps:\")\n",
    "print(f\"Average MSE: {mse_average_across_steps:.6f}\")\n",
    "print(f\"Average RMSE: {rmse_average_across_steps:.6f}\")  \n",
    "print(f\"Average MAE: {mae_average_across_steps:.6f}\")\n",
    "print(f\"Average CR: {cr_average_across_steps:.2f}%\")\n",
    "\n",
    "# Overall metrics (flattened calculation)\n",
    "mse_overall = mean_squared_error(targets.flatten(), predictions.flatten())\n",
    "mae_overall = mean_absolute_error(targets.flatten(), predictions.flatten())\n",
    "rmse_overall = np.sqrt(mse_overall)\n",
    "cr_overall = calculate_CR(targets.flatten(), predictions.flatten())\n",
    "\n",
    "print(f\"\\nOverall metrics (flattened):\")\n",
    "print(f\"MSE: {mse_overall:.6f}\")\n",
    "print(f\"RMSE: {rmse_overall:.6f}\")\n",
    "print(f\"MAE: {mae_overall:.6f}\")\n",
    "print(f\"Overall CR metric: {cr_overall:.2f}%\")\n",
    "\n",
    "print(f\"\\n=== Summary Statistics ===\")\n",
    "print(f\"Average step-wise CR: {np.mean(step_cr):.2f}%\")\n",
    "print(f\"Step-wise CR std: {np.std(step_cr):.2f}%\")\n",
    "print(f\"Best step CR: {np.max(step_cr):.2f}% (Step {np.argmax(step_cr)+1})\")\n",
    "print(f\"Worst step CR: {np.min(step_cr):.2f}% (Step {np.argmin(step_cr)+1})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fcd8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results\n",
    "os.makedirs(config.results_dir, exist_ok=True)\n",
    "\n",
    "# Save prediction results\n",
    "results_df = pd.DataFrame({\n",
    "    'targets': targets.flatten(),\n",
    "    'predictions': predictions.flatten(),\n",
    "    'errors': targets.flatten() - predictions.flatten()\n",
    "})\n",
    "results_df.to_csv(f'{config.results_dir}/itransformer_16_point_predictions.csv', index=False)\n",
    "\n",
    "# Save performance metrics\n",
    "performance_summary = {\n",
    "    'model_info': {\n",
    "        'model_type': 'iTransformer',\n",
    "        'd_model': config.d_model,\n",
    "        'nhead': config.nhead,\n",
    "        'num_layers': config.num_encoder_layers,\n",
    "        'dim_feedforward': config.dim_feedforward,\n",
    "        'total_params': int(total_params),\n",
    "        'input_features': int(config.input_size)\n",
    "    },\n",
    "    'overall_metrics': {\n",
    "        'MSE': float(mse_overall),\n",
    "        'RMSE': float(rmse_overall),\n",
    "        'MAE': float(mae_overall),\n",
    "        'CR': float(cr_overall)\n",
    "    },\n",
    "    'step_wise_metrics': {\n",
    "        'step': list(range(1, 17)),\n",
    "        'MSE': [float(x) for x in step_mse],\n",
    "        'MAE': [float(x) for x in step_mae],\n",
    "        'CR': [float(x) for x in step_cr]\n",
    "    },\n",
    "    'training_info': {\n",
    "        'best_validation_cr': float(early_stopping.best_cr),\n",
    "        'best_epoch': int(best_epoch),\n",
    "        'total_epochs': len(train_losses),\n",
    "        'final_lr': float(optimizer.param_groups[0]['lr']),\n",
    "        'early_stopping_criterion': 'CR_based'\n",
    "    },\n",
    "    'hyperparameters': {\n",
    "        'seq_length': config.seq_length,\n",
    "        'pred_length': config.pred_length,\n",
    "        'd_model': config.d_model,\n",
    "        'nhead': config.nhead,\n",
    "        'batch_size': config.batch_size,\n",
    "        'learning_rate': config.learning_rate,\n",
    "        'dropout': config.dropout\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(f'{config.results_dir}/itransformer_16_point_performance.json', 'w') as f:\n",
    "    json.dump(performance_summary, f, indent=2)\n",
    "\n",
    "# Create detailed report\n",
    "performance_report = f\"\"\"\n",
    "# iTransformer Wind Power Prediction - Performance Report\n",
    "\n",
    "## Model Architecture\n",
    "- **Model Type**: iTransformer (Inverted Transformer)\n",
    "- **Input Sequence**: {config.seq_length} points ({config.seq_length * config.time_interval_minutes / 60:.1f} hours)\n",
    "- **Prediction Length**: {config.pred_length} points ({config.pred_length * config.time_interval_minutes / 60:.1f} hours)\n",
    "- **Model Dimension**: {config.d_model}\n",
    "- **Attention Heads**: {config.nhead}\n",
    "- **Encoder Layers**: {config.num_encoder_layers}\n",
    "- **Total Parameters**: {total_params:,}\n",
    "- **Input Features**: {config.input_size}\n",
    "\n",
    "## Training Results\n",
    "- **Best Validation CR**: {early_stopping.best_cr:.2f}% (Epoch {best_epoch})\n",
    "- **Total Training Epochs**: {len(train_losses)}\n",
    "- **Final Learning Rate**: {optimizer.param_groups[0]['lr']:.2e}\n",
    "\n",
    "## Test Set Performance\n",
    "- **Overall CR**: {cr_overall:.2f}%\n",
    "- **RMSE**: {rmse_overall:.6f}\n",
    "- **MAE**: {mae_overall:.6f}\n",
    "- **MSE**: {mse_overall:.6f}\n",
    "\n",
    "## Step-wise Performance Analysis\n",
    "- **Average CR**: {np.mean(step_cr):.2f}%\n",
    "- **CR Standard Deviation**: {np.std(step_cr):.2f}%\n",
    "- **Best Step**: Step {np.argmax(step_cr)+1} (CR: {np.max(step_cr):.2f}%)\n",
    "- **Worst Step**: Step {np.argmin(step_cr)+1} (CR: {np.min(step_cr):.2f}%)\n",
    "\n",
    "## Detailed Step-wise Metrics\n",
    "\"\"\"\n",
    "\n",
    "for i in range(16):\n",
    "    performance_report += f\"\\n### Step {i+1} ({(i+1)*15} minutes ahead)\"\n",
    "    performance_report += f\"\\n- **CR**: {step_cr[i]:.2f}%\"\n",
    "    performance_report += f\"\\n- **RMSE**: {np.sqrt(step_mse[i]):.6f}\"\n",
    "    performance_report += f\"\\n- **MAE**: {step_mae[i]:.6f}\\n\"\n",
    "\n",
    "with open(f'{config.results_dir}/itransformer_16_point_report.md', 'w', encoding='utf-8') as f:\n",
    "    f.write(performance_report)\n",
    "\n",
    "print(\"\\n=== Results Saved ===\")\n",
    "print(f\"- Predictions: {config.results_dir}/itransformer_16_point_predictions.csv\")\n",
    "print(f\"- Performance: {config.results_dir}/itransformer_16_point_performance.json\")\n",
    "print(f\"- Report: {config.results_dir}/itransformer_16_point_report.md\")\n",
    "print(f\"- Model: {model_save_path}\")\n",
    "\n",
    "print(f\"\\n=== Final Summary ===\")\n",
    "print(f\"iTransformer Model Performance:\")\n",
    "print(f\"  - Best Validation CR: {early_stopping.best_cr:.2f}%\")\n",
    "print(f\"  - Test Set CR: {cr_overall:.2f}%\")\n",
    "print(f\"  - Test Set RMSE: {rmse_overall:.6f}\")\n",
    "print(f\"  - Model Parameters: {total_params:,}\")\n",
    "print(f\"  - Training Efficiency: {len(train_losses)} epochs\")\n",
    "print(f\"\\niTransformer Features:\")\n",
    "print(f\"  - Variable-wise attention mechanism\")\n",
    "print(f\"  - Inverted architecture for multivariate time series\")\n",
    "print(f\"  - State-of-the-art performance on forecasting benchmarks\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
