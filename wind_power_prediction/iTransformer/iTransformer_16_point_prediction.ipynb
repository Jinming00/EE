{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d993875",
   "metadata": {},
   "source": [
    "# iTransformer Wind Power Prediction Model\n",
    "\n",
    "This notebook implements an iTransformer (Inverted Transformer) model for 16-point wind power prediction. iTransformer is the state-of-the-art model for multivariate time series forecasting, published in 2024.\n",
    "\n",
    "## Key Features:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c0169a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== iTransformer Configuration ===\n",
      "Time Configuration: 96 points -> 16 points\n",
      "Model: d_model=64, nhead=8, layers=2\n",
      "Training: batch_size=128, lr=0.0001, epochs=300\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ===== iTransformer Configuration =====\n",
    "\n",
    "\n",
    "class iTransformerConfig:\n",
    "    # Data configuration\n",
    "    seq_length = 96          # Input sequence length (96 points, 24 hours, 15min intervals)\n",
    "    pred_length = 16         # Prediction sequence length (16 points, 4 hours)\n",
    "    train_val_split = 0.8    # Train/validation split ratio\n",
    "    \n",
    "    # Data loading\n",
    "    batch_size = 128         \n",
    "    num_workers = 8          # DataLoader workers\n",
    "    \n",
    "    # Feature engineering\n",
    "    use_feature_engineering = True  # ÊòØÂê¶‰ΩøÁî®ÁâπÂæÅÂ∑•Á®ã\n",
    "    time_interval_minutes = 15\n",
    "\n",
    "    add_trend_features = True        # Ë∂ãÂäøÁâπÂæÅ\n",
    "\n",
    "    add_lag_features = True        # Ê∑ªÂä†ÊªûÂêéÁâπÂæÅ\n",
    "    lag_steps = [1, 2, 4, 8, 16, 24, 48, 96]  #\n",
    "    \n",
    "    add_ramp_features = True       # Ê∑ªÂä†ÂäüÁéáÁà¨Âù°ÁâπÂæÅ\n",
    "    ramp_windows = [4, 8, 16]       # Áà¨Âù°Ê£ÄÊµãÁ™óÂè£\n",
    "\n",
    "    add_anomaly_features = True      # ÂºÇÂ∏∏Ê£ÄÊµãÁâπÂæÅ\n",
    "    anomaly_windows = [24, 96]       # ÂºÇÂ∏∏Ê£ÄÊµãÁöÑÊªöÂä®Á™óÂè£Â§ßÂ∞è\n",
    "\n",
    "    add_statistical_features = True  # ÁªüËÆ°ÊªöÂä®ÁâπÂæÅ\n",
    "    statistical_windows = [4, 8, 16, 24, 48, 96]  # ÁªüËÆ°ÁâπÂæÅÁöÑÊªöÂä®Á™óÂè£Â§ßÂ∞è\n",
    "    enable_rolling_mean = True       # ÂêØÁî®ÊªöÂä®ÂùáÂÄº\n",
    "    enable_rolling_std = True        # ÂêØÁî®ÊªöÂä®Ê†áÂáÜÂ∑Æ\n",
    "    enable_rolling_max = True        # ÂêØÁî®ÊªöÂä®ÊúÄÂ§ßÂÄº\n",
    "    enable_rolling_min = True        # ÂêØÁî®ÊªöÂä®ÊúÄÂ∞èÂÄº\n",
    "    enable_rolling_range = True      # ÂêØÁî®ÊªöÂä®ËåÉÂõ¥\n",
    "\n",
    "\n",
    "    # iTransformer model structure\n",
    "    d_model = 64            # TransformerÁª¥Â∫¶ÔºàËæÉÂ∞è‰ª•ÈÄÇÂ∫îÂ∞èÊï∞ÊçÆÔºâ\n",
    "    nhead = 8                # Ê≥®ÊÑèÂäõÂ§¥Êï∞\n",
    "    num_encoder_layers = 2   # ÁºñÁ†ÅÂô®Â±ÇÊï∞\n",
    "    dim_feedforward = 256   # FFN dimension\n",
    "    dropout = 0.1            # Dropout rate\n",
    "    activation = 'gelu'      # Activation function\n",
    "    \n",
    "    # Training configuration\n",
    "    num_epochs = 300         # Training epochs\n",
    "    learning_rate = 0.0001   # Learning rate\n",
    "\n",
    "    huber_delta = 1.0  \n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    lr_patience = 8          # LR scheduler patience\n",
    "    lr_factor = 0.5          # LR decay factor\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stop_patience = 50 \n",
    "    early_stop_min_delta = 0.0001\n",
    "    \n",
    "    # System configuration\n",
    "    random_seed = 42\n",
    "    gpu_device = '7'\n",
    "    print_freq = 50\n",
    "    \n",
    "    # Path configuration\n",
    "    data_dir = '/data/jinming/ee_prediction/data'\n",
    "    model_save_dir = '/data/jinming/ee_prediction/iTransformer/models'\n",
    "    results_dir = '/data/jinming/ee_prediction/iTransformer/results'\n",
    "\n",
    "# Create configuration instance\n",
    "config = iTransformerConfig()\n",
    "\n",
    "print(\"=== iTransformer Configuration ===\")\n",
    "print(f\"Time Configuration: {config.seq_length} points -> {config.pred_length} points\")\n",
    "print(f\"Model: d_model={config.d_model}, nhead={config.nhead}, layers={config.num_encoder_layers}\")\n",
    "print(f\"Training: batch_size={config.batch_size}, lr={config.learning_rate}, epochs={config.num_epochs}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "447a59ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda (GPU 7)\n",
      "GPU device: NVIDIA RTX A6000\n",
      "GPU memory: 47.44 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU setup\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = config.gpu_device\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(config.random_seed)\n",
    "torch.manual_seed(config.random_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(config.random_seed)\n",
    "    torch.cuda.manual_seed_all(config.random_seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device} (GPU {config.gpu_device})')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU device: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e014c267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iTransformer model class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# iTransformer Model Definition\n",
    "# Based on \"iTransformer: Inverted Transformers Are Effective for Time Series Forecasting\"\n",
    "\n",
    "class VariableEmbedding(nn.Module):\n",
    "    \"\"\"Variable-wise embedding for iTransformer\"\"\"\n",
    "    def __init__(self, seq_len, d_model):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Linear projection from sequence length to model dimension\n",
    "        self.projection = nn.Linear(seq_len, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, n_vars]\n",
    "        # Transpose to [batch_size, n_vars, seq_len]\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # Project each variable independently: [batch_size, n_vars, d_model]\n",
    "        x = self.projection(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class VariablePositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding for variables instead of time steps\"\"\"\n",
    "    def __init__(self, d_model, max_vars=1000):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Create positional encoding for variables\n",
    "        pe = torch.zeros(max_vars, d_model)\n",
    "        position = torch.arange(0, max_vars, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                           (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # [1, max_vars, d_model]\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, n_vars, d_model]\n",
    "        batch_size, n_vars, d_model = x.shape\n",
    "        x = x + self.pe[:, :n_vars, :]\n",
    "        return x\n",
    "\n",
    "class iTransformerLayer(nn.Module):\n",
    "    \"\"\"Single iTransformer encoder layer\"\"\"\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout, activation):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Multi-head self-attention\n",
    "        self.self_attn = nn.MultiheadAttention(\n",
    "            embed_dim=d_model,\n",
    "            num_heads=nhead,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Self-attention\n",
    "        attn_output, _ = self.self_attn(x, x, x)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed-forward\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_output)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class iTransformer(nn.Module):\n",
    "    \"\"\"iTransformer: Inverted Transformer for Time Series Forecasting\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.seq_len = config.seq_length\n",
    "        self.pred_len = config.pred_length\n",
    "        self.d_model = config.d_model\n",
    "        \n",
    "        # Input feature dimension (will be set dynamically)\n",
    "        self.n_vars = getattr(config, 'input_size', 1)\n",
    "        \n",
    "        # Variable embedding: project each variable from seq_len to d_model\n",
    "        self.variable_embedding = VariableEmbedding(self.seq_len, self.d_model)\n",
    "        \n",
    "        # Variable positional encoding\n",
    "        self.variable_pos_encoding = VariablePositionalEncoding(self.d_model)\n",
    "        \n",
    "        # Transformer encoder layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            iTransformerLayer(\n",
    "                d_model=config.d_model,\n",
    "                nhead=config.nhead,\n",
    "                dim_feedforward=config.dim_feedforward,\n",
    "                dropout=config.dropout,\n",
    "                activation=config.activation\n",
    "            )\n",
    "            for _ in range(config.num_encoder_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output projection: from d_model to prediction length\n",
    "        self.output_projection = nn.Linear(self.d_model, self.pred_len)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize model weights\"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name and param.dim() > 1:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "                \n",
    "    def update_input_size(self, input_size):\n",
    "        \"\"\"Dynamically update input feature dimension\"\"\"\n",
    "        self.n_vars = input_size\n",
    "        # No need to recreate layers since iTransformer handles variable dimensions automatically\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, n_vars]\n",
    "        batch_size, seq_len, n_vars = x.shape\n",
    "        \n",
    "        # Variable embedding: [batch_size, n_vars, d_model]\n",
    "        x = self.variable_embedding(x)\n",
    "        \n",
    "        # Add variable positional encoding\n",
    "        x = self.variable_pos_encoding(x)\n",
    "        \n",
    "        # Pass through encoder layers\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Output projection: [batch_size, n_vars, pred_len]\n",
    "        output = self.output_projection(x)\n",
    "        \n",
    "        # For univariate prediction, use the first variable (power)\n",
    "        # For multivariate, you might want to use specific variables or aggregation\n",
    "        if n_vars == 1:\n",
    "            output = output.squeeze(1)  # [batch_size, pred_len]\n",
    "        else:\n",
    "            # Use the first variable (original power) as target\n",
    "            output = output[:, 0, :]  # [batch_size, pred_len]\n",
    "            \n",
    "        return output\n",
    "\n",
    "print(\"iTransformer model class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e57de0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data length: 6999\n",
      "Testing data length: 2999\n",
      "Training data range: [-0.0094, 1.0000]\n",
      "Testing data range: [-0.0093, 0.9446]\n",
      "\n",
      "=== Data Preprocessing ===\n",
      "Converted 445 negative values to 0\n",
      "Converted 138 negative values to 0\n",
      "Final training data range: [0.0000, 1.0000]\n",
      "Final testing data range: [0.0000, 0.9446]\n"
     ]
    }
   ],
   "source": [
    "# Data loading and preprocessing (reuse from previous notebooks)\n",
    "train_data = pd.read_excel(f'{config.data_dir}/train.xlsx')\n",
    "test_data = pd.read_excel(f'{config.data_dir}/test.xlsx')\n",
    "\n",
    "train_power = train_data.iloc[:, 0].values\n",
    "test_power = test_data.iloc[:, 0].values\n",
    "\n",
    "print(f\"Training data length: {len(train_power)}\")\n",
    "print(f\"Testing data length: {len(test_power)}\")\n",
    "print(f\"Training data range: [{train_power.min():.4f}, {train_power.max():.4f}]\")\n",
    "print(f\"Testing data range: [{test_power.min():.4f}, {test_power.max():.4f}]\")\n",
    "\n",
    "# Simplified data preprocessing\n",
    "def simplified_data_preprocessing(data, config):\n",
    "    processed_data = data.copy()\n",
    "    processed_data = np.where(processed_data < 0, 0, processed_data)\n",
    "    \n",
    "    negative_count = np.sum(data < 0)\n",
    "    if negative_count > 0:\n",
    "        print(f\"Converted {negative_count} negative values to 0\")\n",
    "    else:\n",
    "        print(\"No negative values found\")\n",
    "    \n",
    "    return processed_data, None\n",
    "\n",
    "print(\"\\n=== Data Preprocessing ===\")\n",
    "train_power_processed, _ = simplified_data_preprocessing(train_power, config)\n",
    "test_power_processed, _ = simplified_data_preprocessing(test_power, config)\n",
    "\n",
    "print(f\"Final training data range: [{train_power_processed.min():.4f}, {train_power_processed.max():.4f}]\")\n",
    "print(f\"Final testing data range: [{test_power_processed.min():.4f}, {test_power_processed.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2465b266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training dataset...\n",
      "Created 57 features: 57 total\n",
      "Feature categories: {'original': 1, 'lag': 8, 'ramp': 9, 'other': 3, 'statistical': 30, 'anomaly': 4, 'trend': 2}\n",
      "Training dataset created successfully!\n",
      "Dataset length: 6888\n",
      "Features shape: (6999, 57)\n",
      "Input sequences shape: (6888, 96, 57)\n",
      "Target sequences shape: (6888, 16)\n",
      "\n",
      "Creating test dataset...\n",
      "Created 57 features: 57 total\n",
      "Feature categories: {'original': 1, 'lag': 8, 'ramp': 9, 'other': 3, 'statistical': 30, 'anomaly': 4, 'trend': 2}\n",
      "Test dataset created successfully!\n",
      "Dataset length: 2888\n",
      "Features shape: (2999, 57)\n",
      "Input sequences shape: (2888, 96, 57)\n",
      "Target sequences shape: (2888, 16)\n",
      "\n",
      "Updated config.input_size to: 57\n",
      "\n",
      "Feature Engineering Summary:\n",
      "‚úÖ Total features created: 57\n",
      "‚úÖ Feature categories: {'original': 1, 'lag': 8, 'ramp': 9, 'other': 3, 'statistical': 30, 'anomaly': 4, 'trend': 2}\n",
      "\n",
      "Feature Statistics (Raw Features - No Normalization):\n",
      "Features range: [-1.000000, 1.000000]\n",
      "Features mean: 0.167196\n",
      "Features std: 0.321856\n",
      "Target values range: [0.000000, 1.000000]\n",
      "\n",
      "Data Quality Check:\n",
      "Has NaN values: False\n",
      "Has Inf values: False\n",
      "Has very large values (>1e6): False\n",
      "‚úÖ No problematic values found\n",
      "\n",
      "============================================================\n",
      "CREATING DATA LOADERS\n",
      "Training set size: 5510\n",
      "Validation set size: 1378\n",
      "Testing set size: 2888\n",
      "Total features: 57\n",
      "Feature categories: {'original': 1, 'lag': 8, 'ramp': 9, 'other': 3, 'statistical': 30, 'anomaly': 4, 'trend': 2}\n",
      "Ready for training!\n"
     ]
    }
   ],
   "source": [
    "# Êï∞ÊçÆÈõÜÁ±ªÂÆö‰πâ\n",
    "class EnhancedPowerDataset(Dataset):\n",
    "    def __init__(self, data, seq_length=96, pred_length=16, step=1, config=None):\n",
    "        \"\"\"\n",
    "        Â¢ûÂº∫ÁöÑÁîµÂäõÊï∞ÊçÆÈõÜÁ±ªÔºàÊîØÊåÅÂÖ®Èù¢ÁâπÂæÅÂ∑•Á®ãÔºâ\n",
    "        \n",
    "        ÂèÇÊï∞:\n",
    "        data: ÂéüÂßãÊó∂Èó¥Â∫èÂàóÊï∞ÊçÆ\n",
    "        seq_length: ËæìÂÖ•Â∫èÂàóÈïøÂ∫¶\n",
    "        pred_length: È¢ÑÊµãÂ∫èÂàóÈïøÂ∫¶\n",
    "        step: ÊªëÂä®Á™óÂè£Ê≠•Èïø\n",
    "        config: ÈÖçÁΩÆÂØπË±°\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.seq_length = seq_length\n",
    "        self.pred_length = pred_length\n",
    "        self.step = step\n",
    "        self.config = config\n",
    "        \n",
    "        # ÂàõÂª∫ÁâπÂæÅ\n",
    "        if config and config.use_feature_engineering:\n",
    "            self.features, self.feature_names = self._create_enhanced_features()\n",
    "            print(f\"Created {self.features.shape[1]} features: {len(self.feature_names)} total\")\n",
    "            print(f\"Feature categories: {self._get_feature_categories()}\")\n",
    "            \n",
    "            # ÁâπÂæÅÈÄâÊã©ÔºàÂ¶ÇÊûúÂêØÁî®Ôºâ\n",
    "            if getattr(config, 'enable_feature_selection', False) and self.features.shape[1] > config.max_features:\n",
    "                self.features, self.feature_names = self._apply_feature_selection()\n",
    "                print(f\"After feature selection: {self.features.shape[1]} features\")\n",
    "        else:\n",
    "            self.features = data.reshape(-1, 1)\n",
    "            self.feature_names = ['power']\n",
    "        \n",
    "        # ÂàõÂª∫ËæìÂÖ•ËæìÂá∫ÂØπ\n",
    "        self.X, self.y = self._create_sequences()\n",
    "\n",
    "    def _get_feature_categories(self):\n",
    "        \"\"\"Ëé∑ÂèñÁâπÂæÅÁ±ªÂà´ÁªüËÆ°\"\"\"\n",
    "        categories = {}\n",
    "        for name in self.feature_names:\n",
    "            if name == 'power':\n",
    "                category = 'original'\n",
    "            elif name.startswith('lag_'):\n",
    "                category = 'lag'\n",
    "            elif name.startswith('ramp_'):\n",
    "                category = 'ramp'\n",
    "            elif name.startswith('rolling_'):\n",
    "                category = 'statistical'\n",
    "            elif 'outlier' in name or 'anomaly' in name:\n",
    "                category = 'anomaly'\n",
    "            elif name.endswith(('_sin', '_cos')) or 'time' in name or 'hour' in name or 'weekday' in name or 'month' in name or 'season' in name:\n",
    "                category = 'temporal'\n",
    "            elif name.startswith(('diff_', 'pct_', 'cumulative_', 'relative_')):\n",
    "                category = 'trend'\n",
    "            elif name.startswith('power_level_') or name in ['zero_power', 'full_power', 'power_level_change']:\n",
    "                category = 'power_state'\n",
    "            elif name.startswith(('volatility_', 'cv_', 'skewness_', 'kurtosis_')):\n",
    "                category = 'volatility'\n",
    "            elif name.startswith(('diff_historical_', 'ratio_historical_')) or name == 'is_weekend':\n",
    "                category = 'cross_temporal'\n",
    "            elif name in ['dominant_frequency', 'spectral_energy']:\n",
    "                category = 'frequency'\n",
    "            else:\n",
    "                category = 'other'\n",
    "            \n",
    "            categories[category] = categories.get(category, 0) + 1\n",
    "        \n",
    "        return categories\n",
    "\n",
    "\n",
    "    def add_trend_features(self, power_series):\n",
    "        \"\"\"Ê∑ªÂä†Ë∂ãÂäøÂíåÂèòÂåñÁéáÁâπÂæÅÔºàÂΩí‰∏ÄÂåñÁâàÊú¨Ôºâ\"\"\"\n",
    "        features = []\n",
    "        feature_names = []\n",
    "        \n",
    "        # ‰∏ÄÈò∂Â∑ÆÂàÜÔºàÂèòÂåñÁéáÔºâ- ÂΩí‰∏ÄÂåñ\n",
    "        diff_1 = power_series.diff(1).fillna(0)\n",
    "        # Ë£ÅÂâ™Âà∞ÂêàÁêÜËåÉÂõ¥Âπ∂ÂΩí‰∏ÄÂåñÂà∞[-1, 1]\n",
    "        diff_1_normalized = np.clip(diff_1.values, -0.5, 0.5)\n",
    "        features.append(diff_1_normalized)\n",
    "        feature_names.append('diff_1')\n",
    "        \n",
    "        # ‰∫åÈò∂Â∑ÆÂàÜÔºàÂä†ÈÄüÂ∫¶Ôºâ- ÂΩí‰∏ÄÂåñ\n",
    "        diff_2 = power_series.diff(2).fillna(0)\n",
    "        # Ë£ÅÂâ™Âà∞ÂêàÁêÜËåÉÂõ¥Âπ∂ÂΩí‰∏ÄÂåñÂà∞[-1, 1]\n",
    "        diff_2_normalized = np.clip(diff_2.values, -0.3, 0.3)\n",
    "        features.append(diff_2_normalized)\n",
    "        feature_names.append('diff_2')\n",
    "        \n",
    "        return features, feature_names\n",
    "\n",
    "\n",
    "    def add_statistical_features(self, power_series):\n",
    "        \"\"\"Ê∑ªÂä†ÁªüËÆ°ÊªöÂä®Á™óÂè£ÁâπÂæÅÔºà‰ΩøÁî®config‰∏≠ÁöÑÁ™óÂè£ÂèÇÊï∞Ôºâ\"\"\"\n",
    "        features = []\n",
    "        feature_names = []\n",
    "        \n",
    "        # ‰ΩøÁî®config‰∏≠ÁöÑÁªüËÆ°Á™óÂè£ÂèÇÊï∞\n",
    "        windows = self.config.statistical_windows\n",
    "        \n",
    "        for window in windows:\n",
    "            # ÊªöÂä®ÂùáÂÄº\n",
    "            if self.config.enable_rolling_mean:\n",
    "                rolling_mean = power_series.rolling(window=window, min_periods=1).mean().fillna(0).values\n",
    "                features.append(rolling_mean)\n",
    "                feature_names.append(f'rolling_mean_{window}')\n",
    "            \n",
    "            # ÊªöÂä®Ê†áÂáÜÂ∑Æ\n",
    "            if self.config.enable_rolling_std:\n",
    "                rolling_std = power_series.rolling(window=window, min_periods=1).std().fillna(0).values\n",
    "                features.append(rolling_std)\n",
    "                feature_names.append(f'rolling_std_{window}')\n",
    "            \n",
    "            # ÊªöÂä®ÊúÄÂ§ßÂÄº\n",
    "            if self.config.enable_rolling_max:\n",
    "                rolling_max = power_series.rolling(window=window, min_periods=1).max().fillna(0).values\n",
    "                features.append(rolling_max)\n",
    "                feature_names.append(f'rolling_max_{window}')\n",
    "            \n",
    "            # ÊªöÂä®ÊúÄÂ∞èÂÄº\n",
    "            if self.config.enable_rolling_min:\n",
    "                rolling_min = power_series.rolling(window=window, min_periods=1).min().fillna(0).values\n",
    "                features.append(rolling_min)\n",
    "                feature_names.append(f'rolling_min_{window}')\n",
    "            \n",
    "            # ÊªöÂä®ËåÉÂõ¥Ôºàmax - minÔºâ\n",
    "            if self.config.enable_rolling_range and self.config.enable_rolling_max and self.config.enable_rolling_min:\n",
    "                rolling_max = power_series.rolling(window=window, min_periods=1).max().fillna(0).values\n",
    "                rolling_min = power_series.rolling(window=window, min_periods=1).min().fillna(0).values\n",
    "                rolling_range = rolling_max - rolling_min\n",
    "                features.append(rolling_range)\n",
    "                feature_names.append(f'rolling_range_{window}')\n",
    "        \n",
    "        return features, feature_names\n",
    "\n",
    "    def add_anomaly_features(self, power_series):\n",
    "        \"\"\"Ê∑ªÂä†ÂºÇÂ∏∏Ê£ÄÊµãÁâπÂæÅÔºà‰ΩøÁî®config‰∏≠ÁöÑÁ™óÂè£ÂèÇÊï∞Ôºâ\"\"\"\n",
    "        features = []\n",
    "        feature_names = []\n",
    "        \n",
    "        # Z-scoreÂºÇÂ∏∏Ê£ÄÊµã\n",
    "        z_scores = np.abs((power_series - power_series.mean()) / power_series.std())\n",
    "        is_outlier = (z_scores > 2).astype(int).values  # 2‰∏™Ê†áÂáÜÂ∑ÆÂ§ñ‰∏∫ÂºÇÂ∏∏\n",
    "        features.append(is_outlier)\n",
    "        feature_names.append('is_outlier')\n",
    "        \n",
    "        # IQRÂºÇÂ∏∏Ê£ÄÊµã\n",
    "        Q1 = power_series.quantile(0.25)\n",
    "        Q3 = power_series.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        is_iqr_outlier = ((power_series < (Q1 - 1.5 * IQR)) | \n",
    "                          (power_series > (Q3 + 1.5 * IQR))).astype(int).values\n",
    "        features.append(is_iqr_outlier)\n",
    "        feature_names.append('is_iqr_outlier')\n",
    "        \n",
    "        # Â±ÄÈÉ®ÂºÇÂ∏∏Âõ†Â≠êÔºà‰ΩøÁî®config‰∏≠ÁöÑÂºÇÂ∏∏Á™óÂè£ÂèÇÊï∞Ôºâ\n",
    "        windows = self.config.anomaly_windows\n",
    "        for window in windows:\n",
    "            local_mean = power_series.rolling(window=window, center=True).mean()\n",
    "            local_std = power_series.rolling(window=window, center=True).std()\n",
    "            local_anomaly = np.abs(power_series - local_mean) > (2 * local_std)\n",
    "            features.append(local_anomaly.fillna(0).astype(int).values)\n",
    "            feature_names.append(f'local_anomaly_{window}')\n",
    "        \n",
    "        return features, feature_names\n",
    "    \n",
    "    def _apply_feature_selection(self):\n",
    "        \"\"\"Â∫îÁî®ÁâπÂæÅÈÄâÊã©\"\"\"\n",
    "        from sklearn.feature_selection import VarianceThreshold\n",
    "        \n",
    "        # ÊñπÂ∑ÆÈòàÂÄºËøáÊª§\n",
    "        selector = VarianceThreshold(threshold=0.001)\n",
    "        features_filtered = selector.fit_transform(self.features)\n",
    "        selected_features = [name for i, name in enumerate(self.feature_names) \n",
    "                            if selector.get_support()[i]]\n",
    "        \n",
    "        # Â¶ÇÊûú‰ªçÁÑ∂Ë∂ÖËøáÊúÄÂ§ßÁâπÂæÅÊï∞ÔºåÈöèÊú∫ÈÄâÊã©\n",
    "        if len(selected_features) > self.config.max_features:\n",
    "            np.random.seed(self.config.random_seed)\n",
    "            indices = np.random.choice(len(selected_features), self.config.max_features, replace=False)\n",
    "            features_filtered = features_filtered[:, indices]\n",
    "            selected_features = [selected_features[i] for i in indices]\n",
    "        \n",
    "        return features_filtered, selected_features\n",
    "    \n",
    "    def _create_enhanced_features(self):\n",
    "        \"\"\"ÂàõÂª∫Â¢ûÂº∫ÁöÑÁâπÂæÅÔºàÂÆåÊï¥ÁâàÔºâ\"\"\"\n",
    "        features = []\n",
    "        feature_names = []\n",
    "        \n",
    "        power_series = pd.Series(self.data)\n",
    "        \n",
    "        # 1. ÂéüÂßãÂäüÁéáÊï∞ÊçÆ\n",
    "        features.append(self.data)\n",
    "        feature_names.append('power')\n",
    "        \n",
    "        # 2. ÊªûÂêéÁâπÂæÅ\n",
    "        if self.config.add_lag_features:\n",
    "            for lag in self.config.lag_steps:\n",
    "                if lag < len(self.data):\n",
    "                    lag_feature = np.roll(self.data, lag)\n",
    "                    lag_feature[:lag] = lag_feature[lag]\n",
    "                    features.append(lag_feature)\n",
    "                    feature_names.append(f'lag_{lag}')\n",
    "        \n",
    "        # 3. ÂäüÁéáÁà¨Âù°ÁâπÂæÅ\n",
    "        if self.config.add_ramp_features:\n",
    "            for window in self.config.ramp_windows:\n",
    "                # ÂäüÁéáÁà¨Âù°ÁéáÔºöÂçï‰ΩçÊó∂Èó¥ÂÜÖÂäüÁéáÂèòÂåñÂπÖÂ∫¶\n",
    "                ramp_rate = power_series.diff(window).fillna(0).values\n",
    "                features.append(ramp_rate)\n",
    "                feature_names.append(f'ramp_rate_{window}')\n",
    "                \n",
    "                # ÁªùÂØπÁà¨Âù°Áéá\n",
    "                abs_ramp_rate = np.abs(ramp_rate)\n",
    "                features.append(abs_ramp_rate)\n",
    "                feature_names.append(f'abs_ramp_rate_{window}')\n",
    "                \n",
    "                # ÂäüÁéáÁà¨Âù°Âº∫Â∫¶ÔºàÊªöÂä®Á™óÂè£ÂÜÖÊúÄÂ§ßÁªùÂØπÂèòÂåñÔºâ\n",
    "                ramp_intensity = power_series.diff().abs().rolling(window=window, min_periods=1).max().fillna(0).values\n",
    "                features.append(ramp_intensity)\n",
    "                feature_names.append(f'ramp_intensity_{window}')\n",
    "                \n",
    "                # Áà¨Âù°ÊñπÂêëÔºà‰∏äÂçá=1Ôºå‰∏ãÈôç=-1ÔºåÁ®≥ÂÆö=0Ôºâ\n",
    "                ramp_direction = np.sign(ramp_rate)\n",
    "                features.append(ramp_direction)\n",
    "                feature_names.append(f'ramp_direction_{window}')\n",
    "        \n",
    "        # 4. ÁªüËÆ°ÊªöÂä®ÁâπÂæÅ\n",
    "        if self.config.add_statistical_features:\n",
    "            stat_features, stat_names = self.add_statistical_features(power_series)\n",
    "            features.extend(stat_features)\n",
    "            feature_names.extend(stat_names)\n",
    "\n",
    "        # 5. ÂºÇÂ∏∏Ê£ÄÊµãÁâπÂæÅ\n",
    "        if self.config.add_anomaly_features:\n",
    "            anomaly_features, anomaly_names = self.add_anomaly_features(power_series)\n",
    "            features.extend(anomaly_features)\n",
    "            feature_names.extend(anomaly_names)\n",
    "        \n",
    "        \n",
    "        # 6. Êñ∞Â¢ûÔºöË∂ãÂäøÁâπÂæÅ\n",
    "        if getattr(self.config, 'add_trend_features', True):\n",
    "            trend_features, trend_names = self.add_trend_features(power_series)\n",
    "            features.extend(trend_features)\n",
    "            feature_names.extend(trend_names)\n",
    "        \n",
    "    \n",
    "        return np.array(features).T, feature_names  # [time_steps, num_features]\n",
    "    \n",
    "    def _create_sequences(self):\n",
    "        X, y = [], []\n",
    "        for i in range(0, len(self.data) - self.seq_length - self.pred_length + 1, self.step):\n",
    "            # Â§öÁâπÂæÅËæìÂÖ•Â∫èÂàó\n",
    "            X.append(self.features[i:i + self.seq_length])\n",
    "            # ËæìÂá∫Â∫èÂàóÔºà‰ªçÁÑ∂ÊòØÂéüÂßãÂäüÁéáÊï∞ÊçÆÔºâ\n",
    "            y.append(self.data[i + self.seq_length:i + self.seq_length + self.pred_length])\n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.FloatTensor(self.X[idx]), torch.FloatTensor(self.y[idx])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ÂàõÂª∫ËÆ≠ÁªÉÊï∞ÊçÆÈõÜ\n",
    "print(\"Creating training dataset...\")\n",
    "train_dataset = EnhancedPowerDataset(\n",
    "    data=train_power_processed,\n",
    "    seq_length=config.seq_length,\n",
    "    pred_length=config.pred_length,\n",
    "    step=1,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(f\"Training dataset created successfully!\")\n",
    "print(f\"Dataset length: {len(train_dataset)}\")\n",
    "print(f\"Features shape: {train_dataset.features.shape}\")\n",
    "print(f\"Input sequences shape: {train_dataset.X.shape}\")\n",
    "print(f\"Target sequences shape: {train_dataset.y.shape}\")\n",
    "\n",
    "# ÂàõÂª∫ÊµãËØïÊï∞ÊçÆÈõÜ\n",
    "print(\"\\nCreating test dataset...\")\n",
    "test_dataset = EnhancedPowerDataset(\n",
    "    data=test_power_processed,\n",
    "    seq_length=config.seq_length,\n",
    "    pred_length=config.pred_length,\n",
    "    step=1,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(f\"Test dataset created successfully!\")\n",
    "print(f\"Dataset length: {len(test_dataset)}\")\n",
    "print(f\"Features shape: {test_dataset.features.shape}\")\n",
    "print(f\"Input sequences shape: {test_dataset.X.shape}\")\n",
    "print(f\"Target sequences shape: {test_dataset.y.shape}\")\n",
    "\n",
    "# Êõ¥Êñ∞config‰∏≠ÁöÑinput_size\n",
    "config.input_size = train_dataset.features.shape[1]\n",
    "print(f\"\\nUpdated config.input_size to: {config.input_size}\")\n",
    "\n",
    "# ÊòæÁ§∫ÁâπÂæÅÁªüËÆ°‰ø°ÊÅØ\n",
    "print(f\"\\nFeature Engineering Summary:\")\n",
    "print(f\"‚úÖ Total features created: {len(train_dataset.feature_names)}\")\n",
    "print(f\"‚úÖ Feature categories: {train_dataset._get_feature_categories()}\")\n",
    "\n",
    "# Ê£ÄÊü•ÁâπÂæÅËåÉÂõ¥Ôºà‰∏çËøõË°åÂΩí‰∏ÄÂåñÔºâ\n",
    "print(f\"\\nFeature Statistics (Raw Features - No Normalization):\")\n",
    "print(f\"Features range: [{train_dataset.features.min():.6f}, {train_dataset.features.max():.6f}]\")\n",
    "print(f\"Features mean: {train_dataset.features.mean():.6f}\")\n",
    "print(f\"Features std: {train_dataset.features.std():.6f}\")\n",
    "print(f\"Target values range: [{train_dataset.data.min():.6f}, {train_dataset.data.max():.6f}]\")\n",
    "\n",
    "# Ê£ÄÊü•ÊòØÂê¶ÊúâÂºÇÂ∏∏ÂÄº\n",
    "has_nan = np.isnan(train_dataset.features).any()\n",
    "has_inf = np.isinf(train_dataset.features).any()\n",
    "has_very_large = np.any(np.abs(train_dataset.features) > 1e6)\n",
    "\n",
    "print(f\"\\nData Quality Check:\")\n",
    "print(f\"Has NaN values: {has_nan}\")\n",
    "print(f\"Has Inf values: {has_inf}\")\n",
    "print(f\"Has very large values (>1e6): {has_very_large}\")\n",
    "\n",
    "if has_nan or has_inf or has_very_large:\n",
    "    print(\"‚ö†Ô∏è  WARNING: Found problematic values in features!\")\n",
    "    print(\"üîß Applying safe replacement (NaN/Inf ‚Üí 0)...\")\n",
    "    train_dataset.features = np.nan_to_num(train_dataset.features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    test_dataset.features = np.nan_to_num(test_dataset.features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    # ÈáçÊñ∞ÂàõÂª∫Â∫èÂàó\n",
    "    train_dataset.X, train_dataset.y = train_dataset._create_sequences()\n",
    "    test_dataset.X, test_dataset.y = test_dataset._create_sequences()\n",
    "    print(\"‚úÖ Problematic values replaced, sequences recreated\")\n",
    "else:\n",
    "    print(\"‚úÖ No problematic values found\")\n",
    "\n",
    "\n",
    "\n",
    "# ÂàõÂª∫Êï∞ÊçÆÂä†ËΩΩÂô®\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING DATA LOADERS\")\n",
    "\n",
    "\n",
    "# ÂàÜÂâ≤ËÆ≠ÁªÉÈõÜÂíåÈ™åËØÅÈõÜ\n",
    "train_size = int(config.train_val_split * len(train_dataset))\n",
    "train_indices = list(range(train_size))\n",
    "val_indices = list(range(train_size, len(train_dataset)))\n",
    "\n",
    "train_dataset_split = torch.utils.data.Subset(train_dataset, train_indices)\n",
    "val_dataset = torch.utils.data.Subset(train_dataset, val_indices)\n",
    "\n",
    "# ÂàõÂª∫Êï∞ÊçÆÂä†ËΩΩÂô®\n",
    "train_loader = DataLoader(train_dataset_split, batch_size=config.batch_size, shuffle=True, num_workers=config.num_workers)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=config.num_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=config.num_workers)\n",
    "\n",
    "print(f\"Training set size: {len(train_dataset_split)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")\n",
    "print(f\"Testing set size: {len(test_dataset)}\")\n",
    "\n",
    "print(f\"Total features: {len(train_dataset.feature_names)}\")\n",
    "print(f\"Feature categories: {train_dataset._get_feature_categories()}\")\n",
    "print(f\"Ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69394c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== iTransformer Model Information ===\n",
      "Input sequence length: 96\n",
      "Input features: 57\n",
      "Model dimension: 64\n",
      "Attention heads: 8\n",
      "Encoder layers: 2\n",
      "Total parameters: 107,216\n",
      "Trainable parameters: 107,216\n",
      "==================================================\n",
      "Test forward pass - Input shape: torch.Size([2, 96, 57]), Output shape: torch.Size([2, 16])\n"
     ]
    }
   ],
   "source": [
    "# Create iTransformer model\n",
    "model = iTransformer(config).to(device)\n",
    "\n",
    "# Update model's input dimension\n",
    "model.update_input_size(config.input_size)\n",
    "\n",
    "# Model information\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"=== iTransformer Model Information ===\")\n",
    "print(f\"Input sequence length: {config.seq_length}\")\n",
    "print(f\"Input features: {config.input_size}\")\n",
    "print(f\"Model dimension: {config.d_model}\")\n",
    "print(f\"Attention heads: {config.nhead}\")\n",
    "print(f\"Encoder layers: {config.num_encoder_layers}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test forward pass\n",
    "with torch.no_grad():\n",
    "    sample_input = torch.randn(2, config.seq_length, config.input_size).to(device)\n",
    "    sample_output = model(sample_input)\n",
    "    print(f\"Test forward pass - Input shape: {sample_input.shape}, Output shape: {sample_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c35933cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training setup completed!\n",
      "Optimizer: AdamW (lr=0.0001\n",
      "Loss function: Huber Loss\n",
      "LR Scheduler: ReduceLROnPlateau (patience=8)\n",
      "Early stopping: patience=50\n"
     ]
    }
   ],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.HuberLoss(delta=1.0)\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=config.learning_rate, \n",
    "    betas=(0.9, 0.95)  # Better for Transformers\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=config.lr_factor, \n",
    "    patience=config.lr_patience, verbose=True, min_lr=1e-6\n",
    ")\n",
    "\n",
    "# CR metric calculation\n",
    "def calculate_CR(PM, PP):\n",
    "    N = len(PM)\n",
    "    Ri = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        if PM[i] > 0.2:\n",
    "            Ri[i] = (PM[i] - PP[i]) / PM[i]\n",
    "        else:\n",
    "            Ri[i] = (PM[i] - PP[i]) / 0.2\n",
    "    rms_error = np.sqrt(np.mean(Ri**2))\n",
    "    CR = (1 - rms_error) * 100\n",
    "    return CR\n",
    "\n",
    "# Early stopping mechanism\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=15, min_delta=0.0001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_cr = None\n",
    "        self.counter = 0\n",
    "        self.best_weights = None\n",
    "        \n",
    "    def __call__(self, val_cr, model):\n",
    "        if self.best_cr is None:\n",
    "            self.best_cr = val_cr\n",
    "            self.save_checkpoint(model)\n",
    "        elif val_cr > self.best_cr + self.min_delta:\n",
    "            self.best_cr = val_cr\n",
    "            self.counter = 0\n",
    "            self.save_checkpoint(model)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            \n",
    "        return self.counter >= self.patience\n",
    "    \n",
    "    def save_checkpoint(self, model):\n",
    "        self.best_weights = model.state_dict().copy()\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=config.early_stop_patience, \n",
    "    min_delta=config.early_stop_min_delta\n",
    ")\n",
    "\n",
    "print(f\"Training setup completed!\")\n",
    "print(f\"Optimizer: AdamW (lr={config.learning_rate}\")\n",
    "print(f\"Loss function: Huber Loss\")\n",
    "print(f\"LR Scheduler: ReduceLROnPlateau (patience={config.lr_patience})\")\n",
    "print(f\"Early stopping: patience={config.early_stop_patience}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be1a4295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# Training and validation functions\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device, config):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % config.print_freq == 0:\n",
    "            print(f'  Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.6f}')\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "def calculate_validation_cr(model, val_loader, device):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            \n",
    "            all_predictions.append(output.cpu().numpy())\n",
    "            all_targets.append(target.cpu().numpy())\n",
    "    \n",
    "    predictions = np.concatenate(all_predictions, axis=0)\n",
    "    targets = np.concatenate(all_targets, axis=0)\n",
    "    \n",
    "    # Calculate CR on all prediction points (correct approach)\n",
    "    cr_overall = calculate_CR(targets.flatten(), predictions.flatten())\n",
    "    return cr_overall\n",
    "\n",
    "print(\"Training functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e524c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting iTransformer Training ===\n",
      "Model: iTransformer\n",
      "Training epochs: 300\n",
      "Batch size: 128\n",
      "Learning rate: 0.0001\n",
      "Model parameters: 107,216\n",
      "==================================================\n",
      "\n",
      "Epoch 1/300\n",
      "  Batch 0/44, Loss: 0.529845\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_cr_scores = []\n",
    "\n",
    "print(\"=== Starting iTransformer Training ===\")\n",
    "print(f\"Model: iTransformer\")\n",
    "print(f\"Training epochs: {config.num_epochs}\")\n",
    "print(f\"Batch size: {config.batch_size}\")\n",
    "print(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "print(f\"Model parameters: {total_params:,}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "best_epoch = 0\n",
    "for epoch in range(config.num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{config.num_epochs}\")\n",
    "    \n",
    "    # Training\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device, config)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validation\n",
    "    val_loss = validate_epoch(model, val_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Calculate validation CR metric\n",
    "    val_cr = calculate_validation_cr(model, val_loader, device)\n",
    "    val_cr_scores.append(val_cr)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "    print(f\"Validation CR: {val_cr:.2f}%\")\n",
    "    print(f\"Current LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    if early_stopping(val_cr, model):\n",
    "        print(f\"\\nEarly stopping triggered! Training stopped at epoch {epoch+1}\")\n",
    "        print(f\"Best CR metric: {early_stopping.best_cr:.2f}%\")\n",
    "        best_epoch = epoch + 1\n",
    "        break\n",
    "    \n",
    "    if val_cr == early_stopping.best_cr:\n",
    "        best_epoch = epoch + 1\n",
    "\n",
    "print(\"\\n=== Training Completed ===\")\n",
    "print(f\"Best validation CR: {early_stopping.best_cr:.2f}% (Epoch {best_epoch})\")\n",
    "print(f\"Total epochs trained: {len(train_losses)}\")\n",
    "\n",
    "# Restore best weights\n",
    "if early_stopping.best_weights is not None:\n",
    "    model.load_state_dict(early_stopping.best_weights)\n",
    "    print(\"Restored best model weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f5faeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss', color='blue', linewidth=2)\n",
    "plt.plot(val_losses, label='Validation Loss', color='red', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('iTransformer Training Progress - Loss Curves')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(val_cr_scores, label='Validation CR', color='green', linewidth=2)\n",
    "plt.axhline(y=early_stopping.best_cr, color='red', linestyle='--', \n",
    "           label=f'Best CR: {early_stopping.best_cr:.2f}%')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('CR Metric (%)')\n",
    "plt.title('iTransformer Training Progress - CR Metric')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save model\n",
    "model_save_path = f'{config.model_save_dir}/itransformer_16_point_best_model.pth'\n",
    "os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'val_cr_scores': val_cr_scores,\n",
    "    'best_cr': early_stopping.best_cr,\n",
    "    'best_epoch': best_epoch,\n",
    "    'config': config.__dict__,\n",
    "    'feature_names': train_dataset.feature_names,\n",
    "    'model_info': {\n",
    "        'model_type': 'iTransformer',\n",
    "        'd_model': config.d_model,\n",
    "        'nhead': config.nhead,\n",
    "        'num_layers': config.num_encoder_layers,\n",
    "        'total_params': total_params,\n",
    "        'input_size': config.input_size\n",
    "    }\n",
    "}, model_save_path)\n",
    "\n",
    "print(f\"\\nModel saved to: {model_save_path}\")\n",
    "print(f\"Best CR metric: {early_stopping.best_cr:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08a4166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model performance\n",
    "def test_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            \n",
    "            all_predictions.append(output.cpu().numpy())\n",
    "            all_targets.append(target.cpu().numpy())\n",
    "    \n",
    "    predictions = np.concatenate(all_predictions, axis=0)\n",
    "    targets = np.concatenate(all_targets, axis=0)\n",
    "    \n",
    "    return predictions, targets\n",
    "\n",
    "print(\"=== Testing iTransformer Model ===\")\n",
    "predictions, targets = test_model(model, test_loader, device)\n",
    "\n",
    "print(f\"Prediction shape: {predictions.shape}\")\n",
    "print(f\"Target shape: {targets.shape}\")\n",
    "\n",
    "# Calculate step-wise metrics first\n",
    "print(f\"\\n=== Step-wise Test Results ===\")\n",
    "step_mse = []\n",
    "step_mae = []\n",
    "step_cr = []\n",
    "\n",
    "for step in range(16):\n",
    "    step_targets = targets[:, step]\n",
    "    step_predictions = predictions[:, step]\n",
    "    \n",
    "    mse = mean_squared_error(step_targets, step_predictions)\n",
    "    mae = mean_absolute_error(step_targets, step_predictions)\n",
    "    cr = calculate_CR(step_targets, step_predictions)\n",
    "    \n",
    "    step_mse.append(mse)\n",
    "    step_mae.append(mae)\n",
    "    step_cr.append(cr)\n",
    "    \n",
    "    print(f\"Step {step+1:2d}: MSE={mse:.6f}, MAE={mae:.6f}, CR={cr:.2f}%\")\n",
    "\n",
    "# 16‰∏™Êó∂Èó¥Â∞∫Â∫¶ÁöÑÂπ≥ÂùáÊåáÊ†á\n",
    "mse_average_across_steps = np.mean(step_mse)\n",
    "mae_average_across_steps = np.mean(step_mae)\n",
    "rmse_average_across_steps = np.sqrt(mse_average_across_steps)\n",
    "cr_average_across_steps = np.mean(step_cr)\n",
    "\n",
    "print(f\"\\nAverage metrics across 16 time steps:\")\n",
    "print(f\"Average MSE: {mse_average_across_steps:.6f}\")\n",
    "print(f\"Average RMSE: {rmse_average_across_steps:.6f}\")  \n",
    "print(f\"Average MAE: {mae_average_across_steps:.6f}\")\n",
    "print(f\"Average CR: {cr_average_across_steps:.2f}%\")\n",
    "\n",
    "# Overall metrics (flattened calculation)\n",
    "mse_overall = mean_squared_error(targets.flatten(), predictions.flatten())\n",
    "mae_overall = mean_absolute_error(targets.flatten(), predictions.flatten())\n",
    "rmse_overall = np.sqrt(mse_overall)\n",
    "cr_overall = calculate_CR(targets.flatten(), predictions.flatten())\n",
    "\n",
    "print(f\"\\nOverall metrics (flattened):\")\n",
    "print(f\"MSE: {mse_overall:.6f}\")\n",
    "print(f\"RMSE: {rmse_overall:.6f}\")\n",
    "print(f\"MAE: {mae_overall:.6f}\")\n",
    "print(f\"Overall CR metric: {cr_overall:.2f}%\")\n",
    "\n",
    "print(f\"\\n=== Summary Statistics ===\")\n",
    "print(f\"Average step-wise CR: {np.mean(step_cr):.2f}%\")\n",
    "print(f\"Step-wise CR std: {np.std(step_cr):.2f}%\")\n",
    "print(f\"Best step CR: {np.max(step_cr):.2f}% (Step {np.argmax(step_cr)+1})\")\n",
    "print(f\"Worst step CR: {np.min(step_cr):.2f}% (Step {np.argmin(step_cr)+1})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fcd8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results\n",
    "os.makedirs(config.results_dir, exist_ok=True)\n",
    "\n",
    "# Save prediction results\n",
    "results_df = pd.DataFrame({\n",
    "    'targets': targets.flatten(),\n",
    "    'predictions': predictions.flatten(),\n",
    "    'errors': targets.flatten() - predictions.flatten()\n",
    "})\n",
    "results_df.to_csv(f'{config.results_dir}/itransformer_16_point_predictions.csv', index=False)\n",
    "\n",
    "# Save performance metrics\n",
    "performance_summary = {\n",
    "    'model_info': {\n",
    "        'model_type': 'iTransformer',\n",
    "        'd_model': config.d_model,\n",
    "        'nhead': config.nhead,\n",
    "        'num_layers': config.num_encoder_layers,\n",
    "        'dim_feedforward': config.dim_feedforward,\n",
    "        'total_params': int(total_params),\n",
    "        'input_features': int(config.input_size)\n",
    "    },\n",
    "    'overall_metrics': {\n",
    "        'MSE': float(mse_overall),\n",
    "        'RMSE': float(rmse_overall),\n",
    "        'MAE': float(mae_overall),\n",
    "        'CR': float(cr_overall)\n",
    "    },\n",
    "    'step_wise_metrics': {\n",
    "        'step': list(range(1, 17)),\n",
    "        'MSE': [float(x) for x in step_mse],\n",
    "        'MAE': [float(x) for x in step_mae],\n",
    "        'CR': [float(x) for x in step_cr]\n",
    "    },\n",
    "    'training_info': {\n",
    "        'best_validation_cr': float(early_stopping.best_cr),\n",
    "        'best_epoch': int(best_epoch),\n",
    "        'total_epochs': len(train_losses),\n",
    "        'final_lr': float(optimizer.param_groups[0]['lr']),\n",
    "        'early_stopping_criterion': 'CR_based'\n",
    "    },\n",
    "    'hyperparameters': {\n",
    "        'seq_length': config.seq_length,\n",
    "        'pred_length': config.pred_length,\n",
    "        'd_model': config.d_model,\n",
    "        'nhead': config.nhead,\n",
    "        'batch_size': config.batch_size,\n",
    "        'learning_rate': config.learning_rate,\n",
    "        'dropout': config.dropout\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(f'{config.results_dir}/itransformer_16_point_performance.json', 'w') as f:\n",
    "    json.dump(performance_summary, f, indent=2)\n",
    "\n",
    "# Create detailed report\n",
    "performance_report = f\"\"\"\n",
    "# iTransformer Wind Power Prediction - Performance Report\n",
    "\n",
    "## Model Architecture\n",
    "- **Model Type**: iTransformer (Inverted Transformer)\n",
    "- **Input Sequence**: {config.seq_length} points ({config.seq_length * config.time_interval_minutes / 60:.1f} hours)\n",
    "- **Prediction Length**: {config.pred_length} points ({config.pred_length * config.time_interval_minutes / 60:.1f} hours)\n",
    "- **Model Dimension**: {config.d_model}\n",
    "- **Attention Heads**: {config.nhead}\n",
    "- **Encoder Layers**: {config.num_encoder_layers}\n",
    "- **Total Parameters**: {total_params:,}\n",
    "- **Input Features**: {config.input_size}\n",
    "\n",
    "## Training Results\n",
    "- **Best Validation CR**: {early_stopping.best_cr:.2f}% (Epoch {best_epoch})\n",
    "- **Total Training Epochs**: {len(train_losses)}\n",
    "- **Final Learning Rate**: {optimizer.param_groups[0]['lr']:.2e}\n",
    "\n",
    "## Test Set Performance\n",
    "- **Overall CR**: {cr_overall:.2f}%\n",
    "- **RMSE**: {rmse_overall:.6f}\n",
    "- **MAE**: {mae_overall:.6f}\n",
    "- **MSE**: {mse_overall:.6f}\n",
    "\n",
    "## Step-wise Performance Analysis\n",
    "- **Average CR**: {np.mean(step_cr):.2f}%\n",
    "- **CR Standard Deviation**: {np.std(step_cr):.2f}%\n",
    "- **Best Step**: Step {np.argmax(step_cr)+1} (CR: {np.max(step_cr):.2f}%)\n",
    "- **Worst Step**: Step {np.argmin(step_cr)+1} (CR: {np.min(step_cr):.2f}%)\n",
    "\n",
    "## Detailed Step-wise Metrics\n",
    "\"\"\"\n",
    "\n",
    "for i in range(16):\n",
    "    performance_report += f\"\\n### Step {i+1} ({(i+1)*15} minutes ahead)\"\n",
    "    performance_report += f\"\\n- **CR**: {step_cr[i]:.2f}%\"\n",
    "    performance_report += f\"\\n- **RMSE**: {np.sqrt(step_mse[i]):.6f}\"\n",
    "    performance_report += f\"\\n- **MAE**: {step_mae[i]:.6f}\\n\"\n",
    "\n",
    "with open(f'{config.results_dir}/itransformer_16_point_report.md', 'w', encoding='utf-8') as f:\n",
    "    f.write(performance_report)\n",
    "\n",
    "print(\"\\n=== Results Saved ===\")\n",
    "print(f\"- Predictions: {config.results_dir}/itransformer_16_point_predictions.csv\")\n",
    "print(f\"- Performance: {config.results_dir}/itransformer_16_point_performance.json\")\n",
    "print(f\"- Report: {config.results_dir}/itransformer_16_point_report.md\")\n",
    "print(f\"- Model: {model_save_path}\")\n",
    "\n",
    "print(f\"\\n=== Final Summary ===\")\n",
    "print(f\"iTransformer Model Performance:\")\n",
    "print(f\"  - Best Validation CR: {early_stopping.best_cr:.2f}%\")\n",
    "print(f\"  - Test Set CR: {cr_overall:.2f}%\")\n",
    "print(f\"  - Test Set RMSE: {rmse_overall:.6f}\")\n",
    "print(f\"  - Model Parameters: {total_params:,}\")\n",
    "print(f\"  - Training Efficiency: {len(train_losses)} epochs\")\n",
    "print(f\"\\niTransformer Features:\")\n",
    "print(f\"  - Variable-wise attention mechanism\")\n",
    "print(f\"  - Inverted architecture for multivariate time series\")\n",
    "print(f\"  - State-of-the-art performance on forecasting benchmarks\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
