{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2811dd32",
   "metadata": {},
   "source": [
    "# Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ac7b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, confusion_matrix, classification_report, \n",
    "                           roc_auc_score, roc_curve, precision_recall_curve, \n",
    "                           average_precision_score, f1_score)\n",
    "import catboost as cb  # Using CatBoost\n",
    "import optuna  # For hyperparameter optimization\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'  \n",
    "\n",
    "# Ensure output directories exist\n",
    "plots_dir = '/data/jinming/ee_stable/catboost/plots'\n",
    "models_dir = '/data/jinming/ee_stable/catboost/models'\n",
    "results_dir = '/data/jinming/ee_stable/catboost/results'\n",
    "for directory in [plots_dir, models_dir, results_dir]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# 1. Load data\n",
    "print(\"===== Loading Data =====\")\n",
    "train_df = pd.read_csv('/data/jinming/ee_stable/data/train.csv')\n",
    "test_df = pd.read_csv('/data/jinming/ee_stable/data/test.csv')\n",
    "val_df = pd.read_csv('/data/jinming/ee_stable/data/val.csv')\n",
    "\n",
    "# 2. Data preparation\n",
    "print(\"===== Preparing Data =====\")\n",
    "X_train = train_df.drop(['stab', 'stabf_encoded', 'stabf', 'p1', 'p2', 'p3', 'p4'], axis=1)\n",
    "X_test = test_df.drop(['stab', 'stabf_encoded', 'stabf', 'p1', 'p2', 'p3', 'p4'], axis=1)\n",
    "X_val = val_df.drop(['stab', 'stabf_encoded', 'stabf', 'p1', 'p2', 'p3', 'p4'], axis=1)\n",
    "\n",
    "y_train = train_df['stabf_encoded']\n",
    "y_test = test_df['stabf_encoded']\n",
    "y_val = val_df['stabf_encoded']\n",
    "\n",
    "print(f\"Dataset dimensions - Train: {X_train.shape}, Validation: {X_val.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# 3. Feature engineering function\n",
    "def create_features(X_train, X_test, X_val):\n",
    "    # Deep copy to avoid modifying original data\n",
    "    X_train_new = X_train.copy()\n",
    "    X_test_new = X_test.copy()\n",
    "    X_val_new = X_val.copy()\n",
    "    \n",
    "    # Basic interaction features\n",
    "    for df in [X_train_new, X_test_new, X_val_new]:\n",
    "        df['tau1_g1'] = df['tau1'] * df['g1']\n",
    "        df['tau2_g2'] = df['tau2'] * df['g2']\n",
    "        df['tau3_g3'] = df['tau3'] * df['g3']\n",
    "        df['tau4_g4'] = df['tau4'] * df['g4']\n",
    "        \n",
    "        # Delay ratio\n",
    "        df['tau_ratio'] = df[['tau1', 'tau2', 'tau3', 'tau4']].max(axis=1) / df[['tau1', 'tau2', 'tau3', 'tau4']].min(axis=1).replace(0, 0.001)\n",
    "        \n",
    "        # Delay-elasticity ratio: response sensitivity of each node\n",
    "        df['tau1_g1_ratio'] = df['tau1'] / df['g1'].replace(0, 0.001)\n",
    "        df['tau2_g2_ratio'] = df['tau2'] / df['g2'].replace(0, 0.001)\n",
    "        df['tau3_g3_ratio'] = df['tau3'] / df['g3'].replace(0, 0.001)\n",
    "        df['tau4_g4_ratio'] = df['tau4'] / df['g4'].replace(0, 0.001)\n",
    "        \n",
    "        # System total elasticity\n",
    "        df['total_elasticity'] = df['g1'] + df['g2'] + df['g3'] + df['g4']\n",
    "        \n",
    "        # Elasticity distribution non-uniformity\n",
    "        df['elasticity_disparity'] = df[['g1', 'g2', 'g3', 'g4']].max(axis=1) / df[['g1', 'g2', 'g3', 'g4']].min(axis=1).replace(0, 0.001)\n",
    "        \n",
    "        # Non-linear features - quadratic terms\n",
    "        df['tau1_squared'] = df['tau1'] ** 2\n",
    "        df['tau2_squared'] = df['tau2'] ** 2\n",
    "        df['tau3_squared'] = df['tau3'] ** 2\n",
    "        df['tau4_squared'] = df['tau4'] ** 2\n",
    "        \n",
    "        # Node relationship features\n",
    "        df['tau_g_correlation'] = (\n",
    "            (df['tau1'] * df['g1']) + \n",
    "            (df['tau2'] * df['g2']) + \n",
    "            (df['tau3'] * df['g3']) + \n",
    "            (df['tau4'] * df['g4'])\n",
    "        ) / (df['tau1'] + df['tau2'] + df['tau3'] + df['tau4'] + 0.001)\n",
    "        \n",
    "        # System overall response speed indicator\n",
    "        df['system_response_speed'] = 4 / (\n",
    "            (1/df['tau1'].replace(0, 0.001)) + \n",
    "            (1/df['tau2'].replace(0, 0.001)) + \n",
    "            (1/df['tau3'].replace(0, 0.001)) + \n",
    "            (1/df['tau4'].replace(0, 0.001))\n",
    "        )\n",
    "    \n",
    "    return X_train_new, X_test_new, X_val_new\n",
    "\n",
    "# Manual feature selection function\n",
    "def select_features_manual(X_train, X_test, X_val):\n",
    "    \"\"\"Manually select specified feature set\"\"\"\n",
    "    print(\"\\n===== Using Manually Specified Features =====\")\n",
    "    \n",
    "    # Specify features to keep\n",
    "    selected_features = [\n",
    "        # Original tau features\n",
    "        'tau1', 'tau2', 'tau3', 'tau4',\n",
    "        \n",
    "        # Original g features\n",
    "        'g1', 'g2', 'g3', 'g4',\n",
    "        \n",
    "        # tau and g interaction terms\n",
    "        'tau1_g1', 'tau2_g2', 'tau3_g3', 'tau4_g4',\n",
    "        \n",
    "        # tau ratio features\n",
    "        'tau_ratio'\n",
    "    ]\n",
    "    \n",
    "    # Verify all specified features exist\n",
    "    missing_features = [f for f in selected_features if f not in X_train.columns]\n",
    "    if missing_features:\n",
    "        print(f\"Warning: The following specified features do not exist: {', '.join(missing_features)}\")\n",
    "        # Filter out non-existent features\n",
    "        selected_features = [f for f in selected_features if f in X_train.columns]\n",
    "    \n",
    "    print(f\"Using {len(selected_features)} specified features:\")\n",
    "    print(f\"Selected features: {', '.join(selected_features)}\")\n",
    "    \n",
    "    return X_train[selected_features], X_test[selected_features], X_val[selected_features], selected_features\n",
    "\n",
    "# Apply feature engineering\n",
    "print(\"\\n===== Performing Feature Engineering =====\")\n",
    "X_train_featured, X_test_featured, X_val_featured = create_features(X_train, X_test, X_val)\n",
    "print(f\"Number of features after engineering: {X_train_featured.shape[1]}\")\n",
    "\n",
    "# Apply feature selection using manual method\n",
    "X_train_final, X_test_final, X_val_final, selected_features = select_features_manual(\n",
    "   X_train_featured, X_test_featured, X_val_featured)\n",
    "\n",
    "print(f\"Number of features after selection: {X_train_final.shape[1]}\")\n",
    "\n",
    "# Check if GPU is available for CatBoost\n",
    "print(\"\\n===== Checking GPU availability =====\")\n",
    "try:\n",
    "    # Create a more realistic test dataset for GPU test\n",
    "    X_test_gpu = np.random.rand(100, 8)  # 100 samples, 8 features\n",
    "    y_test_gpu = np.random.randint(0, 2, 100)  # Binary labels\n",
    "    \n",
    "    # Create a test CatBoost model with GPU support\n",
    "    test_model = cb.CatBoostClassifier(\n",
    "        iterations=10,\n",
    "        task_type='GPU',\n",
    "        devices='0',\n",
    "        verbose=False\n",
    "    )\n",
    "    test_model.fit(X_test_gpu, y_test_gpu, verbose=False)\n",
    "    print(\"GPU acceleration is available for CatBoost!\")\n",
    "    GPU_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    print(f\"GPU acceleration is NOT available: {e}\")\n",
    "    print(\"Falling back to CPU training\")\n",
    "    GPU_AVAILABLE = False\n",
    "    \n",
    "# 5. Optuna hyperparameter optimization for AUC with GPU\n",
    "print(\"\\n===== Starting Optuna GPU hyperparameter tuning process (AUC) =====\")\n",
    "\n",
    "def objective_auc(trial):\n",
    "    \"\"\"Optuna optimization objective function using AUC as evaluation metric with GPU acceleration\"\"\"\n",
    "    # Define CatBoost parameter search space\n",
    "    params = {\n",
    "        'loss_function': 'Logloss',  # For binary classification\n",
    "        'eval_metric': 'AUC',\n",
    "        'verbose': 0,\n",
    "        \n",
    "        # Core parameters\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'iterations': 2000,          # Will use early stopping to select the best iteration\n",
    "        'depth': trial.suggest_int('depth', 4, 10),\n",
    "        \n",
    "        # Regularization parameters\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-8, 10.0, log=True),\n",
    "        'random_strength': trial.suggest_float('random_strength', 1e-8, 10.0, log=True),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 10.0),\n",
    "        \n",
    "        # Other parameters\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 100),\n",
    "        'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations', 1, 10),\n",
    "        'rsm': trial.suggest_float('rsm', 0.1, 1.0),  # Column sample ratio\n",
    "        \n",
    "        'random_seed': 42\n",
    "    }\n",
    "    \n",
    "    # Add GPU parameters if GPU is available\n",
    "    if GPU_AVAILABLE:\n",
    "        params['task_type'] = 'GPU'\n",
    "        params['devices'] = '0'\n",
    "    \n",
    "    # Create CatBoost model\n",
    "    model = cb.CatBoostClassifier(**params)\n",
    "    \n",
    "    # Train model on training set, using validation set for early stopping\n",
    "    model.fit(\n",
    "        X_train_final, y_train,\n",
    "        eval_set=[(X_val_final, y_val)],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Predict on validation set\n",
    "    y_val_prob = model.predict_proba(X_val_final)[:, 1]\n",
    "    \n",
    "    # Calculate AUC score\n",
    "    auc_score = roc_auc_score(y_val, y_val_prob)\n",
    "    \n",
    "    # Print current trial results\n",
    "    print(f\"Trial {trial.number}: AUC = {auc_score:.4f}\")\n",
    "    \n",
    "    return auc_score\n",
    "\n",
    "# Create Optuna study object with direction to maximize AUC\n",
    "study_auc = optuna.create_study(direction='maximize', study_name='catboost_gpu_auc_optimization')\n",
    "\n",
    "# Run optimization\n",
    "n_trials = 100  # Can adjust based on computational resources and time\n",
    "print(f\"Starting {n_trials} GPU-accelerated hyperparameter tuning trials...\")\n",
    "start_time = time.time()\n",
    "study_auc.optimize(objective_auc, n_trials=n_trials)\n",
    "end_time = time.time()\n",
    "print(f\"Tuning completed! Duration: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Print best parameters and results\n",
    "print(\"\\n===== Best Parameters (AUC) =====\")\n",
    "print(f\"Best AUC score: {study_auc.best_value:.4f}\")\n",
    "print(\"Best parameter combination:\")\n",
    "for key, value in study_auc.best_params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Visualize optimization process\n",
    "plt.figure(figsize=(12, 8))\n",
    "optuna.visualization.matplotlib.plot_optimization_history(study_auc)\n",
    "plt.title('Optimization History - AUC')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{plots_dir}/optuna_catboost_auc_history.png')\n",
    "plt.close()\n",
    "\n",
    "# Visualize hyperparameter importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "optuna.visualization.matplotlib.plot_param_importances(study_auc)\n",
    "plt.title('Hyperparameter Importance')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{plots_dir}/optuna_catboost_auc_param_importances.png')\n",
    "plt.close()\n",
    "\n",
    "# 6. Train final model with best parameters\n",
    "print(\"\\n===== Training Final Model with Best Parameters on GPU =====\")\n",
    "best_params = study_auc.best_params.copy()\n",
    "best_params['loss_function'] = 'Logloss'\n",
    "best_params['eval_metric'] = 'AUC'\n",
    "best_params['random_seed'] = 42\n",
    "\n",
    "# Add GPU parameters if available\n",
    "if GPU_AVAILABLE:\n",
    "    best_params['task_type'] = 'GPU'\n",
    "    best_params['devices'] = '0'\n",
    "    print(\"Using GPU acceleration for final model training\")\n",
    "else:\n",
    "    print(\"Using CPU for final model training\")\n",
    "\n",
    "# Create final model\n",
    "final_model = cb.CatBoostClassifier(**best_params)\n",
    "\n",
    "# Train final model\n",
    "start_time = time.time()\n",
    "final_model.fit(\n",
    "    X_train_final, y_train,\n",
    "    eval_set=[(X_val_final, y_val)],\n",
    "    early_stopping_rounds=100,\n",
    "    verbose=50  # Show progress every 50 iterations\n",
    ")\n",
    "end_time = time.time()\n",
    "print(f\"Final model training completed! Duration: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# 7. Evaluate final model on validation set\n",
    "y_val_prob = final_model.predict_proba(X_val_final)[:, 1]\n",
    "y_val_pred = final_model.predict(X_val_final)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "val_auc = roc_auc_score(y_val, y_val_prob)\n",
    "val_acc = accuracy_score(y_val, y_val_pred)\n",
    "val_f1 = f1_score(y_val, y_val_pred)\n",
    "val_f1_weighted = f1_score(y_val, y_val_pred, average='weighted')\n",
    "\n",
    "print(\"\\n===== Final Model Performance on Validation Set =====\")\n",
    "print(f\"AUC: {val_auc:.4f}\")  # Highlight AUC\n",
    "print(f\"Weighted F1 Score: {val_f1_weighted:.4f}\")\n",
    "print(f\"F1 Score: {val_f1:.4f}\")\n",
    "print(f\"Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "print(\"\\nValidation Set Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_val_pred))\n",
    "print(\"\\nValidation Set Classification Report:\")\n",
    "print(classification_report(y_val, y_val_pred, digits=4))\n",
    "\n",
    "# 8. Evaluate final model on test set\n",
    "y_test_prob = final_model.predict_proba(X_test_final)[:, 1]\n",
    "y_test_pred = final_model.predict(X_test_final)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "test_auc = roc_auc_score(y_test, y_test_prob)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "test_f1_weighted = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "print(\"\\n===== Final Model Performance on Test Set =====\")\n",
    "print(f\"AUC: {test_auc:.4f}\")  # Highlight AUC\n",
    "print(f\"Weighted F1 Score: {test_f1_weighted:.4f}\")\n",
    "print(f\"F1 Score: {test_f1:.4f}\")\n",
    "print(f\"Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "print(\"\\nTest Set Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "print(\"\\nTest Set Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, digits=4))\n",
    "\n",
    "# 9. Plot ROC and PR curves\n",
    "# ROC curve\n",
    "plt.figure(figsize=(12, 10))\n",
    "# Validation set\n",
    "fpr_val, tpr_val, _ = roc_curve(y_val, y_val_prob)\n",
    "plt.plot(fpr_val, tpr_val, label=f'Validation Set (AUC = {val_auc:.4f})')\n",
    "# Test set\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, y_test_prob)\n",
    "plt.plot(fpr_test, tpr_test, label=f'Test Set (AUC = {test_auc:.4f})')\n",
    "# Reference line\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison (CatBoost GPU Model)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{plots_dir}/catboost_gpu_roc_curves.png')\n",
    "plt.close()\n",
    "\n",
    "# PR curve\n",
    "plt.figure(figsize=(12, 10))\n",
    "# Validation set\n",
    "prec_val, rec_val, _ = precision_recall_curve(y_val, y_val_prob)\n",
    "avg_prec_val = average_precision_score(y_val, y_val_prob)\n",
    "plt.plot(rec_val, prec_val, label=f'Validation Set (AP = {avg_prec_val:.4f})')\n",
    "# Test set\n",
    "prec_test, rec_test, _ = precision_recall_curve(y_test, y_test_prob)\n",
    "avg_prec_test = average_precision_score(y_test, y_test_prob)\n",
    "plt.plot(rec_test, prec_test, label=f'Test Set (AP = {avg_prec_test:.4f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('PR Curve Comparison (CatBoost GPU Model)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{plots_dir}/catboost_gpu_pr_curves.png')\n",
    "plt.close()\n",
    "\n",
    "# 10. Feature importance visualization\n",
    "feature_importance = final_model.get_feature_importance()\n",
    "feature_names = X_train_final.columns\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
    "importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.barplot(x='Importance', y='Feature', data=importance_df.head(20))\n",
    "plt.title(\"CatBoost Feature Importance (GPU Model)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{plots_dir}/catboost_gpu_feature_importance.png')\n",
    "plt.close()\n",
    "\n",
    "# 11. Save best model\n",
    "model_path = f'{models_dir}/catboost_gpu_auc.cbm'\n",
    "final_model.save_model(model_path)\n",
    "print(f\"\\nBest model saved to: {model_path}\")\n",
    "\n",
    "# 12. Performance summary\n",
    "print(\"\\n===== Model Performance Summary =====\")\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['AUC', 'Accuracy', 'F1 Score', 'Weighted F1 Score'],\n",
    "    'Validation Set': [val_auc, val_acc, val_f1, val_f1_weighted],\n",
    "    'Test Set': [test_auc, test_acc, test_f1, test_f1_weighted]\n",
    "})\n",
    "print(results_df)\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv(f'{results_dir}/catboost_gpu_auc_performance.csv', index=False)\n",
    "print(f\"Model performance saved to: {results_dir}/catboost_gpu_auc_performance.csv\")\n",
    "\n",
    "# 13. Save feature list for future use\n",
    "with open(f'{models_dir}/catboost_gpu_selected_features.txt', 'w') as f:\n",
    "    f.write('\\n'.join(selected_features))\n",
    "print(f\"Feature list saved to: {models_dir}/catboost_gpu_selected_features.txt\")\n",
    "\n",
    "print(\"\\nAnalysis completed! All results and visualizations have been saved to the specified directories.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ada5552",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d52406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import optuna  # For hyperparameter optimization\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, confusion_matrix, classification_report, \n",
    "                           roc_auc_score, roc_curve, precision_recall_curve, \n",
    "                           average_precision_score, f1_score)\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'  \n",
    "\n",
    "# Set up directory structure for outputs\n",
    "model_name = \"lightgbm\"  # Can be changed to different model names\n",
    "base_dir = f'/data/jinming/ee_stable/{model_name}'\n",
    "plots_dir = f'{base_dir}/plots'\n",
    "models_dir = f'{base_dir}/models'\n",
    "results_dir = f'{base_dir}/results'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for directory in [plots_dir, models_dir, results_dir]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# 1. Load data\n",
    "print(\"===== Loading Data =====\")\n",
    "train_df = pd.read_csv('/data/jinming/ee_stable/data/train.csv')\n",
    "test_df = pd.read_csv('/data/jinming/ee_stable/data/test.csv')\n",
    "val_df = pd.read_csv('/data/jinming/ee_stable/data/val.csv')\n",
    "\n",
    "# 2. Data preparation\n",
    "print(\"===== Preparing Data =====\")\n",
    "X_train = train_df.drop(['stab', 'stabf_encoded', 'stabf', 'p1', 'p2', 'p3', 'p4'], axis=1)\n",
    "X_test = test_df.drop(['stab', 'stabf_encoded', 'stabf', 'p1', 'p2', 'p3', 'p4'], axis=1)\n",
    "X_val = val_df.drop(['stab', 'stabf_encoded', 'stabf', 'p1', 'p2', 'p3', 'p4'], axis=1)\n",
    "\n",
    "y_train = train_df['stabf_encoded']\n",
    "y_test = test_df['stabf_encoded']\n",
    "y_val = val_df['stabf_encoded']\n",
    "\n",
    "print(f\"Dataset dimensions - Train: {X_train.shape}, Validation: {X_val.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# 3. Feature engineering function\n",
    "def create_features(X_train, X_test, X_val):\n",
    "    # Deep copy to avoid modifying original data\n",
    "    X_train_new = X_train.copy()\n",
    "    X_test_new = X_test.copy()\n",
    "    X_val_new = X_val.copy()\n",
    "    \n",
    "    # Basic interaction features\n",
    "    for df in [X_train_new, X_test_new, X_val_new]:\n",
    "        df['tau1_g1'] = df['tau1'] * df['g1']\n",
    "        df['tau2_g2'] = df['tau2'] * df['g2']\n",
    "        df['tau3_g3'] = df['tau3'] * df['g3']\n",
    "        df['tau4_g4'] = df['tau4'] * df['g4']\n",
    "        \n",
    "        # Delay ratio\n",
    "        df['tau_ratio'] = df[['tau1', 'tau2', 'tau3', 'tau4']].max(axis=1) / df[['tau1', 'tau2', 'tau3', 'tau4']].min(axis=1).replace(0, 0.001)\n",
    "        \n",
    "        # Delay-elasticity ratio: response sensitivity of each node\n",
    "        df['tau1_g1_ratio'] = df['tau1'] / df['g1'].replace(0, 0.001)\n",
    "        df['tau2_g2_ratio'] = df['tau2'] / df['g2'].replace(0, 0.001)\n",
    "        df['tau3_g3_ratio'] = df['tau3'] / df['g3'].replace(0, 0.001)\n",
    "        df['tau4_g4_ratio'] = df['tau4'] / df['g4'].replace(0, 0.001)\n",
    "        \n",
    "        # System total elasticity\n",
    "        df['total_elasticity'] = df['g1'] + df['g2'] + df['g3'] + df['g4']\n",
    "        \n",
    "        # Elasticity distribution non-uniformity\n",
    "        df['elasticity_disparity'] = df[['g1', 'g2', 'g3', 'g4']].max(axis=1) / df[['g1', 'g2', 'g3', 'g4']].min(axis=1).replace(0, 0.001)\n",
    "        \n",
    "        # Non-linear features - quadratic terms\n",
    "        df['tau1_squared'] = df['tau1'] ** 2\n",
    "        df['tau2_squared'] = df['tau2'] ** 2\n",
    "        df['tau3_squared'] = df['tau3'] ** 2\n",
    "        df['tau4_squared'] = df['tau4'] ** 2\n",
    "        \n",
    "        # Node relationship features\n",
    "        df['tau_g_correlation'] = (\n",
    "            (df['tau1'] * df['g1']) + \n",
    "            (df['tau2'] * df['g2']) + \n",
    "            (df['tau3'] * df['g3']) + \n",
    "            (df['tau4'] * df['g4'])\n",
    "        ) / (df['tau1'] + df['tau2'] + df['tau3'] + df['tau4'] + 0.001)\n",
    "        \n",
    "        # System overall response speed indicator\n",
    "        df['system_response_speed'] = 4 / (\n",
    "            (1/df['tau1'].replace(0, 0.001)) + \n",
    "            (1/df['tau2'].replace(0, 0.001)) + \n",
    "            (1/df['tau3'].replace(0, 0.001)) + \n",
    "            (1/df['tau4'].replace(0, 0.001))\n",
    "        )\n",
    "    \n",
    "    return X_train_new, X_test_new, X_val_new\n",
    "\n",
    "# 4. Feature selection functions\n",
    "def select_features(X_train, X_test, X_val, y_train):\n",
    "    \"\"\"\n",
    "    Feature selection based on correlation and importance\n",
    "    \"\"\"\n",
    "    print(\"\\n===== Starting Feature Selection =====\")\n",
    "    \n",
    "    # Step 1: Calculate correlation with target\n",
    "    print(\"Step 1: Calculating feature-target correlations\")\n",
    "    \n",
    "    feature_target_corr = {}\n",
    "    for col in X_train.columns:\n",
    "        corr = abs(np.corrcoef(X_train[col], y_train)[0, 1])\n",
    "        feature_target_corr[col] = corr\n",
    "    \n",
    "    feature_corr_df = pd.DataFrame({\n",
    "        'Feature': list(feature_target_corr.keys()),\n",
    "        'Target_Correlation': list(feature_target_corr.values())\n",
    "    }).sort_values('Target_Correlation', ascending=False)\n",
    "    \n",
    "    # Visualize correlations with target\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.barplot(x='Target_Correlation', y='Feature', data=feature_corr_df.head(20))\n",
    "    plt.title('Feature Correlation with Target Variable')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{plots_dir}/target_correlation.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Top 10 features with highest target correlation:\")\n",
    "    print(feature_corr_df.head(10))\n",
    "    \n",
    "    # Step 2: Remove highly correlated features\n",
    "    print(\"\\nStep 2: Removing redundant features\")\n",
    "    \n",
    "    corr_matrix = X_train.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    \n",
    "    # Visualize correlation matrix\n",
    "    plt.figure(figsize=(16, 14))\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    sns.heatmap(corr_matrix, cmap='coolwarm', center=0, mask=mask,\n",
    "                square=True, linewidths=.5, annot=False, fmt='.2f')\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{plots_dir}/feature_correlation.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Highly correlated feature pairs\n",
    "    correlation_threshold = 0.7\n",
    "    to_drop = set()\n",
    "    \n",
    "    for i, row_name in enumerate(upper.index):\n",
    "        for col_name in upper.columns[i:]:\n",
    "            if upper.loc[row_name, col_name] > correlation_threshold:\n",
    "                if feature_target_corr[row_name] > feature_target_corr[col_name]:\n",
    "                    to_drop.add(col_name)\n",
    "                else:\n",
    "                    to_drop.add(row_name)\n",
    "    \n",
    "    print(f\"Removing {len(to_drop)} highly correlated redundant features:\")\n",
    "    print(\", \".join(list(to_drop)))\n",
    "    \n",
    "    # Remove redundant features\n",
    "    X_train_filtered = X_train.drop(columns=list(to_drop))\n",
    "    X_test_filtered = X_test.drop(columns=list(to_drop))\n",
    "    X_val_filtered = X_val.drop(columns=list(to_drop))\n",
    "    \n",
    "    # Step 3: Model-based feature importance\n",
    "    print(\"\\nStep 3: Feature selection based on model importance\")\n",
    "    \n",
    "    # Train a LightGBM model for feature importance assessment\n",
    "    feature_selector = lgb.LGBMClassifier(\n",
    "        objective='binary',\n",
    "        metric='auc',\n",
    "        boosting_type='gbdt',\n",
    "        num_leaves=31,\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=100,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    feature_selector.fit(X_train_filtered, y_train)\n",
    "    \n",
    "    # Get feature importance\n",
    "    importances = feature_selector.feature_importances_\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': X_train_filtered.columns,\n",
    "        'Importance': importances\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Visualize feature importance\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(20))\n",
    "    plt.title('LightGBM Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{plots_dir}/feature_importance.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Dynamic threshold setting\n",
    "    mean_importance = feature_importance_df['Importance'].mean()\n",
    "    importance_threshold = mean_importance * 0.5\n",
    "    print(f\"Dynamic threshold: {importance_threshold:.2f} (50% of mean importance)\")\n",
    "    selected_features = feature_importance_df[feature_importance_df['Importance'] > importance_threshold]['Feature'].tolist()\n",
    "    \n",
    "    # Keep at least 10 most important features if filtered list is too small\n",
    "    if len(selected_features) < 10:\n",
    "        selected_features = feature_importance_df.head(10)['Feature'].tolist()\n",
    "    \n",
    "    print(f\"\\nFinally selected {len(selected_features)}/{X_train.shape[1]} features\")\n",
    "    print(f\"Selected features: {', '.join(selected_features)}\")\n",
    "    \n",
    "    return X_train[selected_features], X_test[selected_features], X_val[selected_features], selected_features\n",
    "\n",
    "# Manual feature selection function\n",
    "def select_features_manual(X_train, X_test, X_val):\n",
    "    \"\"\"Manually select specified feature set\"\"\"\n",
    "    print(\"\\n===== Using Manually Specified Features =====\")\n",
    "    \n",
    "    # Specify features to keep\n",
    "    selected_features = [\n",
    "        # Original tau features\n",
    "        'tau1', 'tau2', 'tau3', 'tau4',\n",
    "        \n",
    "        # Original g features\n",
    "        'g1', 'g2', 'g3', 'g4',\n",
    "        \n",
    "        # tau and g interaction terms\n",
    "        'tau1_g1', 'tau2_g2', 'tau3_g3', 'tau4_g4',\n",
    "        \n",
    "        # tau ratio features\n",
    "        'tau_ratio'\n",
    "    ]\n",
    "    \n",
    "    # Verify all specified features exist\n",
    "    missing_features = [f for f in selected_features if f not in X_train.columns]\n",
    "    if missing_features:\n",
    "        print(f\"Warning: The following specified features do not exist: {', '.join(missing_features)}\")\n",
    "        # Filter out non-existent features\n",
    "        selected_features = [f for f in selected_features if f in X_train.columns]\n",
    "    \n",
    "    print(f\"Using {len(selected_features)} specified features:\")\n",
    "    print(f\"Selected features: {', '.join(selected_features)}\")\n",
    "    \n",
    "    return X_train[selected_features], X_test[selected_features], X_val[selected_features], selected_features\n",
    "\n",
    "# Apply feature engineering\n",
    "print(\"\\n===== Performing Feature Engineering =====\")\n",
    "X_train_featured, X_test_featured, X_val_featured = create_features(X_train, X_test, X_val)\n",
    "print(f\"Number of features after engineering: {X_train_featured.shape[1]}\")\n",
    "\n",
    "# Apply feature selection - can choose automatic or manual method\n",
    "# Use manual feature selection\n",
    "X_train_selected, X_test_selected, X_val_selected, selected_features = select_features_manual(\n",
    "   X_train_featured, X_test_featured, X_val_featured)\n",
    "\n",
    "print(f\"Number of features after selection: {X_train_selected.shape[1]}\")\n",
    "\n",
    "# Use the selected features\n",
    "X_train_final = X_train_selected\n",
    "X_test_final = X_test_selected\n",
    "X_val_final = X_val_selected\n",
    "\n",
    "# 5. Optuna hyperparameter optimization for AUC\n",
    "print(\"\\n===== Starting Optuna Hyperparameter Tuning Process (AUC) =====\")\n",
    "\n",
    "\n",
    "def objective_auc(trial):\n",
    "    # Define LightGBM parameter search space\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbosity': -1,\n",
    "        \n",
    "        # Core parameters\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.5, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 30, 1000),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 150),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "        \n",
    "        # Regularization parameters\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "        \n",
    "        # Other parameters\n",
    "        'min_split_gain': trial.suggest_float('min_split_gain', 0, 0.5),\n",
    "        'min_child_weight': trial.suggest_float('min_child_weight', 1e-5, 10.0, log=True),\n",
    "        \n",
    "        # GPU acceleration parameters \n",
    "        'device': 'gpu',\n",
    "        'gpu_platform_id': 0,\n",
    "        'gpu_device_id': 0,\n",
    "        'num_gpu': 1, \n",
    "        'n_jobs': 16,    \n",
    "\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    # Rest of the function remains the same\n",
    "    # ...\n",
    "    \n",
    "    # Create LightGBM model\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    \n",
    "    # Train model on the training set, using validation set for early stopping\n",
    "    model.fit(\n",
    "        X_train_final, y_train,\n",
    "        eval_set=[(X_val_final, y_val)],\n",
    "        eval_metric='auc',\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]\n",
    "    )\n",
    "    \n",
    "    # Predict on validation set\n",
    "    y_val_prob = model.predict_proba(X_val_final)[:, 1]\n",
    "    \n",
    "    # Calculate AUC score\n",
    "    auc_score = roc_auc_score(y_val, y_val_prob)\n",
    "    \n",
    "    # Print current trial results\n",
    "    print(f\"Trial {trial.number}: AUC = {auc_score:.4f}\")\n",
    "    \n",
    "    return auc_score  # Return AUC as optimization target\n",
    "\n",
    "# Create Optuna study object - optimization direction is to maximize AUC\n",
    "study_auc = optuna.create_study(direction='maximize', study_name='lightgbm_auc_optimization')\n",
    "\n",
    "# Run optimization\n",
    "n_trials = 10  # Can be adjusted based on computational resources and time\n",
    "print(f\"Starting {n_trials} hyperparameter tuning trials...\")\n",
    "start_time = time.time()\n",
    "study_auc.optimize(objective_auc, n_trials=n_trials)\n",
    "end_time = time.time()\n",
    "print(f\"Tuning completed! Duration: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Print best parameters and results\n",
    "print(\"\\n===== Best Parameters (AUC) =====\")\n",
    "print(f\"Best AUC score: {study_auc.best_value:.4f}\")\n",
    "print(\"Best parameter combination:\")\n",
    "for key, value in study_auc.best_params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Visualize optimization process\n",
    "plt.figure(figsize=(12, 8))\n",
    "optuna.visualization.matplotlib.plot_optimization_history(study_auc)\n",
    "plt.title('Optimization History - AUC')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{plots_dir}/optuna_auc_history.png')\n",
    "plt.close()\n",
    "\n",
    "# Visualize hyperparameter importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "optuna.visualization.matplotlib.plot_param_importances(study_auc)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{plots_dir}/optuna_auc_param_importances.png')\n",
    "plt.close()\n",
    "\n",
    "# 6. Train final model with best parameters for weighted F1\n",
    "print(\"\\n===== Training Final Model with Best Parameters =====\")\n",
    "best_params_auc = study_auc.best_params.copy()\n",
    "best_params_auc['objective'] = 'binary'\n",
    "best_params_auc['metric'] = 'auc'\n",
    "best_params_auc['boosting_type'] = 'gbdt'\n",
    "best_params_auc['random_state'] = 42\n",
    "best_params_auc['verbosity'] = -1\n",
    "\n",
    "\n",
    "best_params_auc['device'] = 'gpu'\n",
    "best_params_auc['gpu_platform_id'] = 0  \n",
    "best_params_auc['gpu_device_id'] = 0    \n",
    "best_params_auc['num_gpu'] = 1\n",
    "best_params_auc['n_jobs'] = 2   \n",
    "\n",
    "if 'num_threads' in best_params_auc:\n",
    "    del best_params_auc['num_threads']\n",
    "\n",
    "final_model_auc = lgb.LGBMClassifier(**best_params_auc)\n",
    "\n",
    "start_time = time.time()\n",
    "final_model_auc.fit(\n",
    "    X_train_final, y_train,\n",
    "    eval_set=[(X_val_final, y_val)],\n",
    "    eval_metric='auc',\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=True)]\n",
    ")\n",
    "end_time = time.time()\n",
    "print(f\"Final model training completed! Duration: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# 7. Evaluate final model on validation set\n",
    "y_val_prob = final_model_auc.predict_proba(X_val_final)[:, 1]\n",
    "y_val_pred = final_model_auc.predict(X_val_final)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "val_auc = roc_auc_score(y_val, y_val_prob)\n",
    "val_acc = accuracy_score(y_val, y_val_pred)\n",
    "val_f1 = f1_score(y_val, y_val_pred)\n",
    "val_f1_weighted = f1_score(y_val, y_val_pred, average='weighted')\n",
    "\n",
    "print(\"\\n===== Final Model Performance on Validation Set =====\")\n",
    "print(f\"AUC: {val_auc:.4f}\")  # Highlight AUC\n",
    "print(f\"Weighted F1 Score: {val_f1_weighted:.4f}\")\n",
    "print(f\"F1 Score: {val_f1:.4f}\")\n",
    "print(f\"Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "print(\"\\nValidation Set Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_val_pred))\n",
    "print(\"\\nValidation Set Classification Report:\")\n",
    "print(classification_report(y_val, y_val_pred, digits=4))\n",
    "\n",
    "# 8. Evaluate final model on test set\n",
    "y_test_prob = final_model_auc.predict_proba(X_test_final)[:, 1]\n",
    "y_test_pred = final_model_auc.predict(X_test_final)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "test_auc = roc_auc_score(y_test, y_test_prob)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "test_f1_weighted = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "print(\"\\n===== Final Model Performance on Test Set =====\")\n",
    "print(f\"AUC: {test_auc:.4f}\")  # Highlight AUC\n",
    "print(f\"Weighted F1 Score: {test_f1_weighted:.4f}\")\n",
    "print(f\"F1 Score: {test_f1:.4f}\")\n",
    "print(f\"Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "print(\"\\nTest Set Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "print(\"\\nTest Set Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, digits=4))\n",
    "\n",
    "# 9. Plot ROC and PR curves\n",
    "# ROC curve\n",
    "plt.figure(figsize=(12, 10))\n",
    "# Validation set\n",
    "fpr_val, tpr_val, _ = roc_curve(y_val, y_val_prob)\n",
    "plt.plot(fpr_val, tpr_val, label=f'Validation Set (AUC = {val_auc:.4f})')\n",
    "# Test set\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, y_test_prob)\n",
    "plt.plot(fpr_test, tpr_test, label=f'Test Set (AUC = {test_auc:.4f})')\n",
    "# Reference line\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{plots_dir}/final_model_roc_curves.png')\n",
    "plt.close()\n",
    "\n",
    "# PR curve\n",
    "plt.figure(figsize=(12, 10))\n",
    "# Validation set\n",
    "prec_val, rec_val, _ = precision_recall_curve(y_val, y_val_prob)\n",
    "avg_prec_val = average_precision_score(y_val, y_val_prob)\n",
    "plt.plot(rec_val, prec_val, label=f'Validation Set (AP = {avg_prec_val:.4f})')\n",
    "# Test set\n",
    "prec_test, rec_test, _ = precision_recall_curve(y_test, y_test_prob)\n",
    "avg_prec_test = average_precision_score(y_test, y_test_prob)\n",
    "plt.plot(rec_test, prec_test, label=f'Test Set (AP = {avg_prec_test:.4f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('PR Curve Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{plots_dir}/final_model_pr_curves.png')\n",
    "plt.close()\n",
    "\n",
    "# 10. Feature importance visualization\n",
    "plt.figure(figsize=(14, 10))\n",
    "lgb.plot_importance(final_model_auc, max_num_features=20, importance_type='gain')\n",
    "plt.title(\"LightGBM Feature Importance (Gain)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{plots_dir}/final_model_feature_importance.png')\n",
    "plt.close()\n",
    "\n",
    "# 11. Save best model\n",
    "import joblib\n",
    "model_path = f'{models_dir}/lgbm_optuna_auc.pkl'\n",
    "joblib.dump(final_model_auc, model_path)\n",
    "print(f\"\\nBest model saved to: {model_path}\")\n",
    "\n",
    "# 12. Performance summary\n",
    "print(\"\\n===== Model Performance Summary =====\")\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['AUC', 'Accuracy', 'F1 Score', 'Weighted F1 Score'],\n",
    "    'Validation Set': [val_auc, val_acc, val_f1, val_f1_weighted],\n",
    "    'Test Set': [test_auc, test_acc, test_f1, test_f1_weighted]\n",
    "})\n",
    "print(results_df)\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv(f'{results_dir}/model_performance.csv', index=False)\n",
    "print(f\"Model performance saved to: {results_dir}/model_performance_auc.csv\")\n",
    "\n",
    "# 13. Save feature list for future use\n",
    "with open(f'{models_dir}/selected_features.txt', 'w') as f:\n",
    "    f.write('\\n'.join(selected_features))\n",
    "print(f\"Feature list saved to: {models_dir}/selected_features.txt\")\n",
    "\n",
    "print(\"\\nAnalysis completed! All results and visualizations have been saved to the specified directories.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc23ed41",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420e20be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, confusion_matrix, classification_report, \n",
    "                           roc_auc_score, roc_curve, precision_recall_curve, \n",
    "                           average_precision_score, f1_score)\n",
    "import xgboost as xgb  # Using XGBoost instead of LightGBM\n",
    "import optuna  # For hyperparameter optimization\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ensure output directories exist\n",
    "plots_dir = '/data/jinming/ee_stable/xgboost/plots'\n",
    "models_dir = '/data/jinming/ee_stable/xgboost/models'\n",
    "results_dir = '/data/jinming/ee_stable/xgboost/results'\n",
    "for directory in [plots_dir, models_dir, results_dir]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# 1. Load data\n",
    "print(\"===== Loading Data =====\")\n",
    "train_df = pd.read_csv('/data/jinming/ee_stable/data/train.csv')\n",
    "test_df = pd.read_csv('/data/jinming/ee_stable/data/test.csv')\n",
    "val_df = pd.read_csv('/data/jinming/ee_stable/data/val.csv')\n",
    "\n",
    "# 2. Data preparation\n",
    "print(\"===== Preparing Data =====\")\n",
    "X_train = train_df.drop(['stab', 'stabf_encoded', 'stabf', 'p1', 'p2', 'p3', 'p4'], axis=1)\n",
    "X_test = test_df.drop(['stab', 'stabf_encoded', 'stabf', 'p1', 'p2', 'p3', 'p4'], axis=1)\n",
    "X_val = val_df.drop(['stab', 'stabf_encoded', 'stabf', 'p1', 'p2', 'p3', 'p4'], axis=1)\n",
    "\n",
    "y_train = train_df['stabf_encoded']\n",
    "y_test = test_df['stabf_encoded']\n",
    "y_val = val_df['stabf_encoded']\n",
    "\n",
    "print(f\"Dataset dimensions - Train: {X_train.shape}, Validation: {X_val.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# 3. Feature engineering function\n",
    "def create_features(X_train, X_test, X_val):\n",
    "    # Deep copy to avoid modifying original data\n",
    "    X_train_new = X_train.copy()\n",
    "    X_test_new = X_test.copy()\n",
    "    X_val_new = X_val.copy()\n",
    "    \n",
    "    # Basic interaction features\n",
    "    for df in [X_train_new, X_test_new, X_val_new]:\n",
    "        df['tau1_g1'] = df['tau1'] * df['g1']\n",
    "        df['tau2_g2'] = df['tau2'] * df['g2']\n",
    "        df['tau3_g3'] = df['tau3'] * df['g3']\n",
    "        df['tau4_g4'] = df['tau4'] * df['g4']\n",
    "        \n",
    "        # Delay ratio\n",
    "        df['tau_ratio'] = df[['tau1', 'tau2', 'tau3', 'tau4']].max(axis=1) / df[['tau1', 'tau2', 'tau3', 'tau4']].min(axis=1).replace(0, 0.001)\n",
    "        \n",
    "        # Delay-elasticity ratio: response sensitivity of each node\n",
    "        df['tau1_g1_ratio'] = df['tau1'] / df['g1'].replace(0, 0.001)\n",
    "        df['tau2_g2_ratio'] = df['tau2'] / df['g2'].replace(0, 0.001)\n",
    "        df['tau3_g3_ratio'] = df['tau3'] / df['g3'].replace(0, 0.001)\n",
    "        df['tau4_g4_ratio'] = df['tau4'] / df['g4'].replace(0, 0.001)\n",
    "        \n",
    "        # System total elasticity\n",
    "        df['total_elasticity'] = df['g1'] + df['g2'] + df['g3'] + df['g4']\n",
    "        \n",
    "        # Elasticity distribution non-uniformity\n",
    "        df['elasticity_disparity'] = df[['g1', 'g2', 'g3', 'g4']].max(axis=1) / df[['g1', 'g2', 'g3', 'g4']].min(axis=1).replace(0, 0.001)\n",
    "        \n",
    "        # Non-linear features - quadratic terms\n",
    "        df['tau1_squared'] = df['tau1'] ** 2\n",
    "        df['tau2_squared'] = df['tau2'] ** 2\n",
    "        df['tau3_squared'] = df['tau3'] ** 2\n",
    "        df['tau4_squared'] = df['tau4'] ** 2\n",
    "        \n",
    "        # Node relationship features\n",
    "        df['tau_g_correlation'] = (\n",
    "            (df['tau1'] * df['g1']) + \n",
    "            (df['tau2'] * df['g2']) + \n",
    "            (df['tau3'] * df['g3']) + \n",
    "            (df['tau4'] * df['g4'])\n",
    "        ) / (df['tau1'] + df['tau2'] + df['tau3'] + df['tau4'] + 0.001)\n",
    "        \n",
    "        # System overall response speed indicator\n",
    "        df['system_response_speed'] = 4 / (\n",
    "            (1/df['tau1'].replace(0, 0.001)) + \n",
    "            (1/df['tau2'].replace(0, 0.001)) + \n",
    "            (1/df['tau3'].replace(0, 0.001)) + \n",
    "            (1/df['tau4'].replace(0, 0.001))\n",
    "        )\n",
    "    \n",
    "    return X_train_new, X_test_new, X_val_new\n",
    "\n",
    "# Manual feature selection function\n",
    "def select_features_manual(X_train, X_test, X_val):\n",
    "    \"\"\"Manually select specified feature set\"\"\"\n",
    "    print(\"\\n===== Using Manually Specified Features =====\")\n",
    "    \n",
    "    # Specify features to keep\n",
    "    selected_features = [\n",
    "        # Original tau features\n",
    "        'tau1', 'tau2', 'tau3', 'tau4',\n",
    "        \n",
    "        # Original g features\n",
    "        'g1', 'g2', 'g3', 'g4',\n",
    "        \n",
    "        # tau and g interaction terms\n",
    "        'tau1_g1', 'tau2_g2', 'tau3_g3', 'tau4_g4',\n",
    "        \n",
    "        # tau ratio features\n",
    "        'tau_ratio'\n",
    "    ]\n",
    "    \n",
    "    # Verify all specified features exist\n",
    "    missing_features = [f for f in selected_features if f not in X_train.columns]\n",
    "    if missing_features:\n",
    "        print(f\"Warning: The following specified features do not exist: {', '.join(missing_features)}\")\n",
    "        # Filter out non-existent features\n",
    "        selected_features = [f for f in selected_features if f in X_train.columns]\n",
    "    \n",
    "    print(f\"Using {len(selected_features)} specified features:\")\n",
    "    print(f\"Selected features: {', '.join(selected_features)}\")\n",
    "    \n",
    "    return X_train[selected_features], X_test[selected_features], X_val[selected_features], selected_features\n",
    "\n",
    "# Apply feature engineering\n",
    "print(\"\\n===== Performing Feature Engineering =====\")\n",
    "X_train_featured, X_test_featured, X_val_featured = create_features(X_train, X_test, X_val)\n",
    "print(f\"Number of features after engineering: {X_train_featured.shape[1]}\")\n",
    "\n",
    "# Apply feature selection using manual method\n",
    "X_train_final, X_test_final, X_val_final, selected_features = select_features_manual(\n",
    "   X_train_featured, X_test_featured, X_val_featured)\n",
    "\n",
    "print(f\"Number of features after selection: {X_train_final.shape[1]}\")\n",
    "\n",
    "# 5. 使用 Optuna 进行 XGBoost 超参数优化（GPU加速）\n",
    "print(\"\\n===== 开始 Optuna GPU加速调参过程 (AUC) =====\")\n",
    "\n",
    "def objective_auc(trial):\n",
    "    \"\"\"Optuna 优化目标函数 - 使用验证集上的 AUC 作为评价指标，并使用GPU加速\"\"\"\n",
    "    # 定义XGBoost参数搜索空间\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'verbosity': 0,\n",
    "        \n",
    "        # 核心参数\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.5, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        \n",
    "        # 正则化参数\n",
    "        'min_child_weight': trial.suggest_float('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 10),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-8, 10.0, log=True),\n",
    "        'lambda': trial.suggest_float('lambda', 1e-8, 10.0, log=True),\n",
    "        \n",
    "        # GPU加速相关参数\n",
    "        'tree_method': 'gpu_hist',  # 使用GPU直方图算法\n",
    "        'device': 'cuda',          \n",
    "\n",
    "        'scale_pos_weight': trial.suggest_float('scale_pos_weight', 0.1, 10.0),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    # 直接使用低级API (xgb.train) 进行训练，这在所有版本上都兼容并支持GPU\n",
    "    dtrain = xgb.DMatrix(X_train_final, label=y_train)\n",
    "    dval = xgb.DMatrix(X_val_final, label=y_val)\n",
    "    \n",
    "    # 设置评估集\n",
    "    evals = [(dtrain, 'train'), (dval, 'val')]\n",
    "    evals_result = {}\n",
    "    \n",
    "    # 训练模型，使用早停\n",
    "    model = xgb.train(\n",
    "        params=params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=2000,  # 最大迭代次数\n",
    "        evals=evals,\n",
    "        early_stopping_rounds=50,\n",
    "        evals_result=evals_result,\n",
    "        verbose_eval=False  # 只在试验结束时打印结果\n",
    "    )\n",
    "    \n",
    "    # 在验证集上预测概率\n",
    "    y_val_prob = model.predict(dval)\n",
    "    \n",
    "    # 计算 AUC 分数\n",
    "    auc_score = roc_auc_score(y_val, y_val_prob)\n",
    "    \n",
    "    # 打印当前试验的结果\n",
    "    print(f\"Trial {trial.number}: AUC = {auc_score:.4f}\")\n",
    "    \n",
    "    return auc_score  # 返回 AUC 作为优化目标\n",
    "\n",
    "# 创建Optuna study对象 - 优化方向是最大化AUC\n",
    "study_auc = optuna.create_study(direction='maximize', study_name='xgboost_gpu_auc_optimization')\n",
    "\n",
    "# 运行优化\n",
    "n_trials = 10  # 可以根据计算资源和时间调整\n",
    "print(f\"开始运行 {n_trials} 次GPU加速调参试验...\")\n",
    "start_time = time.time()\n",
    "study_auc.optimize(objective_auc, n_trials=n_trials)\n",
    "end_time = time.time()\n",
    "print(f\"GPU调参完成! 耗时: {end_time - start_time:.2f}秒\")\n",
    "\n",
    "# 打印最佳参数和结果\n",
    "print(\"\\n===== 最佳参数 (AUC) =====\")\n",
    "print(f\"最佳AUC分数: {study_auc.best_value:.4f}\")\n",
    "print(\"最佳参数组合:\")\n",
    "for key, value in study_auc.best_params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "\n",
    "# Visualize optimization process\n",
    "plt.figure(figsize=(12, 8))\n",
    "optuna.visualization.matplotlib.plot_optimization_history(study_auc)\n",
    "plt.title('Optimization History - AUC')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{plots_dir}/optuna_auc_history.png')\n",
    "plt.close()\n",
    "\n",
    "# Visualize hyperparameter importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "optuna.visualization.matplotlib.plot_param_importances(study_auc)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{plots_dir}/optuna_auc_param_importances.png')\n",
    "plt.close()\n",
    "\n",
    "# 6. 使用最佳参数在GPU上训练最终模型\n",
    "print(\"\\n===== 使用GPU训练最终模型 =====\")\n",
    "\n",
    "# 获取最佳参数\n",
    "best_params_auc = study_auc.best_params.copy()\n",
    "\n",
    "# 添加必要的固定参数\n",
    "best_params_auc.update({\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'tree_method': 'gpu_hist',  # 使用GPU加速\n",
    "    'device': 'cuda',           # 指定CUDA设备\n",
    "    'verbosity': 1,\n",
    "    'random_state': 42\n",
    "})\n",
    "\n",
    "# 准备数据矩阵\n",
    "dtrain = xgb.DMatrix(X_train_final, label=y_train)\n",
    "dval = xgb.DMatrix(X_val_final, label=y_val)\n",
    "dtest = xgb.DMatrix(X_test_final, label=y_test)\n",
    "\n",
    "# 设置评估集\n",
    "evals = [(dtrain, 'train'), (dval, 'val')]\n",
    "evals_result = {}\n",
    "\n",
    "# 开始训练计时\n",
    "start_time = time.time()\n",
    "\n",
    "# 使用低级API训练最终模型\n",
    "final_model = xgb.train(\n",
    "    params=best_params_auc,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=3000,  # 设置足够大的最大迭代次数\n",
    "    evals=evals,\n",
    "    early_stopping_rounds=50,\n",
    "    evals_result=evals_result,\n",
    "    verbose_eval=50  # 每50轮显示一次进度\n",
    ")\n",
    "\n",
    "# 结束训练计时并输出时间\n",
    "end_time = time.time()\n",
    "print(f\"GPU训练完成！耗时: {end_time - start_time:.2f}秒\")\n",
    "\n",
    "# 7. 评估最终模型在验证集上的表现\n",
    "y_val_prob = final_model.predict(dval)\n",
    "y_val_pred = (y_val_prob > 0.5).astype(int)  # 将概率转换为预测类别\n",
    "\n",
    "# 计算评估指标\n",
    "val_auc = roc_auc_score(y_val, y_val_prob)\n",
    "val_acc = accuracy_score(y_val, y_val_pred)\n",
    "val_f1 = f1_score(y_val, y_val_pred)\n",
    "val_f1_weighted = f1_score(y_val, y_val_pred, average='weighted')\n",
    "\n",
    "print(\"\\n===== 最终模型在验证集上的性能 =====\")\n",
    "print(f\"AUC: {val_auc:.4f}\")\n",
    "print(f\"Weighted F1 Score: {val_f1_weighted:.4f}\")\n",
    "print(f\"F1 Score: {val_f1:.4f}\")\n",
    "print(f\"Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "print(\"\\n验证集混淆矩阵:\")\n",
    "print(confusion_matrix(y_val, y_val_pred))\n",
    "print(\"\\n验证集分类报告:\")\n",
    "print(classification_report(y_val, y_val_pred, digits=4))\n",
    "\n",
    "# 8. 评估最终模型在测试集上的表现\n",
    "y_test_prob = final_model.predict(dtest)\n",
    "y_test_pred = (y_test_prob > 0.5).astype(int)\n",
    "\n",
    "# 计算评估指标\n",
    "test_auc = roc_auc_score(y_test, y_test_prob)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "test_f1_weighted = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "print(\"\\n===== 最终模型在测试集上的性能 =====\")\n",
    "print(f\"AUC: {test_auc:.4f}\")\n",
    "print(f\"Weighted F1 Score: {test_f1_weighted:.4f}\")\n",
    "print(f\"F1 Score: {test_f1:.4f}\")\n",
    "print(f\"Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# 9. Plot ROC and PR curves\n",
    "# ROC curve\n",
    "plt.figure(figsize=(12, 10))\n",
    "# Validation set\n",
    "fpr_val, tpr_val, _ = roc_curve(y_val, y_val_prob)\n",
    "plt.plot(fpr_val, tpr_val, label=f'Validation Set (AUC = {val_auc:.4f})')\n",
    "# Test set\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, y_test_prob)\n",
    "plt.plot(fpr_test, tpr_test, label=f'Test Set (AUC = {test_auc:.4f})')\n",
    "# Reference line\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison (XGBoost Model)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{plots_dir}/final_model_roc_curves.png')\n",
    "plt.close()\n",
    "\n",
    "# PR curve\n",
    "plt.figure(figsize=(12, 10))\n",
    "# Validation set\n",
    "prec_val, rec_val, _ = precision_recall_curve(y_val, y_val_prob)\n",
    "avg_prec_val = average_precision_score(y_val, y_val_prob)\n",
    "plt.plot(rec_val, prec_val, label=f'Validation Set (AP = {avg_prec_val:.4f})')\n",
    "# Test set\n",
    "prec_test, rec_test, _ = precision_recall_curve(y_test, y_test_prob)\n",
    "avg_prec_test = average_precision_score(y_test, y_test_prob)\n",
    "plt.plot(rec_test, prec_test, label=f'Test Set (AP = {avg_prec_test:.4f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('PR Curve Comparison (XGBoost Model)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{plots_dir}/final_model_pr_curves.png')\n",
    "plt.close()\n",
    "\n",
    "# 10. Feature importance visualization\n",
    "plt.figure(figsize=(14, 10))\n",
    "xgb.plot_importance(final_model_auc, max_num_features=20, importance_type='gain')\n",
    "plt.title(\"XGBoost Feature Importance (Gain)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{plots_dir}/final_model_feature_importance.png')\n",
    "plt.close()\n",
    "\n",
    "# 11. Save best model\n",
    "import joblib\n",
    "model_path = f'{models_dir}/xgb_optuna_auc.pkl'\n",
    "joblib.dump(final_model_auc, model_path)\n",
    "print(f\"\\nBest model saved to: {model_path}\")\n",
    "\n",
    "# 12. Performance summary\n",
    "print(\"\\n===== Model Performance Summary =====\")\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['AUC', 'Accuracy', 'F1 Score', 'Weighted F1 Score'],\n",
    "    'Validation Set': [val_auc, val_acc, val_f1, val_f1_weighted],\n",
    "    'Test Set': [test_auc, test_acc, test_f1, test_f1_weighted]\n",
    "})\n",
    "print(results_df)\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv(f'{results_dir}/model_performance_auc.csv', index=False)\n",
    "print(f\"Model performance saved to: {results_dir}/model_performance_auc.csv\")\n",
    "\n",
    "# 13. Save feature list for future use\n",
    "with open(f'{models_dir}/selected_features.txt', 'w') as f:\n",
    "    f.write('\\n'.join(selected_features))\n",
    "print(f\"Feature list saved to: {models_dir}/selected_features.txt\")\n",
    "\n",
    "print(\"\\nAnalysis completed! All results and visualizations have been saved to the specified directories.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
