{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2811dd32",
   "metadata": {},
   "source": [
    "# Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ac7b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, confusion_matrix, classification_report, \n",
    "                           roc_auc_score, roc_curve, precision_recall_curve, \n",
    "                           average_precision_score, f1_score)\n",
    "import catboost as cb  # Using CatBoost\n",
    "import optuna  # For hyperparameter optimization\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'  \n",
    "\n",
    "# Ensure output directories exist\n",
    "plots_dir = '/data/jinming/ee_stable/catboost/plots'\n",
    "models_dir = '/data/jinming/ee_stable/catboost/models'\n",
    "results_dir = '/data/jinming/ee_stable/catboost/results'\n",
    "for directory in [plots_dir, models_dir, results_dir]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# 1. Load data\n",
    "print(\"===== Loading Data =====\")\n",
    "train_df = pd.read_csv('/data/jinming/ee_stable/data/train.csv')\n",
    "test_df = pd.read_csv('/data/jinming/ee_stable/data/test.csv')\n",
    "val_df = pd.read_csv('/data/jinming/ee_stable/data/val.csv')\n",
    "\n",
    "# 2. Data preparation\n",
    "print(\"===== Preparing Data =====\")\n",
    "X_train = train_df.drop(['stab', 'stabf_encoded', 'stabf', 'p1', 'p2', 'p3', 'p4'], axis=1)\n",
    "X_test = test_df.drop(['stab', 'stabf_encoded', 'stabf', 'p1', 'p2', 'p3', 'p4'], axis=1)\n",
    "X_val = val_df.drop(['stab', 'stabf_encoded', 'stabf', 'p1', 'p2', 'p3', 'p4'], axis=1)\n",
    "\n",
    "y_train = train_df['stabf_encoded']\n",
    "y_test = test_df['stabf_encoded']\n",
    "y_val = val_df['stabf_encoded']\n",
    "\n",
    "print(f\"Dataset dimensions - Train: {X_train.shape}, Validation: {X_val.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# 3. Feature engineering function\n",
    "def create_features(X_train, X_test, X_val):\n",
    "    # Deep copy to avoid modifying original data\n",
    "    X_train_new = X_train.copy()\n",
    "    X_test_new = X_test.copy()\n",
    "    X_val_new = X_val.copy()\n",
    "    \n",
    "    # Basic interaction features\n",
    "    for df in [X_train_new, X_test_new, X_val_new]:\n",
    "        df['tau1_g1'] = df['tau1'] * df['g1']\n",
    "        df['tau2_g2'] = df['tau2'] * df['g2']\n",
    "        df['tau3_g3'] = df['tau3'] * df['g3']\n",
    "        df['tau4_g4'] = df['tau4'] * df['g4']\n",
    "        \n",
    "        # Delay ratio\n",
    "        df['tau_ratio'] = df[['tau1', 'tau2', 'tau3', 'tau4']].max(axis=1) / df[['tau1', 'tau2', 'tau3', 'tau4']].min(axis=1).replace(0, 0.001)\n",
    "        \n",
    "        # Delay-elasticity ratio: response sensitivity of each node\n",
    "        df['tau1_g1_ratio'] = df['tau1'] / df['g1'].replace(0, 0.001)\n",
    "        df['tau2_g2_ratio'] = df['tau2'] / df['g2'].replace(0, 0.001)\n",
    "        df['tau3_g3_ratio'] = df['tau3'] / df['g3'].replace(0, 0.001)\n",
    "        df['tau4_g4_ratio'] = df['tau4'] / df['g4'].replace(0, 0.001)\n",
    "        \n",
    "        # System total elasticity\n",
    "        df['total_elasticity'] = df['g1'] + df['g2'] + df['g3'] + df['g4']\n",
    "        \n",
    "        # Elasticity distribution non-uniformity\n",
    "        df['elasticity_disparity'] = df[['g1', 'g2', 'g3', 'g4']].max(axis=1) / df[['g1', 'g2', 'g3', 'g4']].min(axis=1).replace(0, 0.001)\n",
    "        \n",
    "        # Non-linear features - quadratic terms\n",
    "        df['tau1_squared'] = df['tau1'] ** 2\n",
    "        df['tau2_squared'] = df['tau2'] ** 2\n",
    "        df['tau3_squared'] = df['tau3'] ** 2\n",
    "        df['tau4_squared'] = df['tau4'] ** 2\n",
    "        \n",
    "        # Node relationship features\n",
    "        df['tau_g_correlation'] = (\n",
    "            (df['tau1'] * df['g1']) + \n",
    "            (df['tau2'] * df['g2']) + \n",
    "            (df['tau3'] * df['g3']) + \n",
    "            (df['tau4'] * df['g4'])\n",
    "        ) / (df['tau1'] + df['tau2'] + df['tau3'] + df['tau4'] + 0.001)\n",
    "        \n",
    "        # System overall response speed indicator\n",
    "        df['system_response_speed'] = 4 / (\n",
    "            (1/df['tau1'].replace(0, 0.001)) + \n",
    "            (1/df['tau2'].replace(0, 0.001)) + \n",
    "            (1/df['tau3'].replace(0, 0.001)) + \n",
    "            (1/df['tau4'].replace(0, 0.001))\n",
    "        )\n",
    "    \n",
    "    return X_train_new, X_test_new, X_val_new\n",
    "\n",
    "# Manual feature selection function\n",
    "def select_features_manual(X_train, X_test, X_val):\n",
    "    \"\"\"Manually select specified feature set\"\"\"\n",
    "    print(\"\\n===== Using Manually Specified Features =====\")\n",
    "    \n",
    "    # Specify features to keep\n",
    "    selected_features = [\n",
    "        # Original tau features\n",
    "        'tau1', 'tau2', 'tau3', 'tau4',\n",
    "        \n",
    "        # Original g features\n",
    "        'g1', 'g2', 'g3', 'g4',\n",
    "        \n",
    "        # tau and g interaction terms\n",
    "        'tau1_g1', 'tau2_g2', 'tau3_g3', 'tau4_g4',\n",
    "        \n",
    "        # tau ratio features\n",
    "        'tau_ratio'\n",
    "    ]\n",
    "    \n",
    "    # Verify all specified features exist\n",
    "    missing_features = [f for f in selected_features if f not in X_train.columns]\n",
    "    if missing_features:\n",
    "        print(f\"Warning: The following specified features do not exist: {', '.join(missing_features)}\")\n",
    "        # Filter out non-existent features\n",
    "        selected_features = [f for f in selected_features if f in X_train.columns]\n",
    "    \n",
    "    print(f\"Using {len(selected_features)} specified features:\")\n",
    "    print(f\"Selected features: {', '.join(selected_features)}\")\n",
    "    \n",
    "    return X_train[selected_features], X_test[selected_features], X_val[selected_features], selected_features\n",
    "\n",
    "# Apply feature engineering\n",
    "print(\"\\n===== Performing Feature Engineering =====\")\n",
    "X_train_featured, X_test_featured, X_val_featured = create_features(X_train, X_test, X_val)\n",
    "print(f\"Number of features after engineering: {X_train_featured.shape[1]}\")\n",
    "\n",
    "# Apply feature selection using manual method\n",
    "X_train_final, X_test_final, X_val_final, selected_features = select_features_manual(\n",
    "   X_train_featured, X_test_featured, X_val_featured)\n",
    "\n",
    "print(f\"Number of features after selection: {X_train_final.shape[1]}\")\n",
    "\n",
    "# Check if GPU is available for CatBoost\n",
    "print(\"\\n===== Checking GPU availability =====\")\n",
    "try:\n",
    "    # Create a more realistic test dataset for GPU test\n",
    "    X_test_gpu = np.random.rand(100, 8)  # 100 samples, 8 features\n",
    "    y_test_gpu = np.random.randint(0, 2, 100)  # Binary labels\n",
    "    \n",
    "    # Create a test CatBoost model with GPU support\n",
    "    test_model = cb.CatBoostClassifier(\n",
    "        iterations=10,\n",
    "        task_type='GPU',\n",
    "        devices='0',\n",
    "        verbose=False\n",
    "    )\n",
    "    test_model.fit(X_test_gpu, y_test_gpu, verbose=False)\n",
    "    print(\"GPU acceleration is available for CatBoost!\")\n",
    "    GPU_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    print(f\"GPU acceleration is NOT available: {e}\")\n",
    "    print(\"Falling back to CPU training\")\n",
    "    GPU_AVAILABLE = False\n",
    "    \n",
    "# 5. Optuna hyperparameter optimization for AUC with GPU\n",
    "print(\"\\n===== Starting Optuna GPU hyperparameter tuning process (AUC) =====\")\n",
    "\n",
    "def objective_auc(trial):\n",
    "    \"\"\"Optuna optimization objective function using AUC as evaluation metric with GPU acceleration\"\"\"\n",
    "    # Define CatBoost parameter search space\n",
    "    params = {\n",
    "        'loss_function': 'Logloss',  # For binary classification\n",
    "        'eval_metric': 'AUC',\n",
    "        'verbose': 0,\n",
    "        \n",
    "        # Core parameters\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'iterations': 2000,          # Will use early stopping to select the best iteration\n",
    "        'depth': trial.suggest_int('depth', 4, 10),\n",
    "        \n",
    "        # Regularization parameters\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-8, 10.0, log=True),\n",
    "        'random_strength': trial.suggest_float('random_strength', 1e-8, 10.0, log=True),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 10.0),\n",
    "        \n",
    "        # Other parameters\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 100),\n",
    "        'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations', 1, 10),\n",
    "        'rsm': trial.suggest_float('rsm', 0.1, 1.0),  # Column sample ratio\n",
    "        \n",
    "        'random_seed': 42\n",
    "    }\n",
    "    \n",
    "    # Add GPU parameters if GPU is available\n",
    "    if GPU_AVAILABLE:\n",
    "        params['task_type'] = 'GPU'\n",
    "        params['devices'] = '0'\n",
    "    \n",
    "    # Create CatBoost model\n",
    "    model = cb.CatBoostClassifier(**params)\n",
    "    \n",
    "    # Train model on training set, using validation set for early stopping\n",
    "    model.fit(\n",
    "        X_train_final, y_train,\n",
    "        eval_set=[(X_val_final, y_val)],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Predict on validation set\n",
    "    y_val_prob = model.predict_proba(X_val_final)[:, 1]\n",
    "    \n",
    "    # Calculate AUC score\n",
    "    auc_score = roc_auc_score(y_val, y_val_prob)\n",
    "    \n",
    "    # Print current trial results\n",
    "    print(f\"Trial {trial.number}: AUC = {auc_score:.4f}\")\n",
    "    \n",
    "    return auc_score\n",
    "\n",
    "# Create Optuna study object with direction to maximize AUC\n",
    "study_auc = optuna.create_study(direction='maximize', study_name='catboost_gpu_auc_optimization')\n",
    "\n",
    "# Run optimization\n",
    "n_trials = 100  # Can adjust based on computational resources and time\n",
    "print(f\"Starting {n_trials} GPU-accelerated hyperparameter tuning trials...\")\n",
    "start_time = time.time()\n",
    "study_auc.optimize(objective_auc, n_trials=n_trials)\n",
    "end_time = time.time()\n",
    "print(f\"Tuning completed! Duration: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Print best parameters and results\n",
    "print(\"\\n===== Best Parameters (AUC) =====\")\n",
    "print(f\"Best AUC score: {study_auc.best_value:.4f}\")\n",
    "print(\"Best parameter combination:\")\n",
    "for key, value in study_auc.best_params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Visualize optimization process\n",
    "plt.figure(figsize=(12, 8))\n",
    "optuna.visualization.matplotlib.plot_optimization_history(study_auc)\n",
    "plt.title('Optimization History - AUC')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{plots_dir}/optuna_catboost_auc_history.png')\n",
    "plt.close()\n",
    "\n",
    "# Visualize hyperparameter importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "optuna.visualization.matplotlib.plot_param_importances(study_auc)\n",
    "plt.title('Hyperparameter Importance')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{plots_dir}/optuna_catboost_auc_param_importances.png')\n",
    "plt.close()\n",
    "\n",
    "# 6. Train final model with best parameters\n",
    "print(\"\\n===== Training Final Model with Best Parameters on GPU =====\")\n",
    "best_params = study_auc.best_params.copy()\n",
    "best_params['loss_function'] = 'Logloss'\n",
    "best_params['eval_metric'] = 'AUC'\n",
    "best_params['random_seed'] = 42\n",
    "\n",
    "# Add GPU parameters if available\n",
    "if GPU_AVAILABLE:\n",
    "    best_params['task_type'] = 'GPU'\n",
    "    best_params['devices'] = '0'\n",
    "    print(\"Using GPU acceleration for final model training\")\n",
    "else:\n",
    "    print(\"Using CPU for final model training\")\n",
    "\n",
    "# Create final model\n",
    "final_model = cb.CatBoostClassifier(**best_params)\n",
    "\n",
    "# Train final model\n",
    "start_time = time.time()\n",
    "final_model.fit(\n",
    "    X_train_final, y_train,\n",
    "    eval_set=[(X_val_final, y_val)],\n",
    "    early_stopping_rounds=100,\n",
    "    verbose=50  # Show progress every 50 iterations\n",
    ")\n",
    "end_time = time.time()\n",
    "print(f\"Final model training completed! Duration: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# 7. Evaluate final model on validation set\n",
    "y_val_prob = final_model.predict_proba(X_val_final)[:, 1]\n",
    "y_val_pred = final_model.predict(X_val_final)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "val_auc = roc_auc_score(y_val, y_val_prob)\n",
    "val_acc = accuracy_score(y_val, y_val_pred)\n",
    "val_f1 = f1_score(y_val, y_val_pred)\n",
    "val_f1_weighted = f1_score(y_val, y_val_pred, average='weighted')\n",
    "\n",
    "print(\"\\n===== Final Model Performance on Validation Set =====\")\n",
    "print(f\"AUC: {val_auc:.4f}\")  # Highlight AUC\n",
    "print(f\"Weighted F1 Score: {val_f1_weighted:.4f}\")\n",
    "print(f\"F1 Score: {val_f1:.4f}\")\n",
    "print(f\"Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "print(\"\\nValidation Set Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_val_pred))\n",
    "print(\"\\nValidation Set Classification Report:\")\n",
    "print(classification_report(y_val, y_val_pred, digits=4))\n",
    "\n",
    "# 8. Evaluate final model on test set\n",
    "y_test_prob = final_model.predict_proba(X_test_final)[:, 1]\n",
    "y_test_pred = final_model.predict(X_test_final)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "test_auc = roc_auc_score(y_test, y_test_prob)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "test_f1_weighted = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "print(\"\\n===== Final Model Performance on Test Set =====\")\n",
    "print(f\"AUC: {test_auc:.4f}\")  # Highlight AUC\n",
    "print(f\"Weighted F1 Score: {test_f1_weighted:.4f}\")\n",
    "print(f\"F1 Score: {test_f1:.4f}\")\n",
    "print(f\"Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "print(\"\\nTest Set Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "print(\"\\nTest Set Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, digits=4))\n",
    "\n",
    "# 9. Plot ROC and PR curves\n",
    "# ROC curve\n",
    "plt.figure(figsize=(12, 10))\n",
    "# Validation set\n",
    "fpr_val, tpr_val, _ = roc_curve(y_val, y_val_prob)\n",
    "plt.plot(fpr_val, tpr_val, label=f'Validation Set (AUC = {val_auc:.4f})')\n",
    "# Test set\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, y_test_prob)\n",
    "plt.plot(fpr_test, tpr_test, label=f'Test Set (AUC = {test_auc:.4f})')\n",
    "# Reference line\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison (CatBoost GPU Model)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{plots_dir}/catboost_gpu_roc_curves.png')\n",
    "plt.close()\n",
    "\n",
    "# PR curve\n",
    "plt.figure(figsize=(12, 10))\n",
    "# Validation set\n",
    "prec_val, rec_val, _ = precision_recall_curve(y_val, y_val_prob)\n",
    "avg_prec_val = average_precision_score(y_val, y_val_prob)\n",
    "plt.plot(rec_val, prec_val, label=f'Validation Set (AP = {avg_prec_val:.4f})')\n",
    "# Test set\n",
    "prec_test, rec_test, _ = precision_recall_curve(y_test, y_test_prob)\n",
    "avg_prec_test = average_precision_score(y_test, y_test_prob)\n",
    "plt.plot(rec_test, prec_test, label=f'Test Set (AP = {avg_prec_test:.4f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('PR Curve Comparison (CatBoost GPU Model)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{plots_dir}/catboost_gpu_pr_curves.png')\n",
    "plt.close()\n",
    "\n",
    "# 10. Feature importance visualization\n",
    "feature_importance = final_model.get_feature_importance()\n",
    "feature_names = X_train_final.columns\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
    "importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.barplot(x='Importance', y='Feature', data=importance_df.head(20))\n",
    "plt.title(\"CatBoost Feature Importance (GPU Model)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{plots_dir}/catboost_gpu_feature_importance.png')\n",
    "plt.close()\n",
    "\n",
    "# 11. Save best model\n",
    "model_path = f'{models_dir}/catboost_gpu_auc.cbm'\n",
    "final_model.save_model(model_path)\n",
    "print(f\"\\nBest model saved to: {model_path}\")\n",
    "\n",
    "# 12. Performance summary\n",
    "print(\"\\n===== Model Performance Summary =====\")\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['AUC', 'Accuracy', 'F1 Score', 'Weighted F1 Score'],\n",
    "    'Validation Set': [val_auc, val_acc, val_f1, val_f1_weighted],\n",
    "    'Test Set': [test_auc, test_acc, test_f1, test_f1_weighted]\n",
    "})\n",
    "print(results_df)\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv(f'{results_dir}/catboost_gpu_auc_performance.csv', index=False)\n",
    "print(f\"Model performance saved to: {results_dir}/catboost_gpu_auc_performance.csv\")\n",
    "\n",
    "# 13. Save feature list for future use\n",
    "with open(f'{models_dir}/catboost_gpu_selected_features.txt', 'w') as f:\n",
    "    f.write('\\n'.join(selected_features))\n",
    "print(f\"Feature list saved to: {models_dir}/catboost_gpu_selected_features.txt\")\n",
    "\n",
    "print(\"\\nAnalysis completed! All results and visualizations have been saved to the specified directories.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ada5552",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d52406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import optuna  # For hyperparameter optimization\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, confusion_matrix, classification_report, \n",
    "                           roc_auc_score, roc_curve, precision_recall_curve, \n",
    "                           average_precision_score, f1_score)\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'  \n",
    "\n",
    "# Set up directory structure for outputs\n",
    "model_name = \"lightgbm\"  # Can be changed to different model names\n",
    "base_dir = f'/data/jinming/ee_stable/{model_name}'\n",
    "plots_dir = f'{base_dir}/plots'\n",
    "models_dir = f'{base_dir}/models'\n",
    "results_dir = f'{base_dir}/results'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for directory in [plots_dir, models_dir, results_dir]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# 1. Load data\n",
    "print(\"===== Loading Data =====\")\n",
    "train_df = pd.read_csv('/data/jinming/ee_stable/data/train.csv')\n",
    "test_df = pd.read_csv('/data/jinming/ee_stable/data/test.csv')\n",
    "val_df = pd.read_csv('/data/jinming/ee_stable/data/val.csv')\n",
    "\n",
    "# 2. Data preparation\n",
    "print(\"===== Preparing Data =====\")\n",
    "X_train = train_df.drop(['stab', 'stabf_encoded', 'stabf', 'p1', 'p2', 'p3', 'p4'], axis=1)\n",
    "X_test = test_df.drop(['stab', 'stabf_encoded', 'stabf', 'p1', 'p2', 'p3', 'p4'], axis=1)\n",
    "X_val = val_df.drop(['stab', 'stabf_encoded', 'stabf', 'p1', 'p2', 'p3', 'p4'], axis=1)\n",
    "\n",
    "y_train = train_df['stabf_encoded']\n",
    "y_test = test_df['stabf_encoded']\n",
    "y_val = val_df['stabf_encoded']\n",
    "\n",
    "print(f\"Dataset dimensions - Train: {X_train.shape}, Validation: {X_val.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# 3. Feature engineering function\n",
    "def create_features(X_train, X_test, X_val):\n",
    "    # Deep copy to avoid modifying original data\n",
    "    X_train_new = X_train.copy()\n",
    "    X_test_new = X_test.copy()\n",
    "    X_val_new = X_val.copy()\n",
    "    \n",
    "    # Basic interaction features\n",
    "    for df in [X_train_new, X_test_new, X_val_new]:\n",
    "        df['tau1_g1'] = df['tau1'] * df['g1']\n",
    "        df['tau2_g2'] = df['tau2'] * df['g2']\n",
    "        df['tau3_g3'] = df['tau3'] * df['g3']\n",
    "        df['tau4_g4'] = df['tau4'] * df['g4']\n",
    "        \n",
    "        # Delay ratio\n",
    "        df['tau_ratio'] = df[['tau1', 'tau2', 'tau3', 'tau4']].max(axis=1) / df[['tau1', 'tau2', 'tau3', 'tau4']].min(axis=1).replace(0, 0.001)\n",
    "        \n",
    "        # Delay-elasticity ratio: response sensitivity of each node\n",
    "        df['tau1_g1_ratio'] = df['tau1'] / df['g1'].replace(0, 0.001)\n",
    "        df['tau2_g2_ratio'] = df['tau2'] / df['g2'].replace(0, 0.001)\n",
    "        df['tau3_g3_ratio'] = df['tau3'] / df['g3'].replace(0, 0.001)\n",
    "        df['tau4_g4_ratio'] = df['tau4'] / df['g4'].replace(0, 0.001)\n",
    "        \n",
    "        # System total elasticity\n",
    "        df['total_elasticity'] = df['g1'] + df['g2'] + df['g3'] + df['g4']\n",
    "        \n",
    "        # Elasticity distribution non-uniformity\n",
    "        df['elasticity_disparity'] = df[['g1', 'g2', 'g3', 'g4']].max(axis=1) / df[['g1', 'g2', 'g3', 'g4']].min(axis=1).replace(0, 0.001)\n",
    "        \n",
    "        # Non-linear features - quadratic terms\n",
    "        df['tau1_squared'] = df['tau1'] ** 2\n",
    "        df['tau2_squared'] = df['tau2'] ** 2\n",
    "        df['tau3_squared'] = df['tau3'] ** 2\n",
    "        df['tau4_squared'] = df['tau4'] ** 2\n",
    "        \n",
    "        # Node relationship features\n",
    "        df['tau_g_correlation'] = (\n",
    "            (df['tau1'] * df['g1']) + \n",
    "            (df['tau2'] * df['g2']) + \n",
    "            (df['tau3'] * df['g3']) + \n",
    "            (df['tau4'] * df['g4'])\n",
    "        ) / (df['tau1'] + df['tau2'] + df['tau3'] + df['tau4'] + 0.001)\n",
    "        \n",
    "        # System overall response speed indicator\n",
    "        df['system_response_speed'] = 4 / (\n",
    "            (1/df['tau1'].replace(0, 0.001)) + \n",
    "            (1/df['tau2'].replace(0, 0.001)) + \n",
    "            (1/df['tau3'].replace(0, 0.001)) + \n",
    "            (1/df['tau4'].replace(0, 0.001))\n",
    "        )\n",
    "    \n",
    "    return X_train_new, X_test_new, X_val_new\n",
    "\n",
    "# 4. Feature selection functions\n",
    "def select_features(X_train, X_test, X_val, y_train):\n",
    "    \"\"\"\n",
    "    Feature selection based on correlation and importance\n",
    "    \"\"\"\n",
    "    print(\"\\n===== Starting Feature Selection =====\")\n",
    "    \n",
    "    # Step 1: Calculate correlation with target\n",
    "    print(\"Step 1: Calculating feature-target correlations\")\n",
    "    \n",
    "    feature_target_corr = {}\n",
    "    for col in X_train.columns:\n",
    "        corr = abs(np.corrcoef(X_train[col], y_train)[0, 1])\n",
    "        feature_target_corr[col] = corr\n",
    "    \n",
    "    feature_corr_df = pd.DataFrame({\n",
    "        'Feature': list(feature_target_corr.keys()),\n",
    "        'Target_Correlation': list(feature_target_corr.values())\n",
    "    }).sort_values('Target_Correlation', ascending=False)\n",
    "    \n",
    "    # Visualize correlations with target\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.barplot(x='Target_Correlation', y='Feature', data=feature_corr_df.head(20))\n",
    "    plt.title('Feature Correlation with Target Variable')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{plots_dir}/target_correlation.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Top 10 features with highest target correlation:\")\n",
    "    print(feature_corr_df.head(10))\n",
    "    \n",
    "    # Step 2: Remove highly correlated features\n",
    "    print(\"\\nStep 2: Removing redundant features\")\n",
    "    \n",
    "    corr_matrix = X_train.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    \n",
    "    # Visualize correlation matrix\n",
    "    plt.figure(figsize=(16, 14))\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    sns.heatmap(corr_matrix, cmap='coolwarm', center=0, mask=mask,\n",
    "                square=True, linewidths=.5, annot=False, fmt='.2f')\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{plots_dir}/feature_correlation.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Highly correlated feature pairs\n",
    "    correlation_threshold = 0.7\n",
    "    to_drop = set()\n",
    "    \n",
    "    for i, row_name in enumerate(upper.index):\n",
    "        for col_name in upper.columns[i:]:\n",
    "            if upper.loc[row_name, col_name] > correlation_threshold:\n",
    "                if feature_target_corr[row_name] > feature_target_corr[col_name]:\n",
    "                    to_drop.add(col_name)\n",
    "                else:\n",
    "                    to_drop.add(row_name)\n",
    "    \n",
    "    print(f\"Removing {len(to_drop)} highly correlated redundant features:\")\n",
    "    print(\", \".join(list(to_drop)))\n",
    "    \n",
    "    # Remove redundant features\n",
    "    X_train_filtered = X_train.drop(columns=list(to_drop))\n",
    "    X_test_filtered = X_test.drop(columns=list(to_drop))\n",
    "    X_val_filtered = X_val.drop(columns=list(to_drop))\n",
    "    \n",
    "    # Step 3: Model-based feature importance\n",
    "    print(\"\\nStep 3: Feature selection based on model importance\")\n",
    "    \n",
    "    # Train a LightGBM model for feature importance assessment\n",
    "    feature_selector = lgb.LGBMClassifier(\n",
    "        objective='binary',\n",
    "        metric='auc',\n",
    "        boosting_type='gbdt',\n",
    "        num_leaves=31,\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=100,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    feature_selector.fit(X_train_filtered, y_train)\n",
    "    \n",
    "    # Get feature importance\n",
    "    importances = feature_selector.feature_importances_\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': X_train_filtered.columns,\n",
    "        'Importance': importances\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Visualize feature importance\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(20))\n",
    "    plt.title('LightGBM Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{plots_dir}/feature_importance.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Dynamic threshold setting\n",
    "    mean_importance = feature_importance_df['Importance'].mean()\n",
    "    importance_threshold = mean_importance * 0.5\n",
    "    print(f\"Dynamic threshold: {importance_threshold:.2f} (50% of mean importance)\")\n",
    "    selected_features = feature_importance_df[feature_importance_df['Importance'] > importance_threshold]['Feature'].tolist()\n",
    "    \n",
    "    # Keep at least 10 most important features if filtered list is too small\n",
    "    if len(selected_features) < 10:\n",
    "        selected_features = feature_importance_df.head(10)['Feature'].tolist()\n",
    "    \n",
    "    print(f\"\\nFinally selected {len(selected_features)}/{X_train.shape[1]} features\")\n",
    "    print(f\"Selected features: {', '.join(selected_features)}\")\n",
    "    \n",
    "    return X_train[selected_features], X_test[selected_features], X_val[selected_features], selected_features\n",
    "\n",
    "# Manual feature selection function\n",
    "def select_features_manual(X_train, X_test, X_val):\n",
    "    \"\"\"Manually select specified feature set\"\"\"\n",
    "    print(\"\\n===== Using Manually Specified Features =====\")\n",
    "    \n",
    "    # Specify features to keep\n",
    "    selected_features = [\n",
    "        # Original tau features\n",
    "        'tau1', 'tau2', 'tau3', 'tau4',\n",
    "        \n",
    "        # Original g features\n",
    "        'g1', 'g2', 'g3', 'g4',\n",
    "        \n",
    "        # tau and g interaction terms\n",
    "        'tau1_g1', 'tau2_g2', 'tau3_g3', 'tau4_g4',\n",
    "        \n",
    "        # tau ratio features\n",
    "        'tau_ratio'\n",
    "    ]\n",
    "    \n",
    "    # Verify all specified features exist\n",
    "    missing_features = [f for f in selected_features if f not in X_train.columns]\n",
    "    if missing_features:\n",
    "        print(f\"Warning: The following specified features do not exist: {', '.join(missing_features)}\")\n",
    "        # Filter out non-existent features\n",
    "        selected_features = [f for f in selected_features if f in X_train.columns]\n",
    "    \n",
    "    print(f\"Using {len(selected_features)} specified features:\")\n",
    "    print(f\"Selected features: {', '.join(selected_features)}\")\n",
    "    \n",
    "    return X_train[selected_features], X_test[selected_features], X_val[selected_features], selected_features\n",
    "\n",
    "# Apply feature engineering\n",
    "print(\"\\n===== Performing Feature Engineering =====\")\n",
    "X_train_featured, X_test_featured, X_val_featured = create_features(X_train, X_test, X_val)\n",
    "print(f\"Number of features after engineering: {X_train_featured.shape[1]}\")\n",
    "\n",
    "# Apply feature selection - can choose automatic or manual method\n",
    "# Use manual feature selection\n",
    "X_train_selected, X_test_selected, X_val_selected, selected_features = select_features_manual(\n",
    "   X_train_featured, X_test_featured, X_val_featured)\n",
    "\n",
    "print(f\"Number of features after selection: {X_train_selected.shape[1]}\")\n",
    "\n",
    "# Use the selected features\n",
    "X_train_final = X_train_selected\n",
    "X_test_final = X_test_selected\n",
    "X_val_final = X_val_selected\n",
    "\n",
    "# 5. Optuna hyperparameter optimization for AUC\n",
    "print(\"\\n===== Starting Optuna Hyperparameter Tuning Process (AUC) =====\")\n",
    "\n",
    "\n",
    "def objective_auc(trial):\n",
    "    # Define LightGBM parameter search space\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbosity': -1,\n",
    "        \n",
    "        # Core parameters\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.5, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 30, 1000),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 150),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "        \n",
    "        # Regularization parameters\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "        \n",
    "        # Other parameters\n",
    "        'min_split_gain': trial.suggest_float('min_split_gain', 0, 0.5),\n",
    "        'min_child_weight': trial.suggest_float('min_child_weight', 1e-5, 10.0, log=True),\n",
    "        \n",
    "        # GPU acceleration parameters \n",
    "        'device': 'gpu',\n",
    "        'gpu_platform_id': 0,\n",
    "        'gpu_device_id': 0,\n",
    "        'num_gpu': 1, \n",
    "        'n_jobs': 16,    \n",
    "\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    # Rest of the function remains the same\n",
    "    # ...\n",
    "    \n",
    "    # Create LightGBM model\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    \n",
    "    # Train model on the training set, using validation set for early stopping\n",
    "    model.fit(\n",
    "        X_train_final, y_train,\n",
    "        eval_set=[(X_val_final, y_val)],\n",
    "        eval_metric='auc',\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]\n",
    "    )\n",
    "    \n",
    "    # Predict on validation set\n",
    "    y_val_prob = model.predict_proba(X_val_final)[:, 1]\n",
    "    \n",
    "    # Calculate AUC score\n",
    "    auc_score = roc_auc_score(y_val, y_val_prob)\n",
    "    \n",
    "    # Print current trial results\n",
    "    print(f\"Trial {trial.number}: AUC = {auc_score:.4f}\")\n",
    "    \n",
    "    return auc_score  # Return AUC as optimization target\n",
    "\n",
    "# Create Optuna study object - optimization direction is to maximize AUC\n",
    "study_auc = optuna.create_study(direction='maximize', study_name='lightgbm_auc_optimization')\n",
    "\n",
    "# Run optimization\n",
    "n_trials = 10  # Can be adjusted based on computational resources and time\n",
    "print(f\"Starting {n_trials} hyperparameter tuning trials...\")\n",
    "start_time = time.time()\n",
    "study_auc.optimize(objective_auc, n_trials=n_trials)\n",
    "end_time = time.time()\n",
    "print(f\"Tuning completed! Duration: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Print best parameters and results\n",
    "print(\"\\n===== Best Parameters (AUC) =====\")\n",
    "print(f\"Best AUC score: {study_auc.best_value:.4f}\")\n",
    "print(\"Best parameter combination:\")\n",
    "for key, value in study_auc.best_params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Visualize optimization process\n",
    "plt.figure(figsize=(12, 8))\n",
    "optuna.visualization.matplotlib.plot_optimization_history(study_auc)\n",
    "plt.title('Optimization History - AUC')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{plots_dir}/optuna_auc_history.png')\n",
    "plt.close()\n",
    "\n",
    "# Visualize hyperparameter importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "optuna.visualization.matplotlib.plot_param_importances(study_auc)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{plots_dir}/optuna_auc_param_importances.png')\n",
    "plt.close()\n",
    "\n",
    "# 6. Train final model with best parameters for weighted F1\n",
    "print(\"\\n===== Training Final Model with Best Parameters =====\")\n",
    "best_params_auc = study_auc.best_params.copy()\n",
    "best_params_auc['objective'] = 'binary'\n",
    "best_params_auc['metric'] = 'auc'\n",
    "best_params_auc['boosting_type'] = 'gbdt'\n",
    "best_params_auc['random_state'] = 42\n",
    "best_params_auc['verbosity'] = -1\n",
    "\n",
    "\n",
    "best_params_auc['device'] = 'gpu'\n",
    "best_params_auc['gpu_platform_id'] = 0  \n",
    "best_params_auc['gpu_device_id'] = 0    \n",
    "best_params_auc['num_gpu'] = 1\n",
    "best_params_auc['n_jobs'] = 2   \n",
    "\n",
    "if 'num_threads' in best_params_auc:\n",
    "    del best_params_auc['num_threads']\n",
    "\n",
    "final_model_auc = lgb.LGBMClassifier(**best_params_auc)\n",
    "\n",
    "start_time = time.time()\n",
    "final_model_auc.fit(\n",
    "    X_train_final, y_train,\n",
    "    eval_set=[(X_val_final, y_val)],\n",
    "    eval_metric='auc',\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=True)]\n",
    ")\n",
    "end_time = time.time()\n",
    "print(f\"Final model training completed! Duration: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# 7. Evaluate final model on validation set\n",
    "y_val_prob = final_model_auc.predict_proba(X_val_final)[:, 1]\n",
    "y_val_pred = final_model_auc.predict(X_val_final)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "val_auc = roc_auc_score(y_val, y_val_prob)\n",
    "val_acc = accuracy_score(y_val, y_val_pred)\n",
    "val_f1 = f1_score(y_val, y_val_pred)\n",
    "val_f1_weighted = f1_score(y_val, y_val_pred, average='weighted')\n",
    "\n",
    "print(\"\\n===== Final Model Performance on Validation Set =====\")\n",
    "print(f\"AUC: {val_auc:.4f}\")  # Highlight AUC\n",
    "print(f\"Weighted F1 Score: {val_f1_weighted:.4f}\")\n",
    "print(f\"F1 Score: {val_f1:.4f}\")\n",
    "print(f\"Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "print(\"\\nValidation Set Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_val_pred))\n",
    "print(\"\\nValidation Set Classification Report:\")\n",
    "print(classification_report(y_val, y_val_pred, digits=4))\n",
    "\n",
    "# 8. Evaluate final model on test set\n",
    "y_test_prob = final_model_auc.predict_proba(X_test_final)[:, 1]\n",
    "y_test_pred = final_model_auc.predict(X_test_final)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "test_auc = roc_auc_score(y_test, y_test_prob)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "test_f1_weighted = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "print(\"\\n===== Final Model Performance on Test Set =====\")\n",
    "print(f\"AUC: {test_auc:.4f}\")  # Highlight AUC\n",
    "print(f\"Weighted F1 Score: {test_f1_weighted:.4f}\")\n",
    "print(f\"F1 Score: {test_f1:.4f}\")\n",
    "print(f\"Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "print(\"\\nTest Set Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "print(\"\\nTest Set Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, digits=4))\n",
    "\n",
    "# 9. Plot ROC and PR curves\n",
    "# ROC curve\n",
    "plt.figure(figsize=(12, 10))\n",
    "# Validation set\n",
    "fpr_val, tpr_val, _ = roc_curve(y_val, y_val_prob)\n",
    "plt.plot(fpr_val, tpr_val, label=f'Validation Set (AUC = {val_auc:.4f})')\n",
    "# Test set\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, y_test_prob)\n",
    "plt.plot(fpr_test, tpr_test, label=f'Test Set (AUC = {test_auc:.4f})')\n",
    "# Reference line\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{plots_dir}/final_model_roc_curves.png')\n",
    "plt.close()\n",
    "\n",
    "# PR curve\n",
    "plt.figure(figsize=(12, 10))\n",
    "# Validation set\n",
    "prec_val, rec_val, _ = precision_recall_curve(y_val, y_val_prob)\n",
    "avg_prec_val = average_precision_score(y_val, y_val_prob)\n",
    "plt.plot(rec_val, prec_val, label=f'Validation Set (AP = {avg_prec_val:.4f})')\n",
    "# Test set\n",
    "prec_test, rec_test, _ = precision_recall_curve(y_test, y_test_prob)\n",
    "avg_prec_test = average_precision_score(y_test, y_test_prob)\n",
    "plt.plot(rec_test, prec_test, label=f'Test Set (AP = {avg_prec_test:.4f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('PR Curve Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{plots_dir}/final_model_pr_curves.png')\n",
    "plt.close()\n",
    "\n",
    "# 10. Feature importance visualization\n",
    "plt.figure(figsize=(14, 10))\n",
    "lgb.plot_importance(final_model_auc, max_num_features=20, importance_type='gain')\n",
    "plt.title(\"LightGBM Feature Importance (Gain)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{plots_dir}/final_model_feature_importance.png')\n",
    "plt.close()\n",
    "\n",
    "# 11. Save best model\n",
    "import joblib\n",
    "model_path = f'{models_dir}/lgbm_optuna_auc.pkl'\n",
    "joblib.dump(final_model_auc, model_path)\n",
    "print(f\"\\nBest model saved to: {model_path}\")\n",
    "\n",
    "# 12. Performance summary\n",
    "print(\"\\n===== Model Performance Summary =====\")\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['AUC', 'Accuracy', 'F1 Score', 'Weighted F1 Score'],\n",
    "    'Validation Set': [val_auc, val_acc, val_f1, val_f1_weighted],\n",
    "    'Test Set': [test_auc, test_acc, test_f1, test_f1_weighted]\n",
    "})\n",
    "print(results_df)\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv(f'{results_dir}/model_performance.csv', index=False)\n",
    "print(f\"Model performance saved to: {results_dir}/model_performance_auc.csv\")\n",
    "\n",
    "# 13. Save feature list for future use\n",
    "with open(f'{models_dir}/selected_features.txt', 'w') as f:\n",
    "    f.write('\\n'.join(selected_features))\n",
    "print(f\"Feature list saved to: {models_dir}/selected_features.txt\")\n",
    "\n",
    "print(\"\\nAnalysis completed! All results and visualizations have been saved to the specified directories.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc23ed41",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420e20be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, confusion_matrix, classification_report, \n",
    "                           roc_auc_score, roc_curve, precision_recall_curve, \n",
    "                           average_precision_score, f1_score)\n",
    "import xgboost as xgb  # Using XGBoost instead of LightGBM\n",
    "import optuna  # For hyperparameter optimization\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ensure output directories exist\n",
    "plots_dir = '/data/jinming/ee_stable/xgboost/plots'\n",
    "models_dir = '/data/jinming/ee_stable/xgboost/models'\n",
    "results_dir = '/data/jinming/ee_stable/xgboost/results'\n",
    "for directory in [plots_dir, models_dir, results_dir]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# 1. Load data\n",
    "print(\"===== Loading Data =====\")\n",
    "train_df = pd.read_csv('/data/jinming/ee_stable/data/train.csv')\n",
    "test_df = pd.read_csv('/data/jinming/ee_stable/data/test.csv')\n",
    "val_df = pd.read_csv('/data/jinming/ee_stable/data/val.csv')\n",
    "\n",
    "# 2. Data preparation\n",
    "print(\"===== Preparing Data =====\")\n",
    "X_train = train_df.drop(['stab', 'stabf_encoded', 'stabf', 'p1', 'p2', 'p3', 'p4'], axis=1)\n",
    "X_test = test_df.drop(['stab', 'stabf_encoded', 'stabf', 'p1', 'p2', 'p3', 'p4'], axis=1)\n",
    "X_val = val_df.drop(['stab', 'stabf_encoded', 'stabf', 'p1', 'p2', 'p3', 'p4'], axis=1)\n",
    "\n",
    "y_train = train_df['stabf_encoded']\n",
    "y_test = test_df['stabf_encoded']\n",
    "y_val = val_df['stabf_encoded']\n",
    "\n",
    "print(f\"Dataset dimensions - Train: {X_train.shape}, Validation: {X_val.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# 3. Feature engineering function\n",
    "def create_features(X_train, X_test, X_val):\n",
    "    # Deep copy to avoid modifying original data\n",
    "    X_train_new = X_train.copy()\n",
    "    X_test_new = X_test.copy()\n",
    "    X_val_new = X_val.copy()\n",
    "    \n",
    "    # Basic interaction features\n",
    "    for df in [X_train_new, X_test_new, X_val_new]:\n",
    "        df['tau1_g1'] = df['tau1'] * df['g1']\n",
    "        df['tau2_g2'] = df['tau2'] * df['g2']\n",
    "        df['tau3_g3'] = df['tau3'] * df['g3']\n",
    "        df['tau4_g4'] = df['tau4'] * df['g4']\n",
    "        \n",
    "        # Delay ratio\n",
    "        df['tau_ratio'] = df[['tau1', 'tau2', 'tau3', 'tau4']].max(axis=1) / df[['tau1', 'tau2', 'tau3', 'tau4']].min(axis=1).replace(0, 0.001)\n",
    "        \n",
    "        # Delay-elasticity ratio: response sensitivity of each node\n",
    "        df['tau1_g1_ratio'] = df['tau1'] / df['g1'].replace(0, 0.001)\n",
    "        df['tau2_g2_ratio'] = df['tau2'] / df['g2'].replace(0, 0.001)\n",
    "        df['tau3_g3_ratio'] = df['tau3'] / df['g3'].replace(0, 0.001)\n",
    "        df['tau4_g4_ratio'] = df['tau4'] / df['g4'].replace(0, 0.001)\n",
    "        \n",
    "        # System total elasticity\n",
    "        df['total_elasticity'] = df['g1'] + df['g2'] + df['g3'] + df['g4']\n",
    "        \n",
    "        # Elasticity distribution non-uniformity\n",
    "        df['elasticity_disparity'] = df[['g1', 'g2', 'g3', 'g4']].max(axis=1) / df[['g1', 'g2', 'g3', 'g4']].min(axis=1).replace(0, 0.001)\n",
    "        \n",
    "        # Non-linear features - quadratic terms\n",
    "        df['tau1_squared'] = df['tau1'] ** 2\n",
    "        df['tau2_squared'] = df['tau2'] ** 2\n",
    "        df['tau3_squared'] = df['tau3'] ** 2\n",
    "        df['tau4_squared'] = df['tau4'] ** 2\n",
    "        \n",
    "        # Node relationship features\n",
    "        df['tau_g_correlation'] = (\n",
    "            (df['tau1'] * df['g1']) + \n",
    "            (df['tau2'] * df['g2']) + \n",
    "            (df['tau3'] * df['g3']) + \n",
    "            (df['tau4'] * df['g4'])\n",
    "        ) / (df['tau1'] + df['tau2'] + df['tau3'] + df['tau4'] + 0.001)\n",
    "        \n",
    "        # System overall response speed indicator\n",
    "        df['system_response_speed'] = 4 / (\n",
    "            (1/df['tau1'].replace(0, 0.001)) + \n",
    "            (1/df['tau2'].replace(0, 0.001)) + \n",
    "            (1/df['tau3'].replace(0, 0.001)) + \n",
    "            (1/df['tau4'].replace(0, 0.001))\n",
    "        )\n",
    "    \n",
    "    return X_train_new, X_test_new, X_val_new\n",
    "\n",
    "# Manual feature selection function\n",
    "def select_features_manual(X_train, X_test, X_val):\n",
    "    \"\"\"Manually select specified feature set\"\"\"\n",
    "    print(\"\\n===== Using Manually Specified Features =====\")\n",
    "    \n",
    "    # Specify features to keep\n",
    "    selected_features = [\n",
    "        # Original tau features\n",
    "        'tau1', 'tau2', 'tau3', 'tau4',\n",
    "        \n",
    "        # Original g features\n",
    "        'g1', 'g2', 'g3', 'g4',\n",
    "        \n",
    "        # tau and g interaction terms\n",
    "        'tau1_g1', 'tau2_g2', 'tau3_g3', 'tau4_g4',\n",
    "        \n",
    "        # tau ratio features\n",
    "        'tau_ratio'\n",
    "    ]\n",
    "    \n",
    "    # Verify all specified features exist\n",
    "    missing_features = [f for f in selected_features if f not in X_train.columns]\n",
    "    if missing_features:\n",
    "        print(f\"Warning: The following specified features do not exist: {', '.join(missing_features)}\")\n",
    "        # Filter out non-existent features\n",
    "        selected_features = [f for f in selected_features if f in X_train.columns]\n",
    "    \n",
    "    print(f\"Using {len(selected_features)} specified features:\")\n",
    "    print(f\"Selected features: {', '.join(selected_features)}\")\n",
    "    \n",
    "    return X_train[selected_features], X_test[selected_features], X_val[selected_features], selected_features\n",
    "\n",
    "# Apply feature engineering\n",
    "print(\"\\n===== Performing Feature Engineering =====\")\n",
    "X_train_featured, X_test_featured, X_val_featured = create_features(X_train, X_test, X_val)\n",
    "print(f\"Number of features after engineering: {X_train_featured.shape[1]}\")\n",
    "\n",
    "# Apply feature selection using manual method\n",
    "X_train_final, X_test_final, X_val_final, selected_features = select_features_manual(\n",
    "   X_train_featured, X_test_featured, X_val_featured)\n",
    "\n",
    "print(f\"Number of features after selection: {X_train_final.shape[1]}\")\n",
    "\n",
    "# 5.  Optuna  XGBoost GPU\n",
    "print(\"\\n=====  Optuna GPU (AUC) =====\")\n",
    "\n",
    "def objective_auc(trial):\n",
    "    \"\"\"Optuna  -  AUC GPU\"\"\"\n",
    "    # XGBoost\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'verbosity': 0,\n",
    "        \n",
    "        # \n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.5, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        \n",
    "        # \n",
    "        'min_child_weight': trial.suggest_float('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 10),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-8, 10.0, log=True),\n",
    "        'lambda': trial.suggest_float('lambda', 1e-8, 10.0, log=True),\n",
    "        \n",
    "        # GPU\n",
    "        'tree_method': 'gpu_hist',  # GPU\n",
    "        'device': 'cuda',          \n",
    "\n",
    "        'scale_pos_weight': trial.suggest_float('scale_pos_weight', 0.1, 10.0),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    # API (xgb.train) GPU\n",
    "    dtrain = xgb.DMatrix(X_train_final, label=y_train)\n",
    "    dval = xgb.DMatrix(X_val_final, label=y_val)\n",
    "    \n",
    "    # \n",
    "    evals = [(dtrain, 'train'), (dval, 'val')]\n",
    "    evals_result = {}\n",
    "    \n",
    "    # \n",
    "    model = xgb.train(\n",
    "        params=params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=2000,  # \n",
    "        evals=evals,\n",
    "        early_stopping_rounds=50,\n",
    "        evals_result=evals_result,\n",
    "        verbose_eval=False  # \n",
    "    )\n",
    "    \n",
    "    # \n",
    "    y_val_prob = model.predict(dval)\n",
    "    \n",
    "    #  AUC \n",
    "    auc_score = roc_auc_score(y_val, y_val_prob)\n",
    "    \n",
    "    # \n",
    "    print(f\"Trial {trial.number}: AUC = {auc_score:.4f}\")\n",
    "    \n",
    "    return auc_score  #  AUC \n",
    "\n",
    "# Optuna study - AUC\n",
    "study_auc = optuna.create_study(direction='maximize', study_name='xgboost_gpu_auc_optimization')\n",
    "\n",
    "# \n",
    "n_trials = 10  # \n",
    "print(f\" {n_trials} GPU...\")\n",
    "start_time = time.time()\n",
    "study_auc.optimize(objective_auc, n_trials=n_trials)\n",
    "end_time = time.time()\n",
    "print(f\"GPU! : {end_time - start_time:.2f}\")\n",
    "\n",
    "# \n",
    "print(\"\\n=====  (AUC) =====\")\n",
    "print(f\"AUC: {study_auc.best_value:.4f}\")\n",
    "print(\":\")\n",
    "for key, value in study_auc.best_params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "\n",
    "# Visualize optimization process\n",
    "plt.figure(figsize=(12, 8))\n",
    "optuna.visualization.matplotlib.plot_optimization_history(study_auc)\n",
    "plt.title('Optimization History - AUC')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{plots_dir}/optuna_auc_history.png')\n",
    "plt.close()\n",
    "\n",
    "# Visualize hyperparameter importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "optuna.visualization.matplotlib.plot_param_importances(study_auc)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{plots_dir}/optuna_auc_param_importances.png')\n",
    "plt.close()\n",
    "\n",
    "# 6. GPU\n",
    "print(\"\\n===== GPU =====\")\n",
    "\n",
    "# \n",
    "best_params_auc = study_auc.best_params.copy()\n",
    "\n",
    "# \n",
    "best_params_auc.update({\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'tree_method': 'gpu_hist',  # GPU\n",
    "    'device': 'cuda',           # CUDA\n",
    "    'verbosity': 1,\n",
    "    'random_state': 42\n",
    "})\n",
    "\n",
    "# \n",
    "dtrain = xgb.DMatrix(X_train_final, label=y_train)\n",
    "dval = xgb.DMatrix(X_val_final, label=y_val)\n",
    "dtest = xgb.DMatrix(X_test_final, label=y_test)\n",
    "\n",
    "# \n",
    "evals = [(dtrain, 'train'), (dval, 'val')]\n",
    "evals_result = {}\n",
    "\n",
    "# \n",
    "start_time = time.time()\n",
    "\n",
    "# API\n",
    "final_model = xgb.train(\n",
    "    params=best_params_auc,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=3000,  # \n",
    "    evals=evals,\n",
    "    early_stopping_rounds=50,\n",
    "    evals_result=evals_result,\n",
    "    verbose_eval=50  # 50\n",
    ")\n",
    "\n",
    "# \n",
    "end_time = time.time()\n",
    "print(f\"GPU: {end_time - start_time:.2f}\")\n",
    "\n",
    "# 7. \n",
    "y_val_prob = final_model.predict(dval)\n",
    "y_val_pred = (y_val_prob > 0.5).astype(int)  # \n",
    "\n",
    "# \n",
    "val_auc = roc_auc_score(y_val, y_val_prob)\n",
    "val_acc = accuracy_score(y_val, y_val_pred)\n",
    "val_f1 = f1_score(y_val, y_val_pred)\n",
    "val_f1_weighted = f1_score(y_val, y_val_pred, average='weighted')\n",
    "\n",
    "print(\"\\n=====  =====\")\n",
    "print(f\"AUC: {val_auc:.4f}\")\n",
    "print(f\"Weighted F1 Score: {val_f1_weighted:.4f}\")\n",
    "print(f\"F1 Score: {val_f1:.4f}\")\n",
    "print(f\"Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "print(\"\\n:\")\n",
    "print(confusion_matrix(y_val, y_val_pred))\n",
    "print(\"\\n:\")\n",
    "print(classification_report(y_val, y_val_pred, digits=4))\n",
    "\n",
    "# 8. \n",
    "y_test_prob = final_model.predict(dtest)\n",
    "y_test_pred = (y_test_prob > 0.5).astype(int)\n",
    "\n",
    "# \n",
    "test_auc = roc_auc_score(y_test, y_test_prob)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "test_f1_weighted = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "print(\"\\n=====  =====\")\n",
    "print(f\"AUC: {test_auc:.4f}\")\n",
    "print(f\"Weighted F1 Score: {test_f1_weighted:.4f}\")\n",
    "print(f\"F1 Score: {test_f1:.4f}\")\n",
    "print(f\"Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# 9. Plot ROC and PR curves\n",
    "# ROC curve\n",
    "plt.figure(figsize=(12, 10))\n",
    "# Validation set\n",
    "fpr_val, tpr_val, _ = roc_curve(y_val, y_val_prob)\n",
    "plt.plot(fpr_val, tpr_val, label=f'Validation Set (AUC = {val_auc:.4f})')\n",
    "# Test set\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, y_test_prob)\n",
    "plt.plot(fpr_test, tpr_test, label=f'Test Set (AUC = {test_auc:.4f})')\n",
    "# Reference line\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison (XGBoost Model)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{plots_dir}/final_model_roc_curves.png')\n",
    "plt.close()\n",
    "\n",
    "# PR curve\n",
    "plt.figure(figsize=(12, 10))\n",
    "# Validation set\n",
    "prec_val, rec_val, _ = precision_recall_curve(y_val, y_val_prob)\n",
    "avg_prec_val = average_precision_score(y_val, y_val_prob)\n",
    "plt.plot(rec_val, prec_val, label=f'Validation Set (AP = {avg_prec_val:.4f})')\n",
    "# Test set\n",
    "prec_test, rec_test, _ = precision_recall_curve(y_test, y_test_prob)\n",
    "avg_prec_test = average_precision_score(y_test, y_test_prob)\n",
    "plt.plot(rec_test, prec_test, label=f'Test Set (AP = {avg_prec_test:.4f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('PR Curve Comparison (XGBoost Model)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{plots_dir}/final_model_pr_curves.png')\n",
    "plt.close()\n",
    "\n",
    "# 10. Feature importance visualization\n",
    "plt.figure(figsize=(14, 10))\n",
    "xgb.plot_importance(final_model_auc, max_num_features=20, importance_type='gain')\n",
    "plt.title(\"XGBoost Feature Importance (Gain)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{plots_dir}/final_model_feature_importance.png')\n",
    "plt.close()\n",
    "\n",
    "# 11. Save best model\n",
    "import joblib\n",
    "model_path = f'{models_dir}/xgb_optuna_auc.pkl'\n",
    "joblib.dump(final_model_auc, model_path)\n",
    "print(f\"\\nBest model saved to: {model_path}\")\n",
    "\n",
    "# 12. Performance summary\n",
    "print(\"\\n===== Model Performance Summary =====\")\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['AUC', 'Accuracy', 'F1 Score', 'Weighted F1 Score'],\n",
    "    'Validation Set': [val_auc, val_acc, val_f1, val_f1_weighted],\n",
    "    'Test Set': [test_auc, test_acc, test_f1, test_f1_weighted]\n",
    "})\n",
    "print(results_df)\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv(f'{results_dir}/model_performance_auc.csv', index=False)\n",
    "print(f\"Model performance saved to: {results_dir}/model_performance_auc.csv\")\n",
    "\n",
    "# 13. Save feature list for future use\n",
    "with open(f'{models_dir}/selected_features.txt', 'w') as f:\n",
    "    f.write('\\n'.join(selected_features))\n",
    "print(f\"Feature list saved to: {models_dir}/selected_features.txt\")\n",
    "\n",
    "print(\"\\nAnalysis completed! All results and visualizations have been saved to the specified directories.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
